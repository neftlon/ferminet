{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code from [here](https://github.com/google-deepmind/ferminet?tab=readme-ov-file#pretrained-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory 1: Running binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/johannes/.conda/envs/ferminet/bin/ferminet:4: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  __import__('pkg_resources').require('ferminet==0.2')\n",
      "I0130 11:10:03.209882 128561009378112 ferminet:34] System config:\n",
      "\n",
      "batch_size: 256\n",
      "config_module: ferminet.base_config\n",
      "debug:\n",
      "  check_nan: false\n",
      "  deterministic: false\n",
      "log:\n",
      "  features: false\n",
      "  local_energies: false\n",
      "  restore_path: ''\n",
      "  save_frequency: 10.0\n",
      "  save_path: runs/ne_1000\n",
      "  stats_frequency: 1\n",
      "  walkers: false\n",
      "mcmc:\n",
      "  adapt_frequency: 100\n",
      "  blocks: 1\n",
      "  burn_in: 100\n",
      "  init_means: !!python/tuple []\n",
      "  init_width: 1.0\n",
      "  move_width: 0.02\n",
      "  num_leapfrog_steps: 10\n",
      "  scale_by_nuclear_distance: false\n",
      "  steps: 10\n",
      "  use_hmc: false\n",
      "network:\n",
      "  bias_orbitals: false\n",
      "  complex: false\n",
      "  determinants: 16\n",
      "  ferminet:\n",
      "    electron_nuclear_aux_dims: !!python/tuple []\n",
      "    hidden_dims: !!python/tuple\n",
      "    - &id001 !!python/tuple\n",
      "      - 256\n",
      "      - 32\n",
      "    - *id001\n",
      "    - *id001\n",
      "    - *id001\n",
      "    nuclear_embedding_dim: 0\n",
      "    schnet_electron_electron_convolutions: !!python/tuple []\n",
      "    schnet_electron_nuclear_convolutions: !!python/tuple []\n",
      "    separate_spin_channels: false\n",
      "    use_last_layer: false\n",
      "  full_det: true\n",
      "  jastrow: default\n",
      "  make_envelope_fn: ''\n",
      "  make_envelope_kwargs: {}\n",
      "  make_feature_layer_fn: ''\n",
      "  make_feature_layer_kwargs: {}\n",
      "  network_type: ferminet\n",
      "  psiformer:\n",
      "    heads_dim: 64\n",
      "    mlp_hidden_dims: !!python/tuple\n",
      "    - 256\n",
      "    num_heads: 4\n",
      "    num_layers: 2\n",
      "    use_layer_norm: false\n",
      "  rescale_inputs: false\n",
      "observables:\n",
      "  density: false\n",
      "  density_basis: def2-tzvpd\n",
      "  dipole: false\n",
      "  s2: false\n",
      "optim:\n",
      "  adam:\n",
      "    b1: 0.9\n",
      "    b2: 0.999\n",
      "    eps: 1.0e-08\n",
      "    eps_root: 0.0\n",
      "  center_at_clip: true\n",
      "  clip_local_energy: 5.0\n",
      "  clip_median: false\n",
      "  iterations: 1000\n",
      "  kfac:\n",
      "    cov_ema_decay: 0.95\n",
      "    cov_update_every: 1\n",
      "    damping: 0.001\n",
      "    invert_every: 1\n",
      "    l2_reg: 0.0\n",
      "    mean_center: true\n",
      "    min_damping: 0.0001\n",
      "    momentum: 0.0\n",
      "    momentum_type: regular\n",
      "    norm_constraint: 0.001\n",
      "    register_only_generic: false\n",
      "  laplacian: default\n",
      "  lr:\n",
      "    decay: 1.0\n",
      "    delay: 10000.0\n",
      "    rate: 0.05\n",
      "  objective: vmc\n",
      "  optimizer: kfac\n",
      "  reset_if_nan: false\n",
      "pretrain:\n",
      "  basis: ccpvdz\n",
      "  iterations: 100\n",
      "  method: hf\n",
      "system:\n",
      "  electrons: !!python/tuple\n",
      "  - 5\n",
      "  - 5\n",
      "  make_local_energy_fn: ''\n",
      "  make_local_energy_kwargs: {}\n",
      "  molecule:\n",
      "  - !!python/object:ferminet.utils.system.Atom\n",
      "    atomic_number: 10\n",
      "    charge: 10.0\n",
      "    coords: !!python/tuple\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    - 0.0\n",
      "    symbol: Ne\n",
      "    units: bohr\n",
      "  ndim: 3\n",
      "  pp:\n",
      "    basis: ccecp-cc-pVDZ\n",
      "    symbols: null\n",
      "    type: ccecp\n",
      "  pyscf_mol: null\n",
      "  set_molecule: null\n",
      "  states: 0\n",
      "  type: 1\n",
      "  units: bohr\n",
      "  use_pp: false\n",
      "\n",
      "I0130 11:10:03.330018 128561009378112 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA\n",
      "I0130 11:10:03.330787 128561009378112 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n",
      "I0130 11:10:03.332869 128561009378112 train.py:383] Starting QMC with 1 XLA devices per host across 1 hosts.\n",
      "converged SCF energy = -128.488775551741  <S^2> = -7.1054274e-15  2S+1 = 1\n",
      "I0130 11:10:04.750815 128561009378112 train.py:541] No checkpoint found. Training new model.\n",
      "I0130 11:10:09.253112 128561009378112 pretrain.py:309] Pretrain iter 00000: 0.0672805\n",
      "I0130 11:10:09.262732 128561009378112 pretrain.py:309] Pretrain iter 00001: 0.0499008\n",
      "I0130 11:10:09.271379 128561009378112 pretrain.py:309] Pretrain iter 00002: 0.0382796\n",
      "I0130 11:10:09.280900 128561009378112 pretrain.py:309] Pretrain iter 00003: 0.0314358\n",
      "I0130 11:10:09.289475 128561009378112 pretrain.py:309] Pretrain iter 00004: 0.0277988\n",
      "I0130 11:10:09.299134 128561009378112 pretrain.py:309] Pretrain iter 00005: 0.0255686\n",
      "I0130 11:10:09.308868 128561009378112 pretrain.py:309] Pretrain iter 00006: 0.0241648\n",
      "I0130 11:10:09.317928 128561009378112 pretrain.py:309] Pretrain iter 00007: 0.0230569\n",
      "I0130 11:10:09.328020 128561009378112 pretrain.py:309] Pretrain iter 00008: 0.0220495\n",
      "I0130 11:10:09.336798 128561009378112 pretrain.py:309] Pretrain iter 00009: 0.0211434\n",
      "I0130 11:10:09.346283 128561009378112 pretrain.py:309] Pretrain iter 00010: 0.0204736\n",
      "I0130 11:10:09.354672 128561009378112 pretrain.py:309] Pretrain iter 00011: 0.0199564\n",
      "I0130 11:10:09.363513 128561009378112 pretrain.py:309] Pretrain iter 00012: 0.0194429\n",
      "I0130 11:10:09.371842 128561009378112 pretrain.py:309] Pretrain iter 00013: 0.0190087\n",
      "I0130 11:10:09.380721 128561009378112 pretrain.py:309] Pretrain iter 00014: 0.0185598\n",
      "I0130 11:10:09.388977 128561009378112 pretrain.py:309] Pretrain iter 00015: 0.018217\n",
      "I0130 11:10:09.397868 128561009378112 pretrain.py:309] Pretrain iter 00016: 0.0178711\n",
      "I0130 11:10:09.406251 128561009378112 pretrain.py:309] Pretrain iter 00017: 0.0175697\n",
      "I0130 11:10:09.415120 128561009378112 pretrain.py:309] Pretrain iter 00018: 0.017231\n",
      "I0130 11:10:09.424594 128561009378112 pretrain.py:309] Pretrain iter 00019: 0.0170555\n",
      "I0130 11:10:09.433415 128561009378112 pretrain.py:309] Pretrain iter 00020: 0.0168826\n",
      "I0130 11:10:09.443211 128561009378112 pretrain.py:309] Pretrain iter 00021: 0.0165295\n",
      "I0130 11:10:09.452588 128561009378112 pretrain.py:309] Pretrain iter 00022: 0.01616\n",
      "I0130 11:10:09.462745 128561009378112 pretrain.py:309] Pretrain iter 00023: 0.016334\n",
      "I0130 11:10:09.471555 128561009378112 pretrain.py:309] Pretrain iter 00024: 0.0166391\n",
      "I0130 11:10:09.480534 128561009378112 pretrain.py:309] Pretrain iter 00025: 0.0160843\n",
      "I0130 11:10:09.489221 128561009378112 pretrain.py:309] Pretrain iter 00026: 0.0159737\n",
      "I0130 11:10:09.498272 128561009378112 pretrain.py:309] Pretrain iter 00027: 0.0156368\n",
      "I0130 11:10:09.508390 128561009378112 pretrain.py:309] Pretrain iter 00028: 0.0152168\n",
      "I0130 11:10:09.518062 128561009378112 pretrain.py:309] Pretrain iter 00029: 0.0151722\n",
      "I0130 11:10:09.528068 128561009378112 pretrain.py:309] Pretrain iter 00030: 0.0150882\n",
      "I0130 11:10:09.536905 128561009378112 pretrain.py:309] Pretrain iter 00031: 0.0147018\n",
      "I0130 11:10:09.546246 128561009378112 pretrain.py:309] Pretrain iter 00032: 0.0147784\n",
      "I0130 11:10:09.554640 128561009378112 pretrain.py:309] Pretrain iter 00033: 0.0141484\n",
      "I0130 11:10:09.563685 128561009378112 pretrain.py:309] Pretrain iter 00034: 0.0142556\n",
      "I0130 11:10:09.572151 128561009378112 pretrain.py:309] Pretrain iter 00035: 0.0143725\n",
      "I0130 11:10:09.581073 128561009378112 pretrain.py:309] Pretrain iter 00036: 0.0145937\n",
      "I0130 11:10:09.589236 128561009378112 pretrain.py:309] Pretrain iter 00037: 0.0140925\n",
      "I0130 11:10:09.598371 128561009378112 pretrain.py:309] Pretrain iter 00038: 0.0140952\n",
      "I0130 11:10:09.607289 128561009378112 pretrain.py:309] Pretrain iter 00039: 0.0134903\n",
      "I0130 11:10:09.617635 128561009378112 pretrain.py:309] Pretrain iter 00040: 0.0129965\n",
      "I0130 11:10:09.627395 128561009378112 pretrain.py:309] Pretrain iter 00041: 0.0128566\n",
      "I0130 11:10:09.636204 128561009378112 pretrain.py:309] Pretrain iter 00042: 0.0127309\n",
      "I0130 11:10:09.645587 128561009378112 pretrain.py:309] Pretrain iter 00043: 0.0120961\n",
      "I0130 11:10:09.654449 128561009378112 pretrain.py:309] Pretrain iter 00044: 0.0124397\n",
      "I0130 11:10:09.663454 128561009378112 pretrain.py:309] Pretrain iter 00045: 0.0125216\n",
      "I0130 11:10:09.671894 128561009378112 pretrain.py:309] Pretrain iter 00046: 0.0123644\n",
      "I0130 11:10:09.680789 128561009378112 pretrain.py:309] Pretrain iter 00047: 0.0122325\n",
      "I0130 11:10:09.689126 128561009378112 pretrain.py:309] Pretrain iter 00048: 0.0120051\n",
      "I0130 11:10:09.698114 128561009378112 pretrain.py:309] Pretrain iter 00049: 0.0109424\n",
      "I0130 11:10:09.706941 128561009378112 pretrain.py:309] Pretrain iter 00050: 0.0111337\n",
      "I0130 11:10:09.716390 128561009378112 pretrain.py:309] Pretrain iter 00051: 0.0106652\n",
      "I0130 11:10:09.726635 128561009378112 pretrain.py:309] Pretrain iter 00052: 0.0100901\n",
      "I0130 11:10:09.735793 128561009378112 pretrain.py:309] Pretrain iter 00053: 0.00992842\n",
      "I0130 11:10:09.745298 128561009378112 pretrain.py:309] Pretrain iter 00054: 0.00970881\n",
      "I0130 11:10:09.754378 128561009378112 pretrain.py:309] Pretrain iter 00055: 0.00945027\n",
      "I0130 11:10:09.764220 128561009378112 pretrain.py:309] Pretrain iter 00056: 0.00936782\n",
      "I0130 11:10:09.775032 128561009378112 pretrain.py:309] Pretrain iter 00057: 0.00924092\n",
      "I0130 11:10:09.784433 128561009378112 pretrain.py:309] Pretrain iter 00058: 0.00962423\n",
      "I0130 11:10:09.794015 128561009378112 pretrain.py:309] Pretrain iter 00059: 0.00951692\n",
      "I0130 11:10:09.803149 128561009378112 pretrain.py:309] Pretrain iter 00060: 0.00908535\n",
      "I0130 11:10:09.812786 128561009378112 pretrain.py:309] Pretrain iter 00061: 0.00882167\n",
      "I0130 11:10:09.821332 128561009378112 pretrain.py:309] Pretrain iter 00062: 0.0085543\n",
      "I0130 11:10:09.830219 128561009378112 pretrain.py:309] Pretrain iter 00063: 0.00840836\n",
      "I0130 11:10:09.838600 128561009378112 pretrain.py:309] Pretrain iter 00064: 0.00843828\n",
      "I0130 11:10:09.847501 128561009378112 pretrain.py:309] Pretrain iter 00065: 0.00831778\n",
      "I0130 11:10:09.855746 128561009378112 pretrain.py:309] Pretrain iter 00066: 0.00821102\n",
      "I0130 11:10:09.864814 128561009378112 pretrain.py:309] Pretrain iter 00067: 0.00827662\n",
      "I0130 11:10:09.873342 128561009378112 pretrain.py:309] Pretrain iter 00068: 0.00833808\n",
      "I0130 11:10:09.882702 128561009378112 pretrain.py:309] Pretrain iter 00069: 0.00845393\n",
      "I0130 11:10:09.892184 128561009378112 pretrain.py:309] Pretrain iter 00070: 0.00842013\n",
      "I0130 11:10:09.901035 128561009378112 pretrain.py:309] Pretrain iter 00071: 0.00832474\n",
      "I0130 11:10:09.910640 128561009378112 pretrain.py:309] Pretrain iter 00072: 0.00836503\n",
      "I0130 11:10:09.919672 128561009378112 pretrain.py:309] Pretrain iter 00073: 0.00878689\n",
      "I0130 11:10:09.929221 128561009378112 pretrain.py:309] Pretrain iter 00074: 0.00881717\n",
      "I0130 11:10:09.937764 128561009378112 pretrain.py:309] Pretrain iter 00075: 0.00877872\n",
      "I0130 11:10:09.946765 128561009378112 pretrain.py:309] Pretrain iter 00076: 0.00859367\n",
      "I0130 11:10:09.954958 128561009378112 pretrain.py:309] Pretrain iter 00077: 0.00812064\n",
      "I0130 11:10:09.964083 128561009378112 pretrain.py:309] Pretrain iter 00078: 0.00853121\n",
      "I0130 11:10:09.972590 128561009378112 pretrain.py:309] Pretrain iter 00079: 0.00821267\n",
      "I0130 11:10:09.981614 128561009378112 pretrain.py:309] Pretrain iter 00080: 0.00797733\n",
      "I0130 11:10:09.989914 128561009378112 pretrain.py:309] Pretrain iter 00081: 0.00800031\n",
      "I0130 11:10:09.999357 128561009378112 pretrain.py:309] Pretrain iter 00082: 0.00778526\n",
      "I0130 11:10:10.008875 128561009378112 pretrain.py:309] Pretrain iter 00083: 0.0075545\n",
      "I0130 11:10:10.017927 128561009378112 pretrain.py:309] Pretrain iter 00084: 0.00769813\n",
      "I0130 11:10:10.027644 128561009378112 pretrain.py:309] Pretrain iter 00085: 0.008386\n",
      "I0130 11:10:10.036646 128561009378112 pretrain.py:309] Pretrain iter 00086: 0.00800508\n",
      "I0130 11:10:10.046135 128561009378112 pretrain.py:309] Pretrain iter 00087: 0.00796726\n",
      "I0130 11:10:10.054533 128561009378112 pretrain.py:309] Pretrain iter 00088: 0.00801217\n",
      "I0130 11:10:10.063507 128561009378112 pretrain.py:309] Pretrain iter 00089: 0.00831398\n",
      "I0130 11:10:10.071974 128561009378112 pretrain.py:309] Pretrain iter 00090: 0.00881701\n",
      "I0130 11:10:10.080837 128561009378112 pretrain.py:309] Pretrain iter 00091: 0.00899456\n",
      "I0130 11:10:10.089177 128561009378112 pretrain.py:309] Pretrain iter 00092: 0.00930659\n",
      "I0130 11:10:10.098129 128561009378112 pretrain.py:309] Pretrain iter 00093: 0.0103321\n",
      "I0130 11:10:10.106353 128561009378112 pretrain.py:309] Pretrain iter 00094: 0.0108481\n",
      "I0130 11:10:10.115455 128561009378112 pretrain.py:309] Pretrain iter 00095: 0.0111265\n",
      "I0130 11:10:10.125272 128561009378112 pretrain.py:309] Pretrain iter 00096: 0.0110537\n",
      "I0130 11:10:10.134327 128561009378112 pretrain.py:309] Pretrain iter 00097: 0.0109863\n",
      "I0130 11:10:10.143854 128561009378112 pretrain.py:309] Pretrain iter 00098: 0.0103605\n",
      "I0130 11:10:10.152620 128561009378112 pretrain.py:309] Pretrain iter 00099: 0.0101476\n",
      "I0130 11:10:10.923104 128561009378112 tag_graph_matcher.py:1313] ==================================================\n",
      "I0130 11:10:10.923183 128561009378112 tag_graph_matcher.py:1314] Graph parameter registrations:\n",
      "I0130 11:10:10.923547 128561009378112 tag_graph_matcher.py:1315] {'envelope': [{'pi': 'Auto[scale_and_shift_tag_1]',\n",
      "               'sigma': 'Auto[scale_and_shift_tag_0]'},\n",
      "              {'pi': 'Auto[scale_and_shift_tag_3]',\n",
      "               'sigma': 'Auto[scale_and_shift_tag_2]'}],\n",
      " 'layers': {'input': {},\n",
      "            'streams': [{'double': {'b': 'Auto[repeated_dense_tag_1]',\n",
      "                                    'w': 'Auto[repeated_dense_tag_1]'},\n",
      "                         'single': {'b': 'Auto[repeated_dense_tag_0]',\n",
      "                                    'w': 'Auto[repeated_dense_tag_0]'}},\n",
      "                        {'double': {'b': 'Auto[repeated_dense_tag_3]',\n",
      "                                    'w': 'Auto[repeated_dense_tag_3]'},\n",
      "                         'single': {'b': 'Auto[repeated_dense_tag_2]',\n",
      "                                    'w': 'Auto[repeated_dense_tag_2]'}},\n",
      "                        {'double': {'b': 'Auto[repeated_dense_tag_5]',\n",
      "                                    'w': 'Auto[repeated_dense_tag_5]'},\n",
      "                         'single': {'b': 'Auto[repeated_dense_tag_4]',\n",
      "                                    'w': 'Auto[repeated_dense_tag_4]'}},\n",
      "                        {'single': {'b': 'Auto[repeated_dense_tag_6]',\n",
      "                                    'w': 'Auto[repeated_dense_tag_6]'}}]},\n",
      " 'orbital': [{'w': 'Auto[repeated_dense_tag_7]'},\n",
      "             {'w': 'Auto[repeated_dense_tag_8]'}]}\n",
      "I0130 11:10:10.923584 128561009378112 tag_graph_matcher.py:1317] ==================================================\n",
      "I0130 11:10:11.134528 128561009378112 train.py:803] Burning in MCMC chain for 100 steps\n",
      "I0130 11:10:15.021322 128561009378112 train.py:816] Completed burn-in MCMC steps\n",
      "I0130 11:10:17.546738 128561009378112 train.py:820] Initial energy: -60.3582 E_h\n",
      "I0130 11:10:27.439290 128561009378112 train.py:917] Step 00000: -59.4301 E_h, exp. variance=-0.0000 E_h^2, pmove=0.92\n",
      "I0130 11:10:27.803053 128561009378112 train.py:917] Step 00001: -60.2005 E_h, exp. variance=0.0534 E_h^2, pmove=0.91\n",
      "I0130 11:10:28.097202 128561009378112 train.py:917] Step 00002: -60.7596 E_h, exp. variance=0.1892 E_h^2, pmove=0.91\n",
      "I0130 11:10:28.394622 128561009378112 train.py:917] Step 00003: -60.8148 E_h, exp. variance=0.2961 E_h^2, pmove=0.92\n",
      "I0130 11:10:28.686748 128561009378112 train.py:917] Step 00004: -60.6478 E_h, exp. variance=0.3390 E_h^2, pmove=0.90\n",
      "I0130 11:10:28.981276 128561009378112 train.py:917] Step 00005: -60.0856 E_h, exp. variance=0.3105 E_h^2, pmove=0.92\n",
      "I0130 11:10:29.275296 128561009378112 train.py:917] Step 00006: -59.3190 E_h, exp. variance=0.3063 E_h^2, pmove=0.91\n",
      "I0130 11:10:29.564094 128561009378112 train.py:917] Step 00007: -59.8095 E_h, exp. variance=0.2756 E_h^2, pmove=0.91\n",
      "I0130 11:10:29.854061 128561009378112 train.py:917] Step 00008: -61.1167 E_h, exp. variance=0.4017 E_h^2, pmove=0.91\n",
      "I0130 11:10:30.142538 128561009378112 train.py:917] Step 00009: -60.4527 E_h, exp. variance=0.3851 E_h^2, pmove=0.92\n",
      "I0130 11:10:30.434254 128561009378112 train.py:917] Step 00010: -59.2492 E_h, exp. variance=0.3963 E_h^2, pmove=0.91\n",
      "I0130 11:10:30.726026 128561009378112 train.py:917] Step 00011: -61.1092 E_h, exp. variance=0.4844 E_h^2, pmove=0.90\n",
      "I0130 11:10:31.014950 128561009378112 train.py:917] Step 00012: -61.3913 E_h, exp. variance=0.6011 E_h^2, pmove=0.91\n",
      "I0130 11:10:31.305968 128561009378112 train.py:917] Step 00013: -60.8754 E_h, exp. variance=0.5854 E_h^2, pmove=0.89\n",
      "I0130 11:10:31.593569 128561009378112 train.py:917] Step 00014: -63.0148 E_h, exp. variance=1.2185 E_h^2, pmove=0.91\n",
      "I0130 11:10:31.882862 128561009378112 train.py:917] Step 00015: -62.7073 E_h, exp. variance=1.5273 E_h^2, pmove=0.91\n",
      "I0130 11:10:32.169772 128561009378112 train.py:917] Step 00016: -64.4854 E_h, exp. variance=2.6380 E_h^2, pmove=0.91\n",
      "I0130 11:10:32.464329 128561009378112 train.py:917] Step 00017: -63.3669 E_h, exp. variance=2.8313 E_h^2, pmove=0.91\n",
      "I0130 11:10:32.752724 128561009378112 train.py:917] Step 00018: -62.7822 E_h, exp. variance=2.7357 E_h^2, pmove=0.90\n",
      "I0130 11:10:33.042815 128561009378112 train.py:917] Step 00019: -62.9874 E_h, exp. variance=2.6659 E_h^2, pmove=0.91\n",
      "I0130 11:10:33.334295 128561009378112 train.py:917] Step 00020: -63.5586 E_h, exp. variance=2.7329 E_h^2, pmove=0.90\n",
      "I0130 11:10:33.622600 128561009378112 train.py:917] Step 00021: -63.2826 E_h, exp. variance=2.6505 E_h^2, pmove=0.90\n",
      "I0130 11:10:33.912448 128561009378112 train.py:917] Step 00022: -63.2072 E_h, exp. variance=2.5229 E_h^2, pmove=0.91\n",
      "I0130 11:10:34.199336 128561009378112 train.py:917] Step 00023: -63.4771 E_h, exp. variance=2.4425 E_h^2, pmove=0.90\n",
      "I0130 11:10:34.489543 128561009378112 train.py:917] Step 00024: -66.9860 E_h, exp. variance=4.2311 E_h^2, pmove=0.89\n",
      "I0130 11:10:34.779686 128561009378112 train.py:917] Step 00025: -64.3974 E_h, exp. variance=4.0647 E_h^2, pmove=0.91\n",
      "I0130 11:10:35.066290 128561009378112 train.py:917] Step 00026: -66.6541 E_h, exp. variance=4.9418 E_h^2, pmove=0.90\n",
      "I0130 11:10:35.356307 128561009378112 train.py:917] Step 00027: -65.7825 E_h, exp. variance=5.0225 E_h^2, pmove=0.89\n",
      "I0130 11:10:35.645786 128561009378112 train.py:917] Step 00028: -64.8255 E_h, exp. variance=4.6765 E_h^2, pmove=0.90\n",
      "I0130 11:10:35.933933 128561009378112 train.py:917] Step 00029: -65.1892 E_h, exp. variance=4.4250 E_h^2, pmove=0.90\n",
      "I0130 11:10:36.222614 128561009378112 train.py:917] Step 00030: -65.8500 E_h, exp. variance=4.3627 E_h^2, pmove=0.89\n",
      "I0130 11:10:36.515822 128561009378112 train.py:917] Step 00031: -67.3053 E_h, exp. variance=4.9095 E_h^2, pmove=0.89\n",
      "I0130 11:10:36.804469 128561009378112 train.py:917] Step 00032: -66.4018 E_h, exp. variance=4.8046 E_h^2, pmove=0.90\n",
      "I0130 11:10:37.092561 128561009378112 train.py:917] Step 00033: -68.3602 E_h, exp. variance=5.6392 E_h^2, pmove=0.89\n",
      "I0130 11:10:37.383563 128561009378112 train.py:917] Step 00034: -66.8963 E_h, exp. variance=5.4267 E_h^2, pmove=0.89\n",
      "I0130 11:10:37.672403 128561009378112 train.py:917] Step 00035: -66.6717 E_h, exp. variance=5.1014 E_h^2, pmove=0.90\n",
      "I0130 11:10:37.962040 128561009378112 train.py:917] Step 00036: -67.1972 E_h, exp. variance=4.9245 E_h^2, pmove=0.89\n",
      "I0130 11:10:38.249281 128561009378112 train.py:917] Step 00037: -67.5470 E_h, exp. variance=4.8220 E_h^2, pmove=0.89\n",
      "I0130 11:10:38.541649 128561009378112 train.py:917] Step 00038: -70.0572 E_h, exp. variance=6.0692 E_h^2, pmove=0.90\n",
      "I0130 11:10:38.830920 128561009378112 train.py:917] Step 00039: -68.2018 E_h, exp. variance=5.8554 E_h^2, pmove=0.89\n",
      "I0130 11:10:39.117699 128561009378112 train.py:917] Step 00040: -68.4063 E_h, exp. variance=5.6612 E_h^2, pmove=0.89\n",
      "I0130 11:10:39.410044 128561009378112 train.py:917] Step 00041: -70.3672 E_h, exp. variance=6.4206 E_h^2, pmove=0.89\n",
      "I0130 11:10:39.698969 128561009378112 train.py:917] Step 00042: -70.5922 E_h, exp. variance=6.9967 E_h^2, pmove=0.88\n",
      "I0130 11:10:39.987312 128561009378112 train.py:917] Step 00043: -70.7161 E_h, exp. variance=7.3589 E_h^2, pmove=0.89\n",
      "I0130 11:10:40.277594 128561009378112 train.py:917] Step 00044: -70.3462 E_h, exp. variance=7.2896 E_h^2, pmove=0.89\n",
      "I0130 11:10:40.568176 128561009378112 train.py:917] Step 00045: -71.3006 E_h, exp. variance=7.6034 E_h^2, pmove=0.88\n",
      "I0130 11:10:40.858542 128561009378112 train.py:917] Step 00046: -73.7702 E_h, exp. variance=9.5984 E_h^2, pmove=0.88\n",
      "I0130 11:10:41.146055 128561009378112 train.py:917] Step 00047: -71.5768 E_h, exp. variance=9.3373 E_h^2, pmove=0.88\n",
      "I0130 11:10:41.434471 128561009378112 train.py:917] Step 00048: -74.9145 E_h, exp. variance=11.4787 E_h^2, pmove=0.88\n",
      "I0130 11:10:41.725994 128561009378112 train.py:917] Step 00049: -73.1207 E_h, exp. variance=11.4127 E_h^2, pmove=0.88\n",
      "I0130 11:10:42.014564 128561009378112 train.py:917] Step 00050: -72.9216 E_h, exp. variance=11.0394 E_h^2, pmove=0.88\n",
      "I0130 11:10:42.302829 128561009378112 train.py:917] Step 00051: -75.2442 E_h, exp. variance=12.1422 E_h^2, pmove=0.88\n",
      "I0130 11:10:42.595329 128561009378112 train.py:917] Step 00052: -77.4715 E_h, exp. variance=14.9486 E_h^2, pmove=0.89\n",
      "I0130 11:10:42.884198 128561009378112 train.py:917] Step 00053: -76.7951 E_h, exp. variance=16.0193 E_h^2, pmove=0.88\n",
      "I0130 11:10:43.172781 128561009378112 train.py:917] Step 00054: -75.7767 E_h, exp. variance=15.7079 E_h^2, pmove=0.88\n",
      "I0130 11:10:43.462216 128561009378112 train.py:917] Step 00055: -74.2761 E_h, exp. variance=14.4646 E_h^2, pmove=0.87\n",
      "I0130 11:10:43.751296 128561009378112 train.py:917] Step 00056: -76.9598 E_h, exp. variance=14.7608 E_h^2, pmove=0.87\n",
      "I0130 11:10:44.041540 128561009378112 train.py:917] Step 00057: -77.5475 E_h, exp. variance=15.1465 E_h^2, pmove=0.89\n",
      "I0130 11:10:44.330522 128561009378112 train.py:917] Step 00058: -77.1866 E_h, exp. variance=14.8856 E_h^2, pmove=0.87\n",
      "I0130 11:10:44.620612 128561009378112 train.py:917] Step 00059: -78.0639 E_h, exp. variance=15.0123 E_h^2, pmove=0.89\n",
      "I0130 11:10:44.910904 128561009378112 train.py:917] Step 00060: -80.9042 E_h, exp. variance=17.4949 E_h^2, pmove=0.89\n",
      "I0130 11:10:45.199082 128561009378112 train.py:917] Step 00061: -80.3934 E_h, exp. variance=18.4451 E_h^2, pmove=0.87\n",
      "I0130 11:10:45.486790 128561009378112 train.py:917] Step 00062: -80.7317 E_h, exp. variance=19.0978 E_h^2, pmove=0.89\n",
      "I0130 11:10:45.780999 128561009378112 train.py:917] Step 00063: -82.0204 E_h, exp. variance=20.4601 E_h^2, pmove=0.89\n",
      "I0130 11:10:46.078541 128561009378112 train.py:917] Step 00064: -81.3465 E_h, exp. variance=20.4470 E_h^2, pmove=0.88\n",
      "I0130 11:10:46.372049 128561009378112 train.py:917] Step 00065: -81.7954 E_h, exp. variance=20.4127 E_h^2, pmove=0.87\n",
      "I0130 11:10:46.666558 128561009378112 train.py:917] Step 00066: -81.0287 E_h, exp. variance=19.4657 E_h^2, pmove=0.87\n",
      "I0130 11:10:46.961412 128561009378112 train.py:917] Step 00067: -82.1709 E_h, exp. variance=19.1681 E_h^2, pmove=0.89\n",
      "I0130 11:10:47.253665 128561009378112 train.py:917] Step 00068: -80.3895 E_h, exp. variance=17.6374 E_h^2, pmove=0.88\n",
      "I0130 11:10:47.549034 128561009378112 train.py:917] Step 00069: -80.7120 E_h, exp. variance=16.3038 E_h^2, pmove=0.87\n",
      "I0130 11:10:47.844361 128561009378112 train.py:917] Step 00070: -81.4222 E_h, exp. variance=15.3189 E_h^2, pmove=0.88\n",
      "I0130 11:10:48.136965 128561009378112 train.py:917] Step 00071: -84.0708 E_h, exp. variance=16.0903 E_h^2, pmove=0.88\n",
      "I0130 11:10:48.431517 128561009378112 train.py:917] Step 00072: -84.2906 E_h, exp. variance=16.5313 E_h^2, pmove=0.87\n",
      "I0130 11:10:48.728624 128561009378112 train.py:917] Step 00073: -83.3189 E_h, exp. variance=15.8724 E_h^2, pmove=0.88\n",
      "I0130 11:10:49.021042 128561009378112 train.py:917] Step 00074: -84.2191 E_h, exp. variance=15.6482 E_h^2, pmove=0.87\n",
      "I0130 11:10:49.314525 128561009378112 train.py:917] Step 00075: -84.0150 E_h, exp. variance=15.0624 E_h^2, pmove=0.88\n",
      "I0130 11:10:49.610152 128561009378112 train.py:917] Step 00076: -85.2195 E_h, exp. variance=15.1235 E_h^2, pmove=0.86\n",
      "I0130 11:10:49.908553 128561009378112 train.py:917] Step 00077: -84.7075 E_h, exp. variance=14.5580 E_h^2, pmove=0.87\n",
      "I0130 11:10:50.200379 128561009378112 train.py:917] Step 00078: -84.4846 E_h, exp. variance=13.7566 E_h^2, pmove=0.87\n",
      "I0130 11:10:50.494486 128561009378112 train.py:917] Step 00079: -85.2815 E_h, exp. variance=13.3162 E_h^2, pmove=0.86\n",
      "I0130 11:10:50.792183 128561009378112 train.py:917] Step 00080: -86.8363 E_h, exp. variance=13.7717 E_h^2, pmove=0.86\n",
      "I0130 11:10:51.085495 128561009378112 train.py:917] Step 00081: -87.0353 E_h, exp. variance=13.9893 E_h^2, pmove=0.88\n",
      "I0130 11:10:51.380140 128561009378112 train.py:917] Step 00082: -85.6761 E_h, exp. variance=13.1216 E_h^2, pmove=0.88\n",
      "I0130 11:10:51.674974 128561009378112 train.py:917] Step 00083: -85.0276 E_h, exp. variance=12.0223 E_h^2, pmove=0.86\n",
      "I0130 11:10:51.968798 128561009378112 train.py:917] Step 00084: -85.2607 E_h, exp. variance=11.0554 E_h^2, pmove=0.87\n",
      "I0130 11:10:52.263365 128561009378112 train.py:917] Step 00085: -88.4339 E_h, exp. variance=11.8781 E_h^2, pmove=0.86\n",
      "I0130 11:10:52.555540 128561009378112 train.py:917] Step 00086: -86.8311 E_h, exp. variance=11.2815 E_h^2, pmove=0.86\n",
      "I0130 11:10:52.849456 128561009378112 train.py:917] Step 00087: -86.5307 E_h, exp. variance=10.5156 E_h^2, pmove=0.86\n",
      "I0130 11:10:53.139519 128561009378112 train.py:917] Step 00088: -87.5744 E_h, exp. variance=10.1948 E_h^2, pmove=0.87\n",
      "I0130 11:10:53.428830 128561009378112 train.py:917] Step 00089: -89.8224 E_h, exp. variance=11.2597 E_h^2, pmove=0.87\n",
      "I0130 11:10:53.720005 128561009378112 train.py:917] Step 00090: -90.2416 E_h, exp. variance=12.1647 E_h^2, pmove=0.86\n",
      "I0130 11:10:54.012229 128561009378112 train.py:917] Step 00091: -89.2602 E_h, exp. variance=11.9248 E_h^2, pmove=0.86\n",
      "I0130 11:10:54.301404 128561009378112 train.py:917] Step 00092: -88.6144 E_h, exp. variance=11.2162 E_h^2, pmove=0.86\n",
      "I0130 11:10:54.592656 128561009378112 train.py:917] Step 00093: -91.7502 E_h, exp. variance=12.5495 E_h^2, pmove=0.87\n",
      "I0130 11:10:54.888912 128561009378112 train.py:917] Step 00094: -90.7194 E_h, exp. variance=12.5065 E_h^2, pmove=0.87\n",
      "I0130 11:10:55.179450 128561009378112 train.py:917] Step 00095: -91.1474 E_h, exp. variance=12.5084 E_h^2, pmove=0.87\n",
      "I0130 11:10:55.468137 128561009378112 train.py:917] Step 00096: -91.4047 E_h, exp. variance=12.4337 E_h^2, pmove=0.86\n",
      "I0130 11:10:55.761427 128561009378112 train.py:917] Step 00097: -91.1037 E_h, exp. variance=11.9748 E_h^2, pmove=0.87\n",
      "I0130 11:10:56.049679 128561009378112 train.py:917] Step 00098: -93.2222 E_h, exp. variance=12.8300 E_h^2, pmove=0.86\n",
      "I0130 11:10:56.339366 128561009378112 train.py:917] Step 00099: -92.8703 E_h, exp. variance=12.9485 E_h^2, pmove=0.86\n",
      "I0130 11:10:56.657407 128561009378112 train.py:917] Step 00100: -90.5748 E_h, exp. variance=11.7956 E_h^2, pmove=0.87\n",
      "I0130 11:10:56.964731 128561009378112 train.py:917] Step 00101: -91.0987 E_h, exp. variance=10.8624 E_h^2, pmove=0.86\n",
      "I0130 11:10:57.254200 128561009378112 train.py:917] Step 00102: -93.1613 E_h, exp. variance=10.9114 E_h^2, pmove=0.84\n",
      "I0130 11:10:57.544583 128561009378112 train.py:917] Step 00103: -91.8033 E_h, exp. variance=10.1244 E_h^2, pmove=0.84\n",
      "I0130 11:10:57.837580 128561009378112 train.py:917] Step 00104: -96.1253 E_h, exp. variance=12.3266 E_h^2, pmove=0.84\n",
      "I0130 11:10:58.127283 128561009378112 train.py:917] Step 00105: -93.8763 E_h, exp. variance=11.9756 E_h^2, pmove=0.84\n",
      "I0130 11:10:58.416924 128561009378112 train.py:917] Step 00106: -95.9185 E_h, exp. variance=12.9031 E_h^2, pmove=0.84\n",
      "I0130 11:10:58.708900 128561009378112 train.py:917] Step 00107: -97.0760 E_h, exp. variance=14.3657 E_h^2, pmove=0.84\n",
      "I0130 11:10:59.002481 128561009378112 train.py:917] Step 00108: -96.2137 E_h, exp. variance=14.4534 E_h^2, pmove=0.84\n",
      "I0130 11:10:59.295259 128561009378112 train.py:917] Step 00109: -98.3360 E_h, exp. variance=16.0629 E_h^2, pmove=0.86\n",
      "I0130 11:10:59.583526 128561009378112 train.py:917] Step 00110: -95.8414 E_h, exp. variance=15.1367 E_h^2, pmove=0.84\n",
      "I0130 11:10:59.879619 128561009378112 train.py:917] Step 00111: -94.8465 E_h, exp. variance=13.8199 E_h^2, pmove=0.83\n",
      "I0130 11:11:00.168559 128561009378112 train.py:917] Step 00112: -96.4082 E_h, exp. variance=13.1911 E_h^2, pmove=0.86\n",
      "I0130 11:11:00.460945 128561009378112 train.py:917] Step 00113: -97.2517 E_h, exp. variance=12.9414 E_h^2, pmove=0.84\n",
      "I0130 11:11:00.751103 128561009378112 train.py:917] Step 00114: -99.7293 E_h, exp. variance=14.4496 E_h^2, pmove=0.84\n",
      "I0130 11:11:01.043983 128561009378112 train.py:917] Step 00115: -98.2865 E_h, exp. variance=14.1576 E_h^2, pmove=0.85\n",
      "I0130 11:11:01.333856 128561009378112 train.py:917] Step 00116: -96.9037 E_h, exp. variance=13.0460 E_h^2, pmove=0.83\n",
      "I0130 11:11:01.624886 128561009378112 train.py:917] Step 00117: -99.7260 E_h, exp. variance=13.5453 E_h^2, pmove=0.84\n",
      "I0130 11:11:01.917916 128561009378112 train.py:917] Step 00118: -100.3555 E_h, exp. variance=14.1441 E_h^2, pmove=0.85\n",
      "I0130 11:11:02.209914 128561009378112 train.py:917] Step 00119: -101.7401 E_h, exp. variance=15.5294 E_h^2, pmove=0.84\n",
      "I0130 11:11:02.499239 128561009378112 train.py:917] Step 00120: -100.3467 E_h, exp. variance=15.1600 E_h^2, pmove=0.83\n",
      "I0130 11:11:02.789753 128561009378112 train.py:917] Step 00121: -103.8739 E_h, exp. variance=17.7944 E_h^2, pmove=0.82\n",
      "I0130 11:11:03.081216 128561009378112 train.py:917] Step 00122: -101.1256 E_h, exp. variance=17.0331 E_h^2, pmove=0.85\n",
      "I0130 11:11:03.372214 128561009378112 train.py:917] Step 00123: -101.0116 E_h, exp. variance=16.0936 E_h^2, pmove=0.84\n",
      "I0130 11:11:03.662536 128561009378112 train.py:917] Step 00124: -102.6710 E_h, exp. variance=16.1339 E_h^2, pmove=0.84\n",
      "I0130 11:11:03.955490 128561009378112 train.py:917] Step 00125: -100.8404 E_h, exp. variance=14.8886 E_h^2, pmove=0.83\n",
      "I0130 11:11:04.246714 128561009378112 train.py:917] Step 00126: -102.9558 E_h, exp. variance=14.7938 E_h^2, pmove=0.84\n",
      "I0130 11:11:04.535285 128561009378112 train.py:917] Step 00127: -102.5772 E_h, exp. variance=14.2151 E_h^2, pmove=0.83\n",
      "I0130 11:11:04.827167 128561009378112 train.py:917] Step 00128: -103.2901 E_h, exp. variance=13.9343 E_h^2, pmove=0.82\n",
      "I0130 11:11:05.120900 128561009378112 train.py:917] Step 00129: -102.1960 E_h, exp. variance=12.9415 E_h^2, pmove=0.84\n",
      "I0130 11:11:05.411807 128561009378112 train.py:917] Step 00130: -100.2873 E_h, exp. variance=11.6474 E_h^2, pmove=0.83\n",
      "I0130 11:11:05.701816 128561009378112 train.py:917] Step 00131: -101.9071 E_h, exp. variance=10.7162 E_h^2, pmove=0.83\n",
      "I0130 11:11:05.996557 128561009378112 train.py:917] Step 00132: -103.5261 E_h, exp. variance=10.4922 E_h^2, pmove=0.83\n",
      "I0130 11:11:06.286490 128561009378112 train.py:917] Step 00133: -101.1777 E_h, exp. variance=9.4584 E_h^2, pmove=0.85\n",
      "I0130 11:11:06.577173 128561009378112 train.py:917] Step 00134: -103.9984 E_h, exp. variance=9.4301 E_h^2, pmove=0.83\n",
      "I0130 11:11:06.868554 128561009378112 train.py:917] Step 00135: -104.6679 E_h, exp. variance=9.6170 E_h^2, pmove=0.83\n",
      "I0130 11:11:07.163064 128561009378112 train.py:917] Step 00136: -105.2921 E_h, exp. variance=9.9638 E_h^2, pmove=0.84\n",
      "I0130 11:11:07.451924 128561009378112 train.py:917] Step 00137: -104.4539 E_h, exp. variance=9.5728 E_h^2, pmove=0.82\n",
      "I0130 11:11:07.744269 128561009378112 train.py:917] Step 00138: -105.2884 E_h, exp. variance=9.5192 E_h^2, pmove=0.84\n",
      "I0130 11:11:08.036580 128561009378112 train.py:917] Step 00139: -106.9306 E_h, exp. variance=10.3849 E_h^2, pmove=0.81\n",
      "I0130 11:11:08.329510 128561009378112 train.py:917] Step 00140: -106.0564 E_h, exp. variance=10.2510 E_h^2, pmove=0.83\n",
      "I0130 11:11:08.617944 128561009378112 train.py:917] Step 00141: -106.4573 E_h, exp. variance=10.1790 E_h^2, pmove=0.83\n",
      "I0130 11:11:08.911413 128561009378112 train.py:917] Step 00142: -104.0397 E_h, exp. variance=9.1846 E_h^2, pmove=0.81\n",
      "I0130 11:11:09.204419 128561009378112 train.py:917] Step 00143: -104.8390 E_h, exp. variance=8.4089 E_h^2, pmove=0.83\n",
      "I0130 11:11:09.494550 128561009378112 train.py:917] Step 00144: -105.2423 E_h, exp. variance=7.7806 E_h^2, pmove=0.83\n",
      "I0130 11:11:09.784983 128561009378112 train.py:917] Step 00145: -105.2965 E_h, exp. variance=7.1885 E_h^2, pmove=0.82\n",
      "I0130 11:11:10.079087 128561009378112 train.py:917] Step 00146: -105.5620 E_h, exp. variance=6.6884 E_h^2, pmove=0.82\n",
      "I0130 11:11:10.367861 128561009378112 train.py:917] Step 00147: -110.2972 E_h, exp. variance=9.4107 E_h^2, pmove=0.82\n",
      "I0130 11:11:10.659792 128561009378112 train.py:917] Step 00148: -107.8807 E_h, exp. variance=9.3390 E_h^2, pmove=0.82\n",
      "I0130 11:11:10.950509 128561009378112 train.py:917] Step 00149: -110.2612 E_h, exp. variance=10.8179 E_h^2, pmove=0.81\n",
      "I0130 11:11:11.245296 128561009378112 train.py:917] Step 00150: -110.4635 E_h, exp. variance=11.8638 E_h^2, pmove=0.82\n",
      "I0130 11:11:11.535214 128561009378112 train.py:917] Step 00151: -108.4477 E_h, exp. variance=11.1788 E_h^2, pmove=0.80\n",
      "I0130 11:11:11.828565 128561009378112 train.py:917] Step 00152: -108.1300 E_h, exp. variance=10.3546 E_h^2, pmove=0.82\n",
      "I0130 11:11:12.118765 128561009378112 train.py:917] Step 00153: -110.4681 E_h, exp. variance=10.7333 E_h^2, pmove=0.81\n",
      "I0130 11:11:12.410992 128561009378112 train.py:917] Step 00154: -112.6552 E_h, exp. variance=12.6404 E_h^2, pmove=0.82\n",
      "I0130 11:11:12.700504 128561009378112 train.py:917] Step 00155: -109.9905 E_h, exp. variance=11.9454 E_h^2, pmove=0.81\n",
      "I0130 11:11:12.993880 128561009378112 train.py:917] Step 00156: -115.2759 E_h, exp. variance=15.8790 E_h^2, pmove=0.81\n",
      "I0130 11:11:13.286590 128561009378112 train.py:917] Step 00157: -112.4217 E_h, exp. variance=15.6878 E_h^2, pmove=0.80\n",
      "I0130 11:11:13.577963 128561009378112 train.py:917] Step 00158: -114.0603 E_h, exp. variance=16.5378 E_h^2, pmove=0.82\n",
      "I0130 11:11:13.868555 128561009378112 train.py:917] Step 00159: -113.3450 E_h, exp. variance=16.2885 E_h^2, pmove=0.81\n",
      "I0130 11:11:14.161597 128561009378112 train.py:917] Step 00160: -110.6127 E_h, exp. variance=14.7206 E_h^2, pmove=0.81\n",
      "I0130 11:11:14.451664 128561009378112 train.py:917] Step 00161: -114.2646 E_h, exp. variance=14.9851 E_h^2, pmove=0.81\n",
      "I0130 11:11:14.743520 128561009378112 train.py:917] Step 00162: -114.5091 E_h, exp. variance=15.0725 E_h^2, pmove=0.81\n",
      "I0130 11:11:15.033712 128561009378112 train.py:917] Step 00163: -113.0548 E_h, exp. variance=14.0513 E_h^2, pmove=0.80\n",
      "I0130 11:11:15.328035 128561009378112 train.py:917] Step 00164: -115.1360 E_h, exp. variance=14.2131 E_h^2, pmove=0.81\n",
      "I0130 11:11:15.617001 128561009378112 train.py:917] Step 00165: -114.1246 E_h, exp. variance=13.4694 E_h^2, pmove=0.81\n",
      "I0130 11:11:15.910670 128561009378112 train.py:917] Step 00166: -115.6833 E_h, exp. variance=13.5829 E_h^2, pmove=0.83\n",
      "I0130 11:11:16.203262 128561009378112 train.py:917] Step 00167: -120.1546 E_h, exp. variance=18.1248 E_h^2, pmove=0.79\n",
      "I0130 11:11:16.494788 128561009378112 train.py:917] Step 00168: -115.4112 E_h, exp. variance=16.8946 E_h^2, pmove=0.79\n",
      "I0130 11:11:16.786205 128561009378112 train.py:917] Step 00169: -115.8194 E_h, exp. variance=15.8600 E_h^2, pmove=0.80\n",
      "I0130 11:11:17.081428 128561009378112 train.py:917] Step 00170: -114.9799 E_h, exp. variance=14.5010 E_h^2, pmove=0.79\n",
      "I0130 11:11:17.375472 128561009378112 train.py:917] Step 00171: -115.0807 E_h, exp. variance=13.2616 E_h^2, pmove=0.79\n",
      "I0130 11:11:17.665603 128561009378112 train.py:917] Step 00172: -116.5155 E_h, exp. variance=12.6471 E_h^2, pmove=0.79\n",
      "I0130 11:11:17.957708 128561009378112 train.py:917] Step 00173: -126.5784 E_h, exp. variance=25.6565 E_h^2, pmove=0.79\n",
      "I0130 11:11:18.250825 128561009378112 train.py:917] Step 00174: -114.6690 E_h, exp. variance=23.1206 E_h^2, pmove=0.79\n",
      "I0130 11:11:18.539183 128561009378112 train.py:917] Step 00175: -115.0126 E_h, exp. variance=20.8112 E_h^2, pmove=0.79\n",
      "I0130 11:11:18.828325 128561009378112 train.py:917] Step 00176: -115.0183 E_h, exp. variance=18.7322 E_h^2, pmove=0.80\n",
      "I0130 11:11:19.115530 128561009378112 train.py:917] Step 00177: -114.5121 E_h, exp. variance=16.8960 E_h^2, pmove=0.79\n",
      "I0130 11:11:19.407172 128561009378112 train.py:917] Step 00178: -115.9963 E_h, exp. variance=15.2804 E_h^2, pmove=0.79\n",
      "I0130 11:11:19.697335 128561009378112 train.py:917] Step 00179: -117.0298 E_h, exp. variance=14.0601 E_h^2, pmove=0.80\n",
      "I0130 11:11:19.985321 128561009378112 train.py:917] Step 00180: -115.8296 E_h, exp. variance=12.6735 E_h^2, pmove=0.79\n",
      "I0130 11:11:20.282281 128561009378112 train.py:917] Step 00181: -116.9173 E_h, exp. variance=11.6101 E_h^2, pmove=0.79\n",
      "I0130 11:11:20.585794 128561009378112 train.py:917] Step 00182: -117.8982 E_h, exp. variance=10.9402 E_h^2, pmove=0.79\n",
      "I0130 11:11:20.886503 128561009378112 train.py:917] Step 00183: -116.7270 E_h, exp. variance=9.9241 E_h^2, pmove=0.79\n",
      "I0130 11:11:21.183508 128561009378112 train.py:917] Step 00184: -119.6771 E_h, exp. variance=10.2231 E_h^2, pmove=0.78\n",
      "I0130 11:11:21.477704 128561009378112 train.py:917] Step 00185: -120.5715 E_h, exp. variance=10.8677 E_h^2, pmove=0.78\n",
      "I0130 11:11:21.769850 128561009378112 train.py:917] Step 00186: -119.6035 E_h, exp. variance=10.5406 E_h^2, pmove=0.77\n",
      "I0130 11:11:22.063032 128561009378112 train.py:917] Step 00187: -117.7579 E_h, exp. variance=9.5397 E_h^2, pmove=0.79\n",
      "I0130 11:11:22.358569 128561009378112 train.py:917] Step 00188: -122.7180 E_h, exp. variance=11.4611 E_h^2, pmove=0.78\n",
      "I0130 11:11:22.638958 128561009378112 train.py:917] Step 00189: -118.1534 E_h, exp. variance=10.3396 E_h^2, pmove=0.80\n",
      "I0130 11:11:22.914839 128561009378112 train.py:917] Step 00190: -123.3969 E_h, exp. variance=12.2438 E_h^2, pmove=0.80\n",
      "I0130 11:11:23.183339 128561009378112 train.py:917] Step 00191: -120.6964 E_h, exp. variance=11.5560 E_h^2, pmove=0.79\n",
      "I0130 11:11:23.453523 128561009378112 train.py:917] Step 00192: -120.5117 E_h, exp. variance=10.7651 E_h^2, pmove=0.78\n",
      "I0130 11:11:23.719245 128561009378112 train.py:917] Step 00193: -120.3434 E_h, exp. variance=9.9317 E_h^2, pmove=0.78\n",
      "I0130 11:11:23.984642 128561009378112 train.py:917] Step 00194: -120.9982 E_h, exp. variance=9.3483 E_h^2, pmove=0.77\n",
      "I0130 11:11:24.269244 128561009378112 train.py:917] Step 00195: -120.7005 E_h, exp. variance=8.6505 E_h^2, pmove=0.77\n",
      "I0130 11:11:24.550867 128561009378112 train.py:917] Step 00196: -120.7097 E_h, exp. variance=7.9798 E_h^2, pmove=0.78\n",
      "I0130 11:11:24.832579 128561009378112 train.py:917] Step 00197: -120.0769 E_h, exp. variance=7.2247 E_h^2, pmove=0.79\n",
      "I0130 11:11:25.097206 128561009378112 train.py:917] Step 00198: -122.0346 E_h, exp. variance=7.1007 E_h^2, pmove=0.77\n",
      "I0130 11:11:25.366590 128561009378112 train.py:917] Step 00199: -120.5435 E_h, exp. variance=6.4525 E_h^2, pmove=0.77\n",
      "I0130 11:11:25.632251 128561009378112 train.py:917] Step 00200: -120.6227 E_h, exp. variance=5.8687 E_h^2, pmove=0.76\n",
      "I0130 11:11:25.901207 128561009378112 train.py:917] Step 00201: -118.3582 E_h, exp. variance=5.4901 E_h^2, pmove=0.75\n",
      "I0130 11:11:26.166458 128561009378112 train.py:917] Step 00202: -118.2449 E_h, exp. variance=5.1388 E_h^2, pmove=0.76\n",
      "I0130 11:11:26.435542 128561009378112 train.py:917] Step 00203: -119.4447 E_h, exp. variance=4.6266 E_h^2, pmove=0.75\n",
      "I0130 11:11:26.700441 128561009378112 train.py:917] Step 00204: -120.3227 E_h, exp. variance=4.2155 E_h^2, pmove=0.77\n",
      "I0130 11:11:26.965539 128561009378112 train.py:917] Step 00205: -118.6845 E_h, exp. variance=3.8763 E_h^2, pmove=0.75\n",
      "I0130 11:11:27.231043 128561009378112 train.py:917] Step 00206: -119.9422 E_h, exp. variance=3.5029 E_h^2, pmove=0.75\n",
      "I0130 11:11:27.500507 128561009378112 train.py:917] Step 00207: -119.5042 E_h, exp. variance=3.1532 E_h^2, pmove=0.75\n",
      "I0130 11:11:27.769488 128561009378112 train.py:917] Step 00208: -120.8015 E_h, exp. variance=2.9728 E_h^2, pmove=0.75\n",
      "I0130 11:11:28.034497 128561009378112 train.py:917] Step 00209: -120.5310 E_h, exp. variance=2.7377 E_h^2, pmove=0.75\n",
      "I0130 11:11:28.313073 128561009378112 train.py:917] Step 00210: -120.0715 E_h, exp. variance=2.4714 E_h^2, pmove=0.77\n",
      "I0130 11:11:28.582293 128561009378112 train.py:917] Step 00211: -118.9941 E_h, exp. variance=2.2844 E_h^2, pmove=0.76\n",
      "I0130 11:11:28.848800 128561009378112 train.py:917] Step 00212: -120.5609 E_h, exp. variance=2.1181 E_h^2, pmove=0.76\n",
      "I0130 11:11:29.135587 128561009378112 train.py:917] Step 00213: -120.6877 E_h, exp. variance=1.9752 E_h^2, pmove=0.76\n",
      "I0130 11:11:29.430668 128561009378112 train.py:917] Step 00214: -120.3746 E_h, exp. variance=1.7979 E_h^2, pmove=0.75\n",
      "I0130 11:11:29.720406 128561009378112 train.py:917] Step 00215: -119.7525 E_h, exp. variance=1.6216 E_h^2, pmove=0.74\n",
      "I0130 11:11:30.010293 128561009378112 train.py:917] Step 00216: -121.1058 E_h, exp. variance=1.5842 E_h^2, pmove=0.74\n",
      "I0130 11:11:30.285890 128561009378112 train.py:917] Step 00217: -121.9723 E_h, exp. variance=1.7597 E_h^2, pmove=0.75\n",
      "I0130 11:11:30.565894 128561009378112 train.py:917] Step 00218: -121.5822 E_h, exp. variance=1.7462 E_h^2, pmove=0.75\n",
      "I0130 11:11:30.839539 128561009378112 train.py:917] Step 00219: -120.8701 E_h, exp. variance=1.5938 E_h^2, pmove=0.76\n",
      "I0130 11:11:31.104110 128561009378112 train.py:917] Step 00220: -122.9248 E_h, exp. variance=1.9978 E_h^2, pmove=0.75\n",
      "I0130 11:11:31.370394 128561009378112 train.py:917] Step 00221: -123.9264 E_h, exp. variance=2.7506 E_h^2, pmove=0.73\n",
      "I0130 11:11:31.644567 128561009378112 train.py:917] Step 00222: -124.2190 E_h, exp. variance=3.4091 E_h^2, pmove=0.74\n",
      "I0130 11:11:31.914738 128561009378112 train.py:917] Step 00223: -122.3592 E_h, exp. variance=3.1653 E_h^2, pmove=0.73\n",
      "I0130 11:11:32.193695 128561009378112 train.py:917] Step 00224: -121.2817 E_h, exp. variance=2.8506 E_h^2, pmove=0.74\n",
      "I0130 11:11:32.465351 128561009378112 train.py:917] Step 00225: -121.1476 E_h, exp. variance=2.5717 E_h^2, pmove=0.73\n",
      "I0130 11:11:32.734466 128561009378112 train.py:917] Step 00226: -122.7062 E_h, exp. variance=2.4720 E_h^2, pmove=0.74\n",
      "I0130 11:11:32.999387 128561009378112 train.py:917] Step 00227: -121.1881 E_h, exp. variance=2.2344 E_h^2, pmove=0.74\n",
      "I0130 11:11:33.269241 128561009378112 train.py:917] Step 00228: -123.2570 E_h, exp. variance=2.2942 E_h^2, pmove=0.75\n",
      "I0130 11:11:33.550042 128561009378112 train.py:917] Step 00229: -123.1129 E_h, exp. variance=2.2546 E_h^2, pmove=0.75\n",
      "I0130 11:11:33.827254 128561009378112 train.py:917] Step 00230: -122.1722 E_h, exp. variance=2.0412 E_h^2, pmove=0.73\n",
      "I0130 11:11:34.096050 128561009378112 train.py:917] Step 00231: -120.0023 E_h, exp. variance=2.1419 E_h^2, pmove=0.73\n",
      "I0130 11:11:34.362150 128561009378112 train.py:917] Step 00232: -121.9915 E_h, exp. variance=1.9377 E_h^2, pmove=0.73\n",
      "I0130 11:11:34.629545 128561009378112 train.py:917] Step 00233: -119.4597 E_h, exp. variance=2.1923 E_h^2, pmove=0.73\n",
      "I0130 11:11:34.895730 128561009378112 train.py:917] Step 00234: -122.9054 E_h, exp. variance=2.1589 E_h^2, pmove=0.75\n",
      "I0130 11:11:35.168661 128561009378112 train.py:917] Step 00235: -122.2705 E_h, exp. variance=1.9820 E_h^2, pmove=0.76\n",
      "I0130 11:11:35.438234 128561009378112 train.py:917] Step 00236: -121.3569 E_h, exp. variance=1.7931 E_h^2, pmove=0.74\n",
      "I0130 11:11:35.706105 128561009378112 train.py:917] Step 00237: -120.5519 E_h, exp. variance=1.7215 E_h^2, pmove=0.74\n",
      "I0130 11:11:35.972168 128561009378112 train.py:917] Step 00238: -119.2511 E_h, exp. variance=2.0194 E_h^2, pmove=0.74\n",
      "I0130 11:11:36.246997 128561009378112 train.py:917] Step 00239: -119.9963 E_h, exp. variance=1.9723 E_h^2, pmove=0.74\n",
      "I0130 11:11:36.521061 128561009378112 train.py:917] Step 00240: -120.9168 E_h, exp. variance=1.7812 E_h^2, pmove=0.73\n",
      "I0130 11:11:36.796597 128561009378112 train.py:917] Step 00241: -121.3161 E_h, exp. variance=1.6055 E_h^2, pmove=0.73\n",
      "I0130 11:11:37.075074 128561009378112 train.py:917] Step 00242: -120.6272 E_h, exp. variance=1.4712 E_h^2, pmove=0.73\n",
      "I0130 11:11:37.353446 128561009378112 train.py:917] Step 00243: -120.9686 E_h, exp. variance=1.3260 E_h^2, pmove=0.75\n",
      "I0130 11:11:37.625452 128561009378112 train.py:917] Step 00244: -123.4316 E_h, exp. variance=1.6832 E_h^2, pmove=0.75\n",
      "I0130 11:11:37.893688 128561009378112 train.py:917] Step 00245: -123.8089 E_h, exp. variance=2.0669 E_h^2, pmove=0.74\n",
      "I0130 11:11:38.158092 128561009378112 train.py:917] Step 00246: -123.4430 E_h, exp. variance=2.1727 E_h^2, pmove=0.74\n",
      "I0130 11:11:38.423612 128561009378112 train.py:917] Step 00247: -123.0477 E_h, exp. variance=2.1032 E_h^2, pmove=0.75\n",
      "I0130 11:11:38.690930 128561009378112 train.py:917] Step 00248: -123.5858 E_h, exp. variance=2.1504 E_h^2, pmove=0.73\n",
      "I0130 11:11:38.962160 128561009378112 train.py:917] Step 00249: -122.2981 E_h, exp. variance=1.9403 E_h^2, pmove=0.74\n",
      "I0130 11:11:39.238809 128561009378112 train.py:917] Step 00250: -123.2373 E_h, exp. variance=1.8654 E_h^2, pmove=0.73\n",
      "I0130 11:11:39.514780 128561009378112 train.py:917] Step 00251: -124.6607 E_h, exp. variance=2.2230 E_h^2, pmove=0.73\n",
      "I0130 11:11:39.812552 128561009378112 train.py:917] Step 00252: -121.3974 E_h, exp. variance=2.1000 E_h^2, pmove=0.75\n",
      "I0130 11:11:40.103484 128561009378112 train.py:917] Step 00253: -121.3922 E_h, exp. variance=1.9713 E_h^2, pmove=0.74\n",
      "I0130 11:11:40.380927 128561009378112 train.py:917] Step 00254: -122.6410 E_h, exp. variance=1.7881 E_h^2, pmove=0.74\n",
      "I0130 11:11:40.671785 128561009378112 train.py:917] Step 00255: -121.0435 E_h, exp. variance=1.7485 E_h^2, pmove=0.74\n",
      "I0130 11:11:40.971368 128561009378112 train.py:917] Step 00256: -121.7173 E_h, exp. variance=1.5915 E_h^2, pmove=0.74\n",
      "I0130 11:11:41.263200 128561009378112 train.py:917] Step 00257: -123.4421 E_h, exp. variance=1.5901 E_h^2, pmove=0.74\n",
      "I0130 11:11:41.551575 128561009378112 train.py:917] Step 00258: -123.1797 E_h, exp. variance=1.5088 E_h^2, pmove=0.74\n",
      "I0130 11:11:41.847042 128561009378112 train.py:917] Step 00259: -120.9708 E_h, exp. variance=1.5275 E_h^2, pmove=0.73\n",
      "I0130 11:11:42.136605 128561009378112 train.py:917] Step 00260: -122.1711 E_h, exp. variance=1.3748 E_h^2, pmove=0.74\n",
      "I0130 11:11:42.428760 128561009378112 train.py:917] Step 00261: -123.1241 E_h, exp. variance=1.3138 E_h^2, pmove=0.74\n",
      "I0130 11:11:42.719631 128561009378112 train.py:917] Step 00262: -122.2119 E_h, exp. variance=1.1830 E_h^2, pmove=0.73\n",
      "I0130 11:11:43.008259 128561009378112 train.py:917] Step 00263: -122.0719 E_h, exp. variance=1.0689 E_h^2, pmove=0.75\n",
      "I0130 11:11:43.295900 128561009378112 train.py:917] Step 00264: -123.9473 E_h, exp. variance=1.2167 E_h^2, pmove=0.73\n",
      "I0130 11:11:43.584459 128561009378112 train.py:917] Step 00265: -123.5832 E_h, exp. variance=1.2140 E_h^2, pmove=0.74\n",
      "I0130 11:11:43.879595 128561009378112 train.py:917] Step 00266: -125.1897 E_h, exp. variance=1.7205 E_h^2, pmove=0.72\n",
      "I0130 11:11:44.170873 128561009378112 train.py:917] Step 00267: -122.7904 E_h, exp. variance=1.5485 E_h^2, pmove=0.74\n",
      "I0130 11:11:44.463335 128561009378112 train.py:917] Step 00268: -122.1426 E_h, exp. variance=1.4338 E_h^2, pmove=0.74\n",
      "I0130 11:11:44.752441 128561009378112 train.py:917] Step 00269: -122.3907 E_h, exp. variance=1.3016 E_h^2, pmove=0.74\n",
      "I0130 11:11:45.041739 128561009378112 train.py:917] Step 00270: -122.9318 E_h, exp. variance=1.1759 E_h^2, pmove=0.73\n",
      "I0130 11:11:45.329252 128561009378112 train.py:917] Step 00271: -124.4527 E_h, exp. variance=1.3253 E_h^2, pmove=0.74\n",
      "I0130 11:11:45.615734 128561009378112 train.py:917] Step 00272: -124.0033 E_h, exp. variance=1.3017 E_h^2, pmove=0.73\n",
      "I0130 11:11:45.905357 128561009378112 train.py:917] Step 00273: -122.4399 E_h, exp. variance=1.2011 E_h^2, pmove=0.74\n",
      "I0130 11:11:46.193115 128561009378112 train.py:917] Step 00274: -124.5856 E_h, exp. variance=1.3201 E_h^2, pmove=0.73\n",
      "I0130 11:11:46.479680 128561009378112 train.py:917] Step 00275: -123.6348 E_h, exp. variance=1.2121 E_h^2, pmove=0.73\n",
      "I0130 11:11:46.768955 128561009378112 train.py:917] Step 00276: -124.5294 E_h, exp. variance=1.2572 E_h^2, pmove=0.72\n",
      "I0130 11:11:47.060736 128561009378112 train.py:917] Step 00277: -122.8710 E_h, exp. variance=1.1485 E_h^2, pmove=0.74\n",
      "I0130 11:11:47.349519 128561009378112 train.py:917] Step 00278: -123.5165 E_h, exp. variance=1.0395 E_h^2, pmove=0.73\n",
      "I0130 11:11:47.637346 128561009378112 train.py:917] Step 00279: -121.8297 E_h, exp. variance=1.1269 E_h^2, pmove=0.74\n",
      "I0130 11:11:47.929650 128561009378112 train.py:917] Step 00280: -122.6718 E_h, exp. variance=1.0341 E_h^2, pmove=0.72\n",
      "I0130 11:11:48.216904 128561009378112 train.py:917] Step 00281: -120.6332 E_h, exp. variance=1.4762 E_h^2, pmove=0.73\n",
      "I0130 11:11:48.504585 128561009378112 train.py:917] Step 00282: -122.6103 E_h, exp. variance=1.3337 E_h^2, pmove=0.74\n",
      "I0130 11:11:48.794029 128561009378112 train.py:917] Step 00283: -124.7163 E_h, exp. variance=1.5222 E_h^2, pmove=0.73\n",
      "I0130 11:11:49.083578 128561009378112 train.py:917] Step 00284: -122.7954 E_h, exp. variance=1.3743 E_h^2, pmove=0.73\n",
      "I0130 11:11:49.374082 128561009378112 train.py:917] Step 00285: -122.3478 E_h, exp. variance=1.2743 E_h^2, pmove=0.73\n",
      "I0130 11:11:49.662455 128561009378112 train.py:917] Step 00286: -122.9469 E_h, exp. variance=1.1469 E_h^2, pmove=0.74\n",
      "I0130 11:11:49.953227 128561009378112 train.py:917] Step 00287: -122.5937 E_h, exp. variance=1.0424 E_h^2, pmove=0.74\n",
      "I0130 11:11:50.243321 128561009378112 train.py:917] Step 00288: -122.4742 E_h, exp. variance=0.9541 E_h^2, pmove=0.74\n",
      "I0130 11:11:50.531445 128561009378112 train.py:917] Step 00289: -122.6231 E_h, exp. variance=0.8635 E_h^2, pmove=0.73\n",
      "I0130 11:11:50.821052 128561009378112 train.py:917] Step 00290: -123.0182 E_h, exp. variance=0.7803 E_h^2, pmove=0.75\n",
      "I0130 11:11:51.111464 128561009378112 train.py:917] Step 00291: -120.3807 E_h, exp. variance=1.2509 E_h^2, pmove=0.75\n",
      "I0130 11:11:51.400089 128561009378112 train.py:917] Step 00292: -121.5784 E_h, exp. variance=1.2202 E_h^2, pmove=0.73\n",
      "I0130 11:11:51.686377 128561009378112 train.py:917] Step 00293: -123.8775 E_h, exp. variance=1.2689 E_h^2, pmove=0.73\n",
      "I0130 11:11:51.980279 128561009378112 train.py:917] Step 00294: -122.3268 E_h, exp. variance=1.1507 E_h^2, pmove=0.75\n",
      "I0130 11:11:52.267452 128561009378112 train.py:917] Step 00295: -125.4746 E_h, exp. variance=1.7758 E_h^2, pmove=0.74\n",
      "I0130 11:11:52.555427 128561009378112 train.py:917] Step 00296: -124.6409 E_h, exp. variance=1.8730 E_h^2, pmove=0.75\n",
      "I0130 11:11:52.843498 128561009378112 train.py:917] Step 00297: -122.1740 E_h, exp. variance=1.7577 E_h^2, pmove=0.73\n",
      "I0130 11:11:53.134500 128561009378112 train.py:917] Step 00298: -123.2259 E_h, exp. variance=1.5874 E_h^2, pmove=0.74\n",
      "I0130 11:11:53.424976 128561009378112 train.py:917] Step 00299: -125.5818 E_h, exp. variance=2.0269 E_h^2, pmove=0.73\n",
      "I0130 11:11:53.713582 128561009378112 train.py:917] Step 00300: -125.4382 E_h, exp. variance=2.2506 E_h^2, pmove=0.74\n",
      "I0130 11:11:54.004054 128561009378112 train.py:917] Step 00301: -124.8061 E_h, exp. variance=2.1841 E_h^2, pmove=0.70\n",
      "I0130 11:11:54.293719 128561009378112 train.py:917] Step 00302: -122.3556 E_h, exp. variance=2.1077 E_h^2, pmove=0.70\n",
      "I0130 11:11:54.580086 128561009378112 train.py:917] Step 00303: -123.9828 E_h, exp. variance=1.9191 E_h^2, pmove=0.72\n",
      "I0130 11:11:54.869180 128561009378112 train.py:917] Step 00304: -122.5644 E_h, exp. variance=1.8121 E_h^2, pmove=0.68\n",
      "I0130 11:11:55.160987 128561009378112 train.py:917] Step 00305: -123.5588 E_h, exp. variance=1.6322 E_h^2, pmove=0.72\n",
      "I0130 11:11:55.449552 128561009378112 train.py:917] Step 00306: -122.6204 E_h, exp. variance=1.5310 E_h^2, pmove=0.70\n",
      "I0130 11:11:55.738329 128561009378112 train.py:917] Step 00307: -124.6142 E_h, exp. variance=1.5178 E_h^2, pmove=0.72\n",
      "I0130 11:11:56.028573 128561009378112 train.py:917] Step 00308: -123.4424 E_h, exp. variance=1.3662 E_h^2, pmove=0.71\n",
      "I0130 11:11:56.316922 128561009378112 train.py:917] Step 00309: -124.2519 E_h, exp. variance=1.2822 E_h^2, pmove=0.70\n",
      "I0130 11:11:56.603423 128561009378112 train.py:917] Step 00310: -123.7417 E_h, exp. variance=1.1568 E_h^2, pmove=0.70\n",
      "I0130 11:11:56.893466 128561009378112 train.py:917] Step 00311: -125.7973 E_h, exp. variance=1.4830 E_h^2, pmove=0.70\n",
      "I0130 11:11:57.183193 128561009378112 train.py:917] Step 00312: -122.7810 E_h, exp. variance=1.4287 E_h^2, pmove=0.72\n",
      "I0130 11:11:57.470044 128561009378112 train.py:917] Step 00313: -123.1245 E_h, exp. variance=1.3158 E_h^2, pmove=0.72\n",
      "I0130 11:11:57.759537 128561009378112 train.py:917] Step 00314: -123.9154 E_h, exp. variance=1.1909 E_h^2, pmove=0.70\n",
      "I0130 11:11:58.049146 128561009378112 train.py:917] Step 00315: -123.8769 E_h, exp. variance=1.0756 E_h^2, pmove=0.72\n",
      "I0130 11:11:58.337909 128561009378112 train.py:917] Step 00316: -125.2660 E_h, exp. variance=1.1913 E_h^2, pmove=0.70\n",
      "I0130 11:11:58.625141 128561009378112 train.py:917] Step 00317: -124.0515 E_h, exp. variance=1.0758 E_h^2, pmove=0.71\n",
      "I0130 11:11:58.915896 128561009378112 train.py:917] Step 00318: -123.1438 E_h, exp. variance=1.0156 E_h^2, pmove=0.73\n",
      "I0130 11:11:59.208294 128561009378112 train.py:917] Step 00319: -124.4941 E_h, exp. variance=0.9578 E_h^2, pmove=0.71\n",
      "I0130 11:11:59.496697 128561009378112 train.py:917] Step 00320: -124.6446 E_h, exp. variance=0.9166 E_h^2, pmove=0.72\n",
      "I0130 11:11:59.785834 128561009378112 train.py:917] Step 00321: -126.6649 E_h, exp. variance=1.4912 E_h^2, pmove=0.71\n",
      "I0130 11:12:00.076573 128561009378112 train.py:917] Step 00322: -125.8224 E_h, exp. variance=1.5743 E_h^2, pmove=0.71\n",
      "I0130 11:12:00.366956 128561009378112 train.py:917] Step 00323: -124.6218 E_h, exp. variance=1.4223 E_h^2, pmove=0.72\n",
      "I0130 11:12:00.655117 128561009378112 train.py:917] Step 00324: -123.8370 E_h, exp. variance=1.3087 E_h^2, pmove=0.69\n",
      "I0130 11:12:00.945021 128561009378112 train.py:917] Step 00325: -123.4717 E_h, exp. variance=1.2464 E_h^2, pmove=0.72\n",
      "I0130 11:12:01.248375 128561009378112 train.py:917] Step 00326: -123.1197 E_h, exp. variance=1.2383 E_h^2, pmove=0.70\n",
      "I0130 11:12:01.541462 128561009378112 train.py:917] Step 00327: -123.2536 E_h, exp. variance=1.1858 E_h^2, pmove=0.70\n",
      "I0130 11:12:01.841378 128561009378112 train.py:917] Step 00328: -124.5946 E_h, exp. variance=1.0934 E_h^2, pmove=0.71\n",
      "I0130 11:12:02.148642 128561009378112 train.py:917] Step 00329: -124.6098 E_h, exp. variance=1.0067 E_h^2, pmove=0.70\n",
      "I0130 11:12:02.463582 128561009378112 train.py:917] Step 00330: -124.0508 E_h, exp. variance=0.9071 E_h^2, pmove=0.72\n",
      "I0130 11:12:02.769684 128561009378112 train.py:917] Step 00331: -125.0949 E_h, exp. variance=0.8971 E_h^2, pmove=0.72\n",
      "I0130 11:12:03.071266 128561009378112 train.py:917] Step 00332: -125.2676 E_h, exp. variance=0.9019 E_h^2, pmove=0.71\n",
      "I0130 11:12:03.369524 128561009378112 train.py:917] Step 00333: -124.2361 E_h, exp. variance=0.8128 E_h^2, pmove=0.70\n",
      "I0130 11:12:03.668565 128561009378112 train.py:917] Step 00334: -125.0601 E_h, exp. variance=0.7789 E_h^2, pmove=0.70\n",
      "I0130 11:12:03.976052 128561009378112 train.py:917] Step 00335: -124.2949 E_h, exp. variance=0.7021 E_h^2, pmove=0.70\n",
      "I0130 11:12:04.290745 128561009378112 train.py:917] Step 00336: -125.6915 E_h, exp. variance=0.7831 E_h^2, pmove=0.70\n",
      "I0130 11:12:04.595752 128561009378112 train.py:917] Step 00337: -123.9563 E_h, exp. variance=0.7339 E_h^2, pmove=0.71\n",
      "I0130 11:12:04.902578 128561009378112 train.py:917] Step 00338: -122.8754 E_h, exp. variance=0.8889 E_h^2, pmove=0.71\n",
      "I0130 11:12:05.195783 128561009378112 train.py:917] Step 00339: -123.0505 E_h, exp. variance=0.9425 E_h^2, pmove=0.71\n",
      "I0130 11:12:05.495634 128561009378112 train.py:917] Step 00340: -124.3968 E_h, exp. variance=0.8524 E_h^2, pmove=0.70\n",
      "I0130 11:12:05.804907 128561009378112 train.py:917] Step 00341: -124.1832 E_h, exp. variance=0.7672 E_h^2, pmove=0.70\n",
      "I0130 11:12:06.113260 128561009378112 train.py:917] Step 00342: -124.0799 E_h, exp. variance=0.6918 E_h^2, pmove=0.70\n",
      "I0130 11:12:06.424894 128561009378112 train.py:917] Step 00343: -125.6398 E_h, exp. variance=0.8118 E_h^2, pmove=0.70\n",
      "I0130 11:12:06.729790 128561009378112 train.py:917] Step 00344: -123.7289 E_h, exp. variance=0.7637 E_h^2, pmove=0.71\n",
      "I0130 11:12:07.024970 128561009378112 train.py:917] Step 00345: -121.8723 E_h, exp. variance=1.2067 E_h^2, pmove=0.71\n",
      "I0130 11:12:07.322620 128561009378112 train.py:917] Step 00346: -123.9395 E_h, exp. variance=1.0868 E_h^2, pmove=0.70\n",
      "I0130 11:12:07.617789 128561009378112 train.py:917] Step 00347: -124.0133 E_h, exp. variance=0.9782 E_h^2, pmove=0.69\n",
      "I0130 11:12:07.914113 128561009378112 train.py:917] Step 00348: -124.8688 E_h, exp. variance=0.9446 E_h^2, pmove=0.72\n",
      "I0130 11:12:08.200791 128561009378112 train.py:917] Step 00349: -124.0913 E_h, exp. variance=0.8502 E_h^2, pmove=0.70\n",
      "I0130 11:12:08.492813 128561009378112 train.py:917] Step 00350: -124.1122 E_h, exp. variance=0.7652 E_h^2, pmove=0.70\n",
      "I0130 11:12:08.790491 128561009378112 train.py:917] Step 00351: -123.6000 E_h, exp. variance=0.7118 E_h^2, pmove=0.73\n",
      "I0130 11:12:09.078618 128561009378112 train.py:917] Step 00352: -124.7627 E_h, exp. variance=0.6855 E_h^2, pmove=0.70\n",
      "I0130 11:12:09.368652 128561009378112 train.py:917] Step 00353: -124.9903 E_h, exp. variance=0.6840 E_h^2, pmove=0.69\n",
      "I0130 11:12:09.662932 128561009378112 train.py:917] Step 00354: -125.4611 E_h, exp. variance=0.7558 E_h^2, pmove=0.71\n",
      "I0130 11:12:09.951322 128561009378112 train.py:917] Step 00355: -124.1245 E_h, exp. variance=0.6843 E_h^2, pmove=0.71\n",
      "I0130 11:12:10.245946 128561009378112 train.py:917] Step 00356: -124.0386 E_h, exp. variance=0.6228 E_h^2, pmove=0.71\n",
      "I0130 11:12:10.534684 128561009378112 train.py:917] Step 00357: -123.0104 E_h, exp. variance=0.7077 E_h^2, pmove=0.72\n",
      "I0130 11:12:10.828197 128561009378112 train.py:917] Step 00358: -124.7274 E_h, exp. variance=0.6658 E_h^2, pmove=0.72\n",
      "I0130 11:12:11.117397 128561009378112 train.py:917] Step 00359: -130.1170 E_h, exp. variance=3.7313 E_h^2, pmove=0.71\n",
      "I0130 11:12:11.419158 128561009378112 train.py:917] Step 00360: -123.4511 E_h, exp. variance=3.5238 E_h^2, pmove=0.70\n",
      "I0130 11:12:11.711096 128561009378112 train.py:917] Step 00361: -123.5084 E_h, exp. variance=3.2933 E_h^2, pmove=0.71\n",
      "I0130 11:12:12.011266 128561009378112 train.py:917] Step 00362: -124.0776 E_h, exp. variance=2.9845 E_h^2, pmove=0.71\n",
      "I0130 11:12:12.320502 128561009378112 train.py:917] Step 00363: -123.9304 E_h, exp. variance=2.7161 E_h^2, pmove=0.71\n",
      "I0130 11:12:12.631569 128561009378112 train.py:917] Step 00364: -123.1045 E_h, exp. variance=2.6074 E_h^2, pmove=0.70\n",
      "I0130 11:12:12.940144 128561009378112 train.py:917] Step 00365: -123.5087 E_h, exp. variance=2.4052 E_h^2, pmove=0.70\n",
      "I0130 11:12:13.235465 128561009378112 train.py:917] Step 00366: -124.1104 E_h, exp. variance=2.1661 E_h^2, pmove=0.70\n",
      "I0130 11:12:13.540960 128561009378112 train.py:917] Step 00367: -125.0425 E_h, exp. variance=2.0100 E_h^2, pmove=0.70\n",
      "I0130 11:12:13.846151 128561009378112 train.py:917] Step 00368: -124.1499 E_h, exp. variance=1.8112 E_h^2, pmove=0.70\n",
      "I0130 11:12:14.147253 128561009378112 train.py:917] Step 00369: -125.7989 E_h, exp. variance=1.8353 E_h^2, pmove=0.68\n",
      "I0130 11:12:14.464169 128561009378112 train.py:917] Step 00370: -125.0497 E_h, exp. variance=1.6852 E_h^2, pmove=0.71\n",
      "I0130 11:12:14.761295 128561009378112 train.py:917] Step 00371: -124.5426 E_h, exp. variance=1.5168 E_h^2, pmove=0.70\n",
      "I0130 11:12:15.059050 128561009378112 train.py:917] Step 00372: -123.8316 E_h, exp. variance=1.4060 E_h^2, pmove=0.71\n",
      "I0130 11:12:15.351619 128561009378112 train.py:917] Step 00373: -125.3488 E_h, exp. variance=1.3401 E_h^2, pmove=0.70\n",
      "I0130 11:12:15.654923 128561009378112 train.py:917] Step 00374: -127.2139 E_h, exp. variance=1.8549 E_h^2, pmove=0.73\n",
      "I0130 11:12:15.966700 128561009378112 train.py:917] Step 00375: -126.6094 E_h, exp. variance=1.9649 E_h^2, pmove=0.71\n",
      "I0130 11:12:16.274987 128561009378112 train.py:917] Step 00376: -127.0425 E_h, exp. variance=2.1518 E_h^2, pmove=0.71\n",
      "I0130 11:12:16.590428 128561009378112 train.py:917] Step 00377: -124.8791 E_h, exp. variance=1.9451 E_h^2, pmove=0.71\n",
      "I0130 11:12:16.901129 128561009378112 train.py:917] Step 00378: -124.6635 E_h, exp. variance=1.7722 E_h^2, pmove=0.72\n",
      "I0130 11:12:17.207245 128561009378112 train.py:917] Step 00379: -125.6303 E_h, exp. variance=1.6198 E_h^2, pmove=0.71\n",
      "I0130 11:12:17.499456 128561009378112 train.py:917] Step 00380: -125.3805 E_h, exp. variance=1.4623 E_h^2, pmove=0.71\n",
      "I0130 11:12:17.788577 128561009378112 train.py:917] Step 00381: -126.1079 E_h, exp. variance=1.3936 E_h^2, pmove=0.70\n",
      "I0130 11:12:18.076826 128561009378112 train.py:917] Step 00382: -124.9894 E_h, exp. variance=1.2614 E_h^2, pmove=0.69\n",
      "I0130 11:12:18.369798 128561009378112 train.py:917] Step 00383: -125.4115 E_h, exp. variance=1.1378 E_h^2, pmove=0.69\n",
      "I0130 11:12:18.681815 128561009378112 train.py:917] Step 00384: -124.4454 E_h, exp. variance=1.0839 E_h^2, pmove=0.70\n",
      "I0130 11:12:18.992829 128561009378112 train.py:917] Step 00385: -125.5179 E_h, exp. variance=0.9858 E_h^2, pmove=0.71\n",
      "I0130 11:12:19.304168 128561009378112 train.py:917] Step 00386: -123.4286 E_h, exp. variance=1.1739 E_h^2, pmove=0.71\n",
      "I0130 11:12:19.606616 128561009378112 train.py:917] Step 00387: -123.4950 E_h, exp. variance=1.2700 E_h^2, pmove=0.71\n",
      "I0130 11:12:19.914439 128561009378112 train.py:917] Step 00388: -123.4549 E_h, exp. variance=1.3260 E_h^2, pmove=0.71\n",
      "I0130 11:12:20.219018 128561009378112 train.py:917] Step 00389: -124.9803 E_h, exp. variance=1.1987 E_h^2, pmove=0.70\n",
      "I0130 11:12:20.514556 128561009378112 train.py:917] Step 00390: -124.7315 E_h, exp. variance=1.0789 E_h^2, pmove=0.70\n",
      "I0130 11:12:20.808431 128561009378112 train.py:917] Step 00391: -124.4306 E_h, exp. variance=0.9807 E_h^2, pmove=0.70\n",
      "I0130 11:12:21.101223 128561009378112 train.py:917] Step 00392: -124.2783 E_h, exp. variance=0.9007 E_h^2, pmove=0.70\n",
      "I0130 11:12:21.394992 128561009378112 train.py:917] Step 00393: -125.1691 E_h, exp. variance=0.8320 E_h^2, pmove=0.68\n",
      "I0130 11:12:21.688343 128561009378112 train.py:917] Step 00394: -123.4803 E_h, exp. variance=0.8895 E_h^2, pmove=0.70\n",
      "I0130 11:12:21.985223 128561009378112 train.py:917] Step 00395: -125.0131 E_h, exp. variance=0.8155 E_h^2, pmove=0.70\n",
      "I0130 11:12:22.291044 128561009378112 train.py:917] Step 00396: -124.3080 E_h, exp. variance=0.7443 E_h^2, pmove=0.70\n",
      "I0130 11:12:22.603349 128561009378112 train.py:917] Step 00397: -124.2323 E_h, exp. variance=0.6828 E_h^2, pmove=0.70\n",
      "I0130 11:12:22.899047 128561009378112 train.py:917] Step 00398: -123.8739 E_h, exp. variance=0.6587 E_h^2, pmove=0.70\n",
      "I0130 11:12:23.208747 128561009378112 train.py:917] Step 00399: -126.3276 E_h, exp. variance=0.8920 E_h^2, pmove=0.71\n",
      "I0130 11:12:23.504466 128561009378112 train.py:917] Step 00400: -124.6269 E_h, exp. variance=0.8031 E_h^2, pmove=0.70\n",
      "I0130 11:12:23.801047 128561009378112 train.py:917] Step 00401: -122.5899 E_h, exp. variance=1.1163 E_h^2, pmove=0.68\n",
      "I0130 11:12:24.094679 128561009378112 train.py:917] Step 00402: -124.5108 E_h, exp. variance=1.0048 E_h^2, pmove=0.68\n",
      "I0130 11:12:24.386882 128561009378112 train.py:917] Step 00403: -124.4315 E_h, exp. variance=0.9045 E_h^2, pmove=0.67\n",
      "I0130 11:12:24.681956 128561009378112 train.py:917] Step 00404: -123.6763 E_h, exp. variance=0.8709 E_h^2, pmove=0.67\n",
      "I0130 11:12:24.975401 128561009378112 train.py:917] Step 00405: -124.3034 E_h, exp. variance=0.7845 E_h^2, pmove=0.67\n",
      "I0130 11:12:25.269454 128561009378112 train.py:917] Step 00406: -122.9808 E_h, exp. variance=0.8829 E_h^2, pmove=0.68\n",
      "I0130 11:12:25.562841 128561009378112 train.py:917] Step 00407: -128.7567 E_h, exp. variance=2.6286 E_h^2, pmove=0.68\n",
      "I0130 11:12:25.859893 128561009378112 train.py:917] Step 00408: -124.1917 E_h, exp. variance=2.3884 E_h^2, pmove=0.68\n",
      "I0130 11:12:26.169839 128561009378112 train.py:917] Step 00409: -124.1262 E_h, exp. variance=2.1737 E_h^2, pmove=0.70\n",
      "I0130 11:12:26.465527 128561009378112 train.py:917] Step 00410: -124.3905 E_h, exp. variance=1.9600 E_h^2, pmove=0.66\n",
      "I0130 11:12:26.767786 128561009378112 train.py:917] Step 00411: -124.2525 E_h, exp. variance=1.7732 E_h^2, pmove=0.66\n",
      "I0130 11:12:27.068262 128561009378112 train.py:917] Step 00412: -127.1253 E_h, exp. variance=2.1974 E_h^2, pmove=0.68\n",
      "I0130 11:12:27.381186 128561009378112 train.py:917] Step 00413: -125.0026 E_h, exp. variance=1.9814 E_h^2, pmove=0.68\n",
      "I0130 11:12:27.677530 128561009378112 train.py:917] Step 00414: -124.1923 E_h, exp. variance=1.8186 E_h^2, pmove=0.67\n",
      "I0130 11:12:27.970902 128561009378112 train.py:917] Step 00415: -125.7988 E_h, exp. variance=1.7346 E_h^2, pmove=0.69\n",
      "I0130 11:12:28.264826 128561009378112 train.py:917] Step 00416: -124.2531 E_h, exp. variance=1.5943 E_h^2, pmove=0.69\n",
      "I0130 11:12:28.559343 128561009378112 train.py:917] Step 00417: -126.0115 E_h, exp. variance=1.5670 E_h^2, pmove=0.68\n",
      "I0130 11:12:28.858371 128561009378112 train.py:917] Step 00418: -124.8665 E_h, exp. variance=1.4106 E_h^2, pmove=0.68\n",
      "I0130 11:12:29.146825 128561009378112 train.py:917] Step 00419: -124.5189 E_h, exp. variance=1.2837 E_h^2, pmove=0.66\n",
      "I0130 11:12:29.438161 128561009378112 train.py:917] Step 00420: -124.2496 E_h, exp. variance=1.1906 E_h^2, pmove=0.67\n",
      "I0130 11:12:29.731444 128561009378112 train.py:917] Step 00421: -125.7691 E_h, exp. variance=1.1538 E_h^2, pmove=0.68\n",
      "I0130 11:12:30.030857 128561009378112 train.py:917] Step 00422: -127.7385 E_h, exp. variance=1.7591 E_h^2, pmove=0.69\n",
      "I0130 11:12:30.324370 128561009378112 train.py:917] Step 00423: -125.2628 E_h, exp. variance=1.5836 E_h^2, pmove=0.66\n",
      "I0130 11:12:30.616731 128561009378112 train.py:917] Step 00424: -123.3978 E_h, exp. variance=1.7172 E_h^2, pmove=0.68\n",
      "I0130 11:12:30.914582 128561009378112 train.py:917] Step 00425: -124.6894 E_h, exp. variance=1.5552 E_h^2, pmove=0.67\n",
      "I0130 11:12:31.207682 128561009378112 train.py:917] Step 00426: -126.3374 E_h, exp. variance=1.5641 E_h^2, pmove=0.69\n",
      "I0130 11:12:31.497480 128561009378112 train.py:917] Step 00427: -124.8808 E_h, exp. variance=1.4129 E_h^2, pmove=0.68\n",
      "I0130 11:12:31.793558 128561009378112 train.py:917] Step 00428: -123.2581 E_h, exp. variance=1.5759 E_h^2, pmove=0.68\n",
      "I0130 11:12:32.087306 128561009378112 train.py:917] Step 00429: -126.5858 E_h, exp. variance=1.6702 E_h^2, pmove=0.68\n",
      "I0130 11:12:32.386581 128561009378112 train.py:917] Step 00430: -124.7950 E_h, exp. variance=1.5105 E_h^2, pmove=0.68\n",
      "I0130 11:12:32.686869 128561009378112 train.py:917] Step 00431: -123.6332 E_h, exp. variance=1.5405 E_h^2, pmove=0.66\n",
      "I0130 11:12:32.999255 128561009378112 train.py:917] Step 00432: -124.5229 E_h, exp. variance=1.4000 E_h^2, pmove=0.69\n",
      "I0130 11:12:33.307370 128561009378112 train.py:917] Step 00433: -125.9169 E_h, exp. variance=1.3584 E_h^2, pmove=0.69\n",
      "I0130 11:12:33.599444 128561009378112 train.py:917] Step 00434: -124.9312 E_h, exp. variance=1.2227 E_h^2, pmove=0.69\n",
      "I0130 11:12:33.896086 128561009378112 train.py:917] Step 00435: -127.8088 E_h, exp. variance=1.8250 E_h^2, pmove=0.69\n",
      "I0130 11:12:34.183707 128561009378112 train.py:917] Step 00436: -126.6399 E_h, exp. variance=1.8151 E_h^2, pmove=0.68\n",
      "I0130 11:12:34.477391 128561009378112 train.py:917] Step 00437: -124.6629 E_h, exp. variance=1.6817 E_h^2, pmove=0.69\n",
      "I0130 11:12:34.780311 128561009378112 train.py:917] Step 00438: -124.5778 E_h, exp. variance=1.5631 E_h^2, pmove=0.67\n",
      "I0130 11:12:35.081016 128561009378112 train.py:917] Step 00439: -124.7772 E_h, exp. variance=1.4266 E_h^2, pmove=0.67\n",
      "I0130 11:12:35.372181 128561009378112 train.py:917] Step 00440: -124.0132 E_h, exp. variance=1.4106 E_h^2, pmove=0.67\n",
      "I0130 11:12:35.666835 128561009378112 train.py:917] Step 00441: -124.1660 E_h, exp. variance=1.3448 E_h^2, pmove=0.68\n",
      "I0130 11:12:35.964972 128561009378112 train.py:917] Step 00442: -125.7849 E_h, exp. variance=1.2673 E_h^2, pmove=0.68\n",
      "I0130 11:12:36.254359 128561009378112 train.py:917] Step 00443: -122.8680 E_h, exp. variance=1.5765 E_h^2, pmove=0.68\n",
      "I0130 11:12:36.542814 128561009378112 train.py:917] Step 00444: -122.5445 E_h, exp. variance=1.8967 E_h^2, pmove=0.68\n",
      "I0130 11:12:36.843775 128561009378112 train.py:917] Step 00445: -125.3285 E_h, exp. variance=1.7525 E_h^2, pmove=0.67\n",
      "I0130 11:12:37.134237 128561009378112 train.py:917] Step 00446: -125.0478 E_h, exp. variance=1.5888 E_h^2, pmove=0.69\n",
      "I0130 11:12:37.430233 128561009378112 train.py:917] Step 00447: -124.5148 E_h, exp. variance=1.4339 E_h^2, pmove=0.68\n",
      "I0130 11:12:37.732219 128561009378112 train.py:917] Step 00448: -124.4901 E_h, exp. variance=1.2946 E_h^2, pmove=0.66\n",
      "I0130 11:12:38.023620 128561009378112 train.py:917] Step 00449: -125.5060 E_h, exp. variance=1.2261 E_h^2, pmove=0.67\n",
      "I0130 11:12:38.313966 128561009378112 train.py:917] Step 00450: -124.5129 E_h, exp. variance=1.1093 E_h^2, pmove=0.69\n",
      "I0130 11:12:38.615579 128561009378112 train.py:917] Step 00451: -123.6917 E_h, exp. variance=1.0972 E_h^2, pmove=0.68\n",
      "I0130 11:12:38.928189 128561009378112 train.py:917] Step 00452: -124.2315 E_h, exp. variance=1.0021 E_h^2, pmove=0.67\n",
      "I0130 11:12:39.216487 128561009378112 train.py:917] Step 00453: -125.2649 E_h, exp. variance=0.9424 E_h^2, pmove=0.67\n",
      "I0130 11:12:39.525122 128561009378112 train.py:917] Step 00454: -124.9620 E_h, exp. variance=0.8562 E_h^2, pmove=0.67\n",
      "I0130 11:12:39.827733 128561009378112 train.py:917] Step 00455: -127.9569 E_h, exp. variance=1.7301 E_h^2, pmove=0.67\n",
      "I0130 11:12:40.131322 128561009378112 train.py:917] Step 00456: -125.0675 E_h, exp. variance=1.5573 E_h^2, pmove=0.67\n",
      "I0130 11:12:40.436811 128561009378112 train.py:917] Step 00457: -126.9273 E_h, exp. variance=1.7279 E_h^2, pmove=0.67\n",
      "I0130 11:12:40.742162 128561009378112 train.py:917] Step 00458: -125.0133 E_h, exp. variance=1.5587 E_h^2, pmove=0.67\n",
      "I0130 11:12:41.037580 128561009378112 train.py:917] Step 00459: -126.2677 E_h, exp. variance=1.5067 E_h^2, pmove=0.68\n",
      "I0130 11:12:41.337999 128561009378112 train.py:917] Step 00460: -125.7405 E_h, exp. variance=1.3734 E_h^2, pmove=0.67\n",
      "I0130 11:12:41.636479 128561009378112 train.py:917] Step 00461: -126.3814 E_h, exp. variance=1.3328 E_h^2, pmove=0.67\n",
      "I0130 11:12:41.936529 128561009378112 train.py:917] Step 00462: -127.1791 E_h, exp. variance=1.4690 E_h^2, pmove=0.68\n",
      "I0130 11:12:42.248172 128561009378112 train.py:917] Step 00463: -125.4730 E_h, exp. variance=1.3241 E_h^2, pmove=0.68\n",
      "I0130 11:12:42.544404 128561009378112 train.py:917] Step 00464: -126.8190 E_h, exp. variance=1.3240 E_h^2, pmove=0.68\n",
      "I0130 11:12:42.853767 128561009378112 train.py:917] Step 00465: -128.4809 E_h, exp. variance=1.8736 E_h^2, pmove=0.67\n",
      "I0130 11:12:43.166754 128561009378112 train.py:917] Step 00466: -127.6106 E_h, exp. variance=1.9188 E_h^2, pmove=0.66\n",
      "I0130 11:12:43.478163 128561009378112 train.py:917] Step 00467: -126.9333 E_h, exp. variance=1.7801 E_h^2, pmove=0.68\n",
      "I0130 11:12:43.771224 128561009378112 train.py:917] Step 00468: -127.4684 E_h, exp. variance=1.7377 E_h^2, pmove=0.68\n",
      "I0130 11:12:44.062609 128561009378112 train.py:917] Step 00469: -127.5515 E_h, exp. variance=1.6909 E_h^2, pmove=0.66\n",
      "I0130 11:12:44.362611 128561009378112 train.py:917] Step 00470: -127.7716 E_h, exp. variance=1.6714 E_h^2, pmove=0.68\n",
      "I0130 11:12:44.663540 128561009378112 train.py:917] Step 00471: -124.4245 E_h, exp. variance=1.9347 E_h^2, pmove=0.66\n",
      "I0130 11:12:44.952528 128561009378112 train.py:917] Step 00472: -128.2802 E_h, exp. variance=2.0618 E_h^2, pmove=0.67\n",
      "I0130 11:12:45.255763 128561009378112 train.py:917] Step 00473: -125.2704 E_h, exp. variance=2.0104 E_h^2, pmove=0.66\n",
      "I0130 11:12:45.561932 128561009378112 train.py:917] Step 00474: -126.0168 E_h, exp. variance=1.8263 E_h^2, pmove=0.66\n",
      "I0130 11:12:45.870268 128561009378112 train.py:917] Step 00475: -125.5773 E_h, exp. variance=1.7056 E_h^2, pmove=0.68\n",
      "I0130 11:12:46.183417 128561009378112 train.py:917] Step 00476: -125.5293 E_h, exp. variance=1.5919 E_h^2, pmove=0.64\n",
      "I0130 11:12:46.496131 128561009378112 train.py:917] Step 00477: -123.6476 E_h, exp. variance=2.0396 E_h^2, pmove=0.66\n",
      "I0130 11:12:46.803242 128561009378112 train.py:917] Step 00478: -124.0182 E_h, exp. variance=2.1837 E_h^2, pmove=0.65\n",
      "I0130 11:12:47.118152 128561009378112 train.py:917] Step 00479: -123.8299 E_h, exp. variance=2.3105 E_h^2, pmove=0.69\n",
      "I0130 11:12:47.421705 128561009378112 train.py:917] Step 00480: -123.8351 E_h, exp. variance=2.3574 E_h^2, pmove=0.66\n",
      "I0130 11:12:47.721996 128561009378112 train.py:917] Step 00481: -123.2123 E_h, exp. variance=2.5589 E_h^2, pmove=0.68\n",
      "I0130 11:12:48.021123 128561009378112 train.py:917] Step 00482: -125.0570 E_h, exp. variance=2.3048 E_h^2, pmove=0.68\n",
      "I0130 11:12:48.333315 128561009378112 train.py:917] Step 00483: -124.2102 E_h, exp. variance=2.1594 E_h^2, pmove=0.68\n",
      "I0130 11:12:48.695068 128561009378112 train.py:917] Step 00484: -124.7783 E_h, exp. variance=1.9519 E_h^2, pmove=0.69\n",
      "I0130 11:12:49.062269 128561009378112 train.py:917] Step 00485: -124.2872 E_h, exp. variance=1.8097 E_h^2, pmove=0.68\n",
      "I0130 11:12:49.424150 128561009378112 train.py:917] Step 00486: -123.9962 E_h, exp. variance=1.7154 E_h^2, pmove=0.67\n",
      "I0130 11:12:49.783607 128561009378112 train.py:917] Step 00487: -125.4615 E_h, exp. variance=1.5744 E_h^2, pmove=0.68\n",
      "I0130 11:12:50.148874 128561009378112 train.py:917] Step 00488: -126.2350 E_h, exp. variance=1.5684 E_h^2, pmove=0.65\n",
      "I0130 11:12:50.513797 128561009378112 train.py:917] Step 00489: -123.6659 E_h, exp. variance=1.5883 E_h^2, pmove=0.67\n",
      "I0130 11:12:50.876930 128561009378112 train.py:917] Step 00490: -124.9046 E_h, exp. variance=1.4295 E_h^2, pmove=0.68\n",
      "I0130 11:12:51.248895 128561009378112 train.py:917] Step 00491: -127.4113 E_h, exp. variance=1.8429 E_h^2, pmove=0.68\n",
      "I0130 11:12:51.615769 128561009378112 train.py:917] Step 00492: -124.6695 E_h, exp. variance=1.6815 E_h^2, pmove=0.68\n",
      "I0130 11:12:51.984723 128561009378112 train.py:917] Step 00493: -123.9473 E_h, exp. variance=1.6378 E_h^2, pmove=0.68\n",
      "I0130 11:12:52.357981 128561009378112 train.py:917] Step 00494: -123.9560 E_h, exp. variance=1.5732 E_h^2, pmove=0.67\n",
      "I0130 11:12:52.725083 128561009378112 train.py:917] Step 00495: -124.7989 E_h, exp. variance=1.4168 E_h^2, pmove=0.67\n",
      "I0130 11:12:53.021633 128561009378112 train.py:917] Step 00496: -127.0392 E_h, exp. variance=1.6906 E_h^2, pmove=0.68\n",
      "I0130 11:12:53.318875 128561009378112 train.py:917] Step 00497: -127.2040 E_h, exp. variance=1.9180 E_h^2, pmove=0.69\n",
      "I0130 11:12:53.632441 128561009378112 train.py:917] Step 00498: -125.5878 E_h, exp. variance=1.7329 E_h^2, pmove=0.67\n",
      "I0130 11:12:53.947091 128561009378112 train.py:917] Step 00499: -124.8773 E_h, exp. variance=1.5790 E_h^2, pmove=0.67\n",
      "I0130 11:12:54.265457 128561009378112 train.py:917] Step 00500: -125.4550 E_h, exp. variance=1.4234 E_h^2, pmove=0.68\n",
      "I0130 11:12:54.580195 128561009378112 train.py:917] Step 00501: -125.0955 E_h, exp. variance=1.2853 E_h^2, pmove=0.64\n",
      "I0130 11:12:54.893090 128561009378112 train.py:917] Step 00502: -124.3238 E_h, exp. variance=1.2408 E_h^2, pmove=0.64\n",
      "I0130 11:12:55.208137 128561009378112 train.py:917] Step 00503: -126.3120 E_h, exp. variance=1.2293 E_h^2, pmove=0.65\n",
      "I0130 11:12:55.521997 128561009378112 train.py:917] Step 00504: -125.7064 E_h, exp. variance=1.1209 E_h^2, pmove=0.65\n",
      "I0130 11:12:55.835927 128561009378112 train.py:917] Step 00505: -124.7259 E_h, exp. variance=1.0433 E_h^2, pmove=0.63\n",
      "I0130 11:12:56.150215 128561009378112 train.py:917] Step 00506: -125.6003 E_h, exp. variance=0.9480 E_h^2, pmove=0.64\n",
      "I0130 11:12:56.466061 128561009378112 train.py:917] Step 00507: -125.4886 E_h, exp. variance=0.8559 E_h^2, pmove=0.64\n",
      "I0130 11:12:56.782160 128561009378112 train.py:917] Step 00508: -127.0122 E_h, exp. variance=1.0242 E_h^2, pmove=0.65\n",
      "I0130 11:12:57.110657 128561009378112 train.py:917] Step 00509: -124.8285 E_h, exp. variance=0.9625 E_h^2, pmove=0.63\n",
      "I0130 11:12:57.424695 128561009378112 train.py:917] Step 00510: -125.4638 E_h, exp. variance=0.8663 E_h^2, pmove=0.64\n",
      "I0130 11:12:57.737517 128561009378112 train.py:917] Step 00511: -127.2618 E_h, exp. variance=1.0796 E_h^2, pmove=0.66\n",
      "I0130 11:12:58.048599 128561009378112 train.py:917] Step 00512: -127.4173 E_h, exp. variance=1.2627 E_h^2, pmove=0.65\n",
      "I0130 11:12:58.345701 128561009378112 train.py:917] Step 00513: -126.6772 E_h, exp. variance=1.2059 E_h^2, pmove=0.62\n",
      "I0130 11:12:58.637013 128561009378112 train.py:917] Step 00514: -124.1876 E_h, exp. variance=1.3451 E_h^2, pmove=0.65\n",
      "I0130 11:12:58.942510 128561009378112 train.py:917] Step 00515: -125.9744 E_h, exp. variance=1.2166 E_h^2, pmove=0.64\n",
      "I0130 11:12:59.237458 128561009378112 train.py:917] Step 00516: -125.5192 E_h, exp. variance=1.0994 E_h^2, pmove=0.65\n",
      "I0130 11:12:59.538166 128561009378112 train.py:917] Step 00517: -125.9317 E_h, exp. variance=0.9935 E_h^2, pmove=0.65\n",
      "I0130 11:12:59.834866 128561009378112 train.py:917] Step 00518: -127.4363 E_h, exp. variance=1.1527 E_h^2, pmove=0.66\n",
      "I0130 11:13:00.132692 128561009378112 train.py:917] Step 00519: -128.5233 E_h, exp. variance=1.6517 E_h^2, pmove=0.64\n",
      "I0130 11:13:00.429826 128561009378112 train.py:917] Step 00520: -127.7005 E_h, exp. variance=1.6968 E_h^2, pmove=0.64\n",
      "I0130 11:13:00.725394 128561009378112 train.py:917] Step 00521: -124.3024 E_h, exp. variance=1.8952 E_h^2, pmove=0.64\n",
      "I0130 11:13:01.021145 128561009378112 train.py:917] Step 00522: -125.8801 E_h, exp. variance=1.7110 E_h^2, pmove=0.64\n",
      "I0130 11:13:01.316159 128561009378112 train.py:917] Step 00523: -127.1853 E_h, exp. variance=1.6462 E_h^2, pmove=0.66\n",
      "I0130 11:13:01.613573 128561009378112 train.py:917] Step 00524: -125.9344 E_h, exp. variance=1.4883 E_h^2, pmove=0.64\n",
      "I0130 11:13:01.909603 128561009378112 train.py:917] Step 00525: -126.6430 E_h, exp. variance=1.3588 E_h^2, pmove=0.64\n",
      "I0130 11:13:02.205072 128561009378112 train.py:917] Step 00526: -126.9320 E_h, exp. variance=1.2677 E_h^2, pmove=0.63\n",
      "I0130 11:13:02.502689 128561009378112 train.py:917] Step 00527: -127.3401 E_h, exp. variance=1.2389 E_h^2, pmove=0.63\n",
      "I0130 11:13:02.800204 128561009378112 train.py:917] Step 00528: -125.4638 E_h, exp. variance=1.1941 E_h^2, pmove=0.65\n",
      "I0130 11:13:03.095307 128561009378112 train.py:917] Step 00529: -124.9562 E_h, exp. variance=1.2390 E_h^2, pmove=0.65\n",
      "I0130 11:13:03.391441 128561009378112 train.py:917] Step 00530: -126.6314 E_h, exp. variance=1.1341 E_h^2, pmove=0.64\n",
      "I0130 11:13:03.689503 128561009378112 train.py:917] Step 00531: -123.8652 E_h, exp. variance=1.5189 E_h^2, pmove=0.65\n",
      "I0130 11:13:03.986470 128561009378112 train.py:917] Step 00532: -125.3052 E_h, exp. variance=1.4083 E_h^2, pmove=0.64\n",
      "I0130 11:13:04.282472 128561009378112 train.py:917] Step 00533: -124.2075 E_h, exp. variance=1.5299 E_h^2, pmove=0.66\n",
      "I0130 11:13:04.579741 128561009378112 train.py:917] Step 00534: -125.4968 E_h, exp. variance=1.3824 E_h^2, pmove=0.65\n",
      "I0130 11:13:04.875976 128561009378112 train.py:917] Step 00535: -127.1635 E_h, exp. variance=1.4318 E_h^2, pmove=0.64\n",
      "I0130 11:13:05.171284 128561009378112 train.py:917] Step 00536: -124.5430 E_h, exp. variance=1.4457 E_h^2, pmove=0.65\n",
      "I0130 11:13:05.468465 128561009378112 train.py:917] Step 00537: -124.8021 E_h, exp. variance=1.3789 E_h^2, pmove=0.64\n",
      "I0130 11:13:05.765773 128561009378112 train.py:917] Step 00538: -125.6390 E_h, exp. variance=1.2410 E_h^2, pmove=0.66\n",
      "I0130 11:13:06.061563 128561009378112 train.py:917] Step 00539: -126.1649 E_h, exp. variance=1.1418 E_h^2, pmove=0.65\n",
      "I0130 11:13:06.355463 128561009378112 train.py:917] Step 00540: -126.4007 E_h, exp. variance=1.0729 E_h^2, pmove=0.63\n",
      "I0130 11:13:06.652610 128561009378112 train.py:917] Step 00541: -129.4526 E_h, exp. variance=2.1912 E_h^2, pmove=0.63\n",
      "I0130 11:13:06.949442 128561009378112 train.py:917] Step 00542: -126.9641 E_h, exp. variance=2.0345 E_h^2, pmove=0.63\n",
      "I0130 11:13:07.247010 128561009378112 train.py:917] Step 00543: -126.8446 E_h, exp. variance=1.8667 E_h^2, pmove=0.64\n",
      "I0130 11:13:07.544905 128561009378112 train.py:917] Step 00544: -124.8129 E_h, exp. variance=1.8732 E_h^2, pmove=0.66\n",
      "I0130 11:13:07.841468 128561009378112 train.py:917] Step 00545: -123.9167 E_h, exp. variance=2.1272 E_h^2, pmove=0.63\n",
      "I0130 11:13:08.137845 128561009378112 train.py:917] Step 00546: -126.6579 E_h, exp. variance=1.9649 E_h^2, pmove=0.63\n",
      "I0130 11:13:08.433931 128561009378112 train.py:917] Step 00547: -127.2001 E_h, exp. variance=1.9013 E_h^2, pmove=0.64\n",
      "I0130 11:13:08.732202 128561009378112 train.py:917] Step 00548: -127.5561 E_h, exp. variance=1.9004 E_h^2, pmove=0.63\n",
      "I0130 11:13:09.029069 128561009378112 train.py:917] Step 00549: -125.8304 E_h, exp. variance=1.7263 E_h^2, pmove=0.64\n",
      "I0130 11:13:09.323033 128561009378112 train.py:917] Step 00550: -124.9941 E_h, exp. variance=1.6865 E_h^2, pmove=0.64\n",
      "I0130 11:13:09.621329 128561009378112 train.py:917] Step 00551: -126.0927 E_h, exp. variance=1.5179 E_h^2, pmove=0.65\n",
      "I0130 11:13:09.918771 128561009378112 train.py:917] Step 00552: -125.3837 E_h, exp. variance=1.4107 E_h^2, pmove=0.66\n",
      "I0130 11:13:10.216032 128561009378112 train.py:917] Step 00553: -124.9976 E_h, exp. variance=1.3633 E_h^2, pmove=0.64\n",
      "I0130 11:13:10.512173 128561009378112 train.py:917] Step 00554: -126.9932 E_h, exp. variance=1.3315 E_h^2, pmove=0.63\n",
      "I0130 11:13:10.811966 128561009378112 train.py:917] Step 00555: -126.7983 E_h, exp. variance=1.2524 E_h^2, pmove=0.64\n",
      "I0130 11:13:11.108163 128561009378112 train.py:917] Step 00556: -127.1219 E_h, exp. variance=1.2210 E_h^2, pmove=0.66\n",
      "I0130 11:13:11.404305 128561009378112 train.py:917] Step 00557: -126.0474 E_h, exp. variance=1.1011 E_h^2, pmove=0.64\n",
      "I0130 11:13:11.702376 128561009378112 train.py:917] Step 00558: -127.1777 E_h, exp. variance=1.0792 E_h^2, pmove=0.63\n",
      "I0130 11:13:12.000434 128561009378112 train.py:917] Step 00559: -126.9824 E_h, exp. variance=1.0149 E_h^2, pmove=0.64\n",
      "I0130 11:13:12.298587 128561009378112 train.py:917] Step 00560: -130.6232 E_h, exp. variance=2.5522 E_h^2, pmove=0.64\n",
      "I0130 11:13:12.596840 128561009378112 train.py:917] Step 00561: -125.6596 E_h, exp. variance=2.4105 E_h^2, pmove=0.63\n",
      "I0130 11:13:12.893925 128561009378112 train.py:917] Step 00562: -125.9058 E_h, exp. variance=2.2221 E_h^2, pmove=0.64\n",
      "I0130 11:13:13.190288 128561009378112 train.py:917] Step 00563: -127.1453 E_h, exp. variance=2.0272 E_h^2, pmove=0.64\n",
      "I0130 11:13:13.486300 128561009378112 train.py:917] Step 00564: -127.7515 E_h, exp. variance=1.9339 E_h^2, pmove=0.64\n",
      "I0130 11:13:13.785951 128561009378112 train.py:917] Step 00565: -126.3671 E_h, exp. variance=1.7543 E_h^2, pmove=0.63\n",
      "I0130 11:13:14.083314 128561009378112 train.py:917] Step 00566: -124.4559 E_h, exp. variance=2.0403 E_h^2, pmove=0.63\n",
      "I0130 11:13:14.380831 128561009378112 train.py:917] Step 00567: -125.2078 E_h, exp. variance=1.9851 E_h^2, pmove=0.63\n",
      "I0130 11:13:14.679007 128561009378112 train.py:917] Step 00568: -125.5662 E_h, exp. variance=1.8440 E_h^2, pmove=0.64\n",
      "I0130 11:13:14.976676 128561009378112 train.py:917] Step 00569: -127.3335 E_h, exp. variance=1.7585 E_h^2, pmove=0.64\n",
      "I0130 11:13:15.273902 128561009378112 train.py:917] Step 00570: -126.2448 E_h, exp. variance=1.5846 E_h^2, pmove=0.65\n",
      "I0130 11:13:15.570760 128561009378112 train.py:917] Step 00571: -125.7902 E_h, exp. variance=1.4569 E_h^2, pmove=0.63\n",
      "I0130 11:13:15.870608 128561009378112 train.py:917] Step 00572: -126.0219 E_h, exp. variance=1.3191 E_h^2, pmove=0.64\n",
      "I0130 11:13:16.168336 128561009378112 train.py:917] Step 00573: -124.1512 E_h, exp. variance=1.5979 E_h^2, pmove=0.64\n",
      "I0130 11:13:16.465542 128561009378112 train.py:917] Step 00574: -125.4177 E_h, exp. variance=1.4769 E_h^2, pmove=0.63\n",
      "I0130 11:13:16.765157 128561009378112 train.py:917] Step 00575: -126.1330 E_h, exp. variance=1.3306 E_h^2, pmove=0.64\n",
      "I0130 11:13:17.062486 128561009378112 train.py:917] Step 00576: -125.7279 E_h, exp. variance=1.2052 E_h^2, pmove=0.63\n",
      "I0130 11:13:17.359860 128561009378112 train.py:917] Step 00577: -126.5280 E_h, exp. variance=1.1106 E_h^2, pmove=0.64\n",
      "I0130 11:13:17.654613 128561009378112 train.py:917] Step 00578: -127.1444 E_h, exp. variance=1.1083 E_h^2, pmove=0.64\n",
      "I0130 11:13:17.951591 128561009378112 train.py:917] Step 00579: -126.7981 E_h, exp. variance=1.0347 E_h^2, pmove=0.64\n",
      "I0130 11:13:18.246487 128561009378112 train.py:917] Step 00580: -126.3288 E_h, exp. variance=0.9323 E_h^2, pmove=0.64\n",
      "I0130 11:13:18.538111 128561009378112 train.py:917] Step 00581: -125.7514 E_h, exp. variance=0.8597 E_h^2, pmove=0.64\n",
      "I0130 11:13:18.837708 128561009378112 train.py:917] Step 00582: -126.8770 E_h, exp. variance=0.8172 E_h^2, pmove=0.64\n",
      "I0130 11:13:19.132389 128561009378112 train.py:917] Step 00583: -126.1864 E_h, exp. variance=0.7359 E_h^2, pmove=0.65\n",
      "I0130 11:13:19.425846 128561009378112 train.py:917] Step 00584: -126.5731 E_h, exp. variance=0.6719 E_h^2, pmove=0.65\n",
      "I0130 11:13:19.720159 128561009378112 train.py:917] Step 00585: -126.5321 E_h, exp. variance=0.6106 E_h^2, pmove=0.64\n",
      "I0130 11:13:20.015117 128561009378112 train.py:917] Step 00586: -125.4084 E_h, exp. variance=0.6216 E_h^2, pmove=0.65\n",
      "I0130 11:13:20.308888 128561009378112 train.py:917] Step 00587: -125.5187 E_h, exp. variance=0.6029 E_h^2, pmove=0.64\n",
      "I0130 11:13:20.602179 128561009378112 train.py:917] Step 00588: -126.8002 E_h, exp. variance=0.5813 E_h^2, pmove=0.64\n",
      "I0130 11:13:20.899517 128561009378112 train.py:917] Step 00589: -125.0913 E_h, exp. variance=0.6359 E_h^2, pmove=0.64\n",
      "I0130 11:13:21.193835 128561009378112 train.py:917] Step 00590: -125.2286 E_h, exp. variance=0.6403 E_h^2, pmove=0.64\n",
      "I0130 11:13:21.486366 128561009378112 train.py:917] Step 00591: -125.9333 E_h, exp. variance=0.5768 E_h^2, pmove=0.64\n",
      "I0130 11:13:21.784695 128561009378112 train.py:917] Step 00592: -126.1629 E_h, exp. variance=0.5214 E_h^2, pmove=0.65\n",
      "I0130 11:13:22.080605 128561009378112 train.py:917] Step 00593: -124.6528 E_h, exp. variance=0.6374 E_h^2, pmove=0.65\n",
      "I0130 11:13:22.373040 128561009378112 train.py:917] Step 00594: -124.4827 E_h, exp. variance=0.7500 E_h^2, pmove=0.65\n",
      "I0130 11:13:22.666179 128561009378112 train.py:917] Step 00595: -125.9571 E_h, exp. variance=0.6792 E_h^2, pmove=0.66\n",
      "I0130 11:13:22.963218 128561009378112 train.py:917] Step 00596: -126.2926 E_h, exp. variance=0.6364 E_h^2, pmove=0.65\n",
      "I0130 11:13:23.257048 128561009378112 train.py:917] Step 00597: -126.1129 E_h, exp. variance=0.5806 E_h^2, pmove=0.66\n",
      "I0130 11:13:23.552573 128561009378112 train.py:917] Step 00598: -126.3132 E_h, exp. variance=0.5422 E_h^2, pmove=0.64\n",
      "I0130 11:13:23.850768 128561009378112 train.py:917] Step 00599: -127.0070 E_h, exp. variance=0.5996 E_h^2, pmove=0.64\n",
      "I0130 11:13:24.145985 128561009378112 train.py:917] Step 00600: -125.9837 E_h, exp. variance=0.5397 E_h^2, pmove=0.66\n",
      "I0130 11:13:24.438591 128561009378112 train.py:917] Step 00601: -125.6024 E_h, exp. variance=0.5001 E_h^2, pmove=0.60\n",
      "I0130 11:13:24.733796 128561009378112 train.py:917] Step 00602: -125.7779 E_h, exp. variance=0.4532 E_h^2, pmove=0.61\n",
      "I0130 11:13:25.032039 128561009378112 train.py:917] Step 00603: -126.6183 E_h, exp. variance=0.4488 E_h^2, pmove=0.60\n",
      "I0130 11:13:25.326883 128561009378112 train.py:917] Step 00604: -125.6048 E_h, exp. variance=0.4188 E_h^2, pmove=0.62\n",
      "I0130 11:13:25.620530 128561009378112 train.py:917] Step 00605: -125.7931 E_h, exp. variance=0.3797 E_h^2, pmove=0.61\n",
      "I0130 11:13:25.918329 128561009378112 train.py:917] Step 00606: -127.0447 E_h, exp. variance=0.4490 E_h^2, pmove=0.61\n",
      "I0130 11:13:26.213740 128561009378112 train.py:917] Step 00607: -126.4426 E_h, exp. variance=0.4171 E_h^2, pmove=0.61\n",
      "I0130 11:13:26.506468 128561009378112 train.py:917] Step 00608: -126.6178 E_h, exp. variance=0.3995 E_h^2, pmove=0.61\n",
      "I0130 11:13:26.801363 128561009378112 train.py:917] Step 00609: -127.2031 E_h, exp. variance=0.4590 E_h^2, pmove=0.62\n",
      "I0130 11:13:27.098366 128561009378112 train.py:917] Step 00610: -126.2380 E_h, exp. variance=0.4131 E_h^2, pmove=0.61\n",
      "I0130 11:13:27.391139 128561009378112 train.py:917] Step 00611: -125.4054 E_h, exp. variance=0.4368 E_h^2, pmove=0.62\n",
      "I0130 11:13:27.685288 128561009378112 train.py:917] Step 00612: -127.2548 E_h, exp. variance=0.4990 E_h^2, pmove=0.60\n",
      "I0130 11:13:27.983645 128561009378112 train.py:917] Step 00613: -125.2418 E_h, exp. variance=0.5459 E_h^2, pmove=0.62\n",
      "I0130 11:13:28.278360 128561009378112 train.py:917] Step 00614: -125.7849 E_h, exp. variance=0.5050 E_h^2, pmove=0.61\n",
      "I0130 11:13:28.570739 128561009378112 train.py:917] Step 00615: -125.9977 E_h, exp. variance=0.4562 E_h^2, pmove=0.61\n",
      "I0130 11:13:28.867377 128561009378112 train.py:917] Step 00616: -125.8943 E_h, exp. variance=0.4153 E_h^2, pmove=0.61\n",
      "I0130 11:13:29.166135 128561009378112 train.py:917] Step 00617: -127.5012 E_h, exp. variance=0.5506 E_h^2, pmove=0.62\n",
      "I0130 11:13:29.461395 128561009378112 train.py:917] Step 00618: -126.3537 E_h, exp. variance=0.4967 E_h^2, pmove=0.60\n",
      "I0130 11:13:29.756700 128561009378112 train.py:917] Step 00619: -125.5328 E_h, exp. variance=0.4935 E_h^2, pmove=0.60\n",
      "I0130 11:13:30.053977 128561009378112 train.py:917] Step 00620: -125.4265 E_h, exp. variance=0.4951 E_h^2, pmove=0.62\n",
      "I0130 11:13:30.349169 128561009378112 train.py:917] Step 00621: -126.1658 E_h, exp. variance=0.4459 E_h^2, pmove=0.60\n",
      "I0130 11:13:30.643382 128561009378112 train.py:917] Step 00622: -127.3118 E_h, exp. variance=0.5313 E_h^2, pmove=0.62\n",
      "I0130 11:13:30.938656 128561009378112 train.py:917] Step 00623: -125.8406 E_h, exp. variance=0.4919 E_h^2, pmove=0.61\n",
      "I0130 11:13:31.235320 128561009378112 train.py:917] Step 00624: -126.7259 E_h, exp. variance=0.4684 E_h^2, pmove=0.63\n",
      "I0130 11:13:31.530060 128561009378112 train.py:917] Step 00625: -126.3180 E_h, exp. variance=0.4220 E_h^2, pmove=0.64\n",
      "I0130 11:13:31.824810 128561009378112 train.py:917] Step 00626: -128.7566 E_h, exp. variance=0.9444 E_h^2, pmove=0.62\n",
      "I0130 11:13:32.120734 128561009378112 train.py:917] Step 00627: -124.7577 E_h, exp. variance=1.1239 E_h^2, pmove=0.61\n",
      "I0130 11:13:32.415507 128561009378112 train.py:917] Step 00628: -126.6299 E_h, exp. variance=1.0197 E_h^2, pmove=0.61\n",
      "I0130 11:13:32.710020 128561009378112 train.py:917] Step 00629: -125.2042 E_h, exp. variance=1.0376 E_h^2, pmove=0.61\n",
      "I0130 11:13:33.008272 128561009378112 train.py:917] Step 00630: -125.5404 E_h, exp. variance=0.9783 E_h^2, pmove=0.62\n",
      "I0130 11:13:33.303374 128561009378112 train.py:917] Step 00631: -126.8411 E_h, exp. variance=0.9207 E_h^2, pmove=0.61\n",
      "I0130 11:13:33.598341 128561009378112 train.py:917] Step 00632: -126.8746 E_h, exp. variance=0.8649 E_h^2, pmove=0.60\n",
      "I0130 11:13:33.907191 128561009378112 train.py:917] Step 00633: -126.9060 E_h, exp. variance=0.8112 E_h^2, pmove=0.61\n",
      "I0130 11:13:34.221534 128561009378112 train.py:917] Step 00634: -125.6347 E_h, exp. variance=0.7778 E_h^2, pmove=0.61\n",
      "I0130 11:13:34.537831 128561009378112 train.py:917] Step 00635: -126.8688 E_h, exp. variance=0.7301 E_h^2, pmove=0.60\n",
      "I0130 11:13:34.861150 128561009378112 train.py:917] Step 00636: -130.6249 E_h, exp. variance=2.3032 E_h^2, pmove=0.62\n",
      "I0130 11:13:35.168058 128561009378112 train.py:917] Step 00637: -126.2314 E_h, exp. variance=2.0995 E_h^2, pmove=0.60\n",
      "I0130 11:13:35.462695 128561009378112 train.py:917] Step 00638: -125.2246 E_h, exp. variance=2.0912 E_h^2, pmove=0.61\n",
      "I0130 11:13:35.763302 128561009378112 train.py:917] Step 00639: -124.9514 E_h, exp. variance=2.1184 E_h^2, pmove=0.60\n",
      "I0130 11:13:36.073071 128561009378112 train.py:917] Step 00640: -125.0389 E_h, exp. variance=2.0757 E_h^2, pmove=0.61\n",
      "I0130 11:13:36.386152 128561009378112 train.py:917] Step 00641: -125.3515 E_h, exp. variance=1.9445 E_h^2, pmove=0.62\n",
      "I0130 11:13:36.690679 128561009378112 train.py:917] Step 00642: -125.6096 E_h, exp. variance=1.7794 E_h^2, pmove=0.60\n",
      "I0130 11:13:36.985648 128561009378112 train.py:917] Step 00643: -126.7639 E_h, exp. variance=1.6384 E_h^2, pmove=0.62\n",
      "I0130 11:13:37.291604 128561009378112 train.py:917] Step 00644: -125.2838 E_h, exp. variance=1.5480 E_h^2, pmove=0.61\n",
      "I0130 11:13:37.593046 128561009378112 train.py:917] Step 00645: -124.5698 E_h, exp. variance=1.6032 E_h^2, pmove=0.61\n",
      "I0130 11:13:37.887572 128561009378112 train.py:917] Step 00646: -126.4319 E_h, exp. variance=1.4643 E_h^2, pmove=0.61\n",
      "I0130 11:13:38.200397 128561009378112 train.py:917] Step 00647: -126.3396 E_h, exp. variance=1.3286 E_h^2, pmove=0.61\n",
      "I0130 11:13:38.506601 128561009378112 train.py:917] Step 00648: -125.5485 E_h, exp. variance=1.2164 E_h^2, pmove=0.62\n",
      "I0130 11:13:38.801862 128561009378112 train.py:917] Step 00649: -127.3099 E_h, exp. variance=1.2540 E_h^2, pmove=0.61\n",
      "I0130 11:13:39.100072 128561009378112 train.py:917] Step 00650: -125.4675 E_h, exp. variance=1.1661 E_h^2, pmove=0.62\n",
      "I0130 11:13:39.398750 128561009378112 train.py:917] Step 00651: -127.6161 E_h, exp. variance=1.2707 E_h^2, pmove=0.60\n",
      "I0130 11:13:39.697371 128561009378112 train.py:917] Step 00652: -125.7321 E_h, exp. variance=1.1637 E_h^2, pmove=0.61\n",
      "I0130 11:13:39.998304 128561009378112 train.py:917] Step 00653: -127.1742 E_h, exp. variance=1.1403 E_h^2, pmove=0.62\n",
      "I0130 11:13:40.299855 128561009378112 train.py:917] Step 00654: -125.6270 E_h, exp. variance=1.0623 E_h^2, pmove=0.61\n",
      "I0130 11:13:40.600690 128561009378112 train.py:917] Step 00655: -125.4068 E_h, exp. variance=1.0122 E_h^2, pmove=0.62\n",
      "I0130 11:13:40.903820 128561009378112 train.py:917] Step 00656: -125.5245 E_h, exp. variance=0.9426 E_h^2, pmove=0.62\n",
      "I0130 11:13:41.213935 128561009378112 train.py:917] Step 00657: -126.5275 E_h, exp. variance=0.8682 E_h^2, pmove=0.60\n",
      "I0130 11:13:41.527406 128561009378112 train.py:917] Step 00658: -126.0807 E_h, exp. variance=0.7814 E_h^2, pmove=0.63\n",
      "I0130 11:13:41.837713 128561009378112 train.py:917] Step 00659: -128.2549 E_h, exp. variance=1.1202 E_h^2, pmove=0.59\n",
      "I0130 11:13:42.154388 128561009378112 train.py:917] Step 00660: -126.6499 E_h, exp. variance=1.0181 E_h^2, pmove=0.60\n",
      "I0130 11:13:42.467579 128561009378112 train.py:917] Step 00661: -125.4021 E_h, exp. variance=0.9974 E_h^2, pmove=0.61\n",
      "I0130 11:13:42.779098 128561009378112 train.py:917] Step 00662: -128.7276 E_h, exp. variance=1.4474 E_h^2, pmove=0.62\n",
      "I0130 11:13:43.087296 128561009378112 train.py:917] Step 00663: -125.8731 E_h, exp. variance=1.3384 E_h^2, pmove=0.61\n",
      "I0130 11:13:43.399876 128561009378112 train.py:917] Step 00664: -126.2017 E_h, exp. variance=1.2097 E_h^2, pmove=0.61\n",
      "I0130 11:13:43.708328 128561009378112 train.py:917] Step 00665: -127.3423 E_h, exp. variance=1.1659 E_h^2, pmove=0.59\n",
      "I0130 11:13:44.019085 128561009378112 train.py:917] Step 00666: -126.4065 E_h, exp. variance=1.0502 E_h^2, pmove=0.60\n",
      "I0130 11:13:44.332509 128561009378112 train.py:917] Step 00667: -124.3599 E_h, exp. variance=1.3569 E_h^2, pmove=0.60\n",
      "I0130 11:13:44.641623 128561009378112 train.py:917] Step 00668: -123.8751 E_h, exp. variance=1.7438 E_h^2, pmove=0.60\n",
      "I0130 11:13:44.951689 128561009378112 train.py:917] Step 00669: -126.5427 E_h, exp. variance=1.5918 E_h^2, pmove=0.61\n",
      "I0130 11:13:45.270961 128561009378112 train.py:917] Step 00670: -125.1380 E_h, exp. variance=1.5149 E_h^2, pmove=0.61\n",
      "I0130 11:13:45.583015 128561009378112 train.py:917] Step 00671: -126.1085 E_h, exp. variance=1.3645 E_h^2, pmove=0.60\n",
      "I0130 11:13:45.899466 128561009378112 train.py:917] Step 00672: -125.8271 E_h, exp. variance=1.2310 E_h^2, pmove=0.58\n",
      "I0130 11:13:46.213231 128561009378112 train.py:917] Step 00673: -126.6466 E_h, exp. variance=1.1466 E_h^2, pmove=0.61\n",
      "I0130 11:13:46.515730 128561009378112 train.py:917] Step 00674: -126.7625 E_h, exp. variance=1.0768 E_h^2, pmove=0.59\n",
      "I0130 11:13:46.820598 128561009378112 train.py:917] Step 00675: -126.0053 E_h, exp. variance=0.9704 E_h^2, pmove=0.61\n",
      "I0130 11:13:47.134894 128561009378112 train.py:917] Step 00676: -126.0808 E_h, exp. variance=0.8735 E_h^2, pmove=0.61\n",
      "I0130 11:13:47.454681 128561009378112 train.py:917] Step 00677: -126.3797 E_h, exp. variance=0.7926 E_h^2, pmove=0.61\n",
      "I0130 11:13:47.770639 128561009378112 train.py:917] Step 00678: -127.1123 E_h, exp. variance=0.7987 E_h^2, pmove=0.60\n",
      "I0130 11:13:48.084587 128561009378112 train.py:917] Step 00679: -126.1700 E_h, exp. variance=0.7193 E_h^2, pmove=0.59\n",
      "I0130 11:13:48.397810 128561009378112 train.py:917] Step 00680: -126.2246 E_h, exp. variance=0.6473 E_h^2, pmove=0.61\n",
      "I0130 11:13:48.692159 128561009378112 train.py:917] Step 00681: -126.6339 E_h, exp. variance=0.5974 E_h^2, pmove=0.60\n",
      "I0130 11:13:48.991414 128561009378112 train.py:917] Step 00682: -126.5383 E_h, exp. variance=0.5442 E_h^2, pmove=0.61\n",
      "I0130 11:13:49.294363 128561009378112 train.py:917] Step 00683: -125.6107 E_h, exp. variance=0.5320 E_h^2, pmove=0.61\n",
      "I0130 11:13:49.602159 128561009378112 train.py:917] Step 00684: -127.0279 E_h, exp. variance=0.5365 E_h^2, pmove=0.60\n",
      "I0130 11:13:49.900811 128561009378112 train.py:917] Step 00685: -125.7355 E_h, exp. variance=0.5123 E_h^2, pmove=0.60\n",
      "I0130 11:13:50.202400 128561009378112 train.py:917] Step 00686: -126.9134 E_h, exp. variance=0.5006 E_h^2, pmove=0.61\n",
      "I0130 11:13:50.501447 128561009378112 train.py:917] Step 00687: -124.7335 E_h, exp. variance=0.6761 E_h^2, pmove=0.61\n",
      "I0130 11:13:50.801907 128561009378112 train.py:917] Step 00688: -125.4055 E_h, exp. variance=0.6595 E_h^2, pmove=0.62\n",
      "I0130 11:13:51.111976 128561009378112 train.py:917] Step 00689: -125.5159 E_h, exp. variance=0.6225 E_h^2, pmove=0.63\n",
      "I0130 11:13:51.424829 128561009378112 train.py:917] Step 00690: -127.4027 E_h, exp. variance=0.7308 E_h^2, pmove=0.63\n",
      "I0130 11:13:51.737531 128561009378112 train.py:917] Step 00691: -127.5791 E_h, exp. variance=0.8379 E_h^2, pmove=0.61\n",
      "I0130 11:13:52.051765 128561009378112 train.py:917] Step 00692: -126.1617 E_h, exp. variance=0.7560 E_h^2, pmove=0.63\n",
      "I0130 11:13:52.358107 128561009378112 train.py:917] Step 00693: -128.1483 E_h, exp. variance=0.9908 E_h^2, pmove=0.61\n",
      "I0130 11:13:52.659345 128561009378112 train.py:917] Step 00694: -125.7400 E_h, exp. variance=0.9406 E_h^2, pmove=0.59\n",
      "I0130 11:13:52.966489 128561009378112 train.py:917] Step 00695: -126.5326 E_h, exp. variance=0.8480 E_h^2, pmove=0.61\n",
      "I0130 11:13:53.274949 128561009378112 train.py:917] Step 00696: -125.9293 E_h, exp. variance=0.7845 E_h^2, pmove=0.61\n",
      "I0130 11:13:53.583431 128561009378112 train.py:917] Step 00697: -125.5303 E_h, exp. variance=0.7692 E_h^2, pmove=0.61\n",
      "I0130 11:13:53.891741 128561009378112 train.py:917] Step 00698: -125.2518 E_h, exp. variance=0.7881 E_h^2, pmove=0.62\n",
      "I0130 11:13:54.185989 128561009378112 train.py:917] Step 00699: -126.2303 E_h, exp. variance=0.7095 E_h^2, pmove=0.59\n",
      "I0130 11:13:54.487441 128561009378112 train.py:917] Step 00700: -126.8559 E_h, exp. variance=0.6790 E_h^2, pmove=0.60\n",
      "I0130 11:13:54.789242 128561009378112 train.py:917] Step 00701: -126.2702 E_h, exp. variance=0.6111 E_h^2, pmove=0.58\n",
      "I0130 11:13:55.103854 128561009378112 train.py:917] Step 00702: -127.1335 E_h, exp. variance=0.6196 E_h^2, pmove=0.58\n",
      "I0130 11:13:55.404036 128561009378112 train.py:917] Step 00703: -127.5670 E_h, exp. variance=0.6926 E_h^2, pmove=0.58\n",
      "I0130 11:13:55.696721 128561009378112 train.py:917] Step 00704: -126.3076 E_h, exp. variance=0.6256 E_h^2, pmove=0.58\n",
      "I0130 11:13:55.986498 128561009378112 train.py:917] Step 00705: -127.7819 E_h, exp. variance=0.7229 E_h^2, pmove=0.57\n",
      "I0130 11:13:56.291336 128561009378112 train.py:917] Step 00706: -127.4572 E_h, exp. variance=0.7195 E_h^2, pmove=0.58\n",
      "I0130 11:13:56.591447 128561009378112 train.py:917] Step 00707: -125.8553 E_h, exp. variance=0.7073 E_h^2, pmove=0.58\n",
      "I0130 11:13:56.895773 128561009378112 train.py:917] Step 00708: -127.0237 E_h, exp. variance=0.6536 E_h^2, pmove=0.57\n",
      "I0130 11:13:57.197124 128561009378112 train.py:917] Step 00709: -127.7515 E_h, exp. variance=0.7011 E_h^2, pmove=0.59\n",
      "I0130 11:13:57.489646 128561009378112 train.py:917] Step 00710: -127.2111 E_h, exp. variance=0.6506 E_h^2, pmove=0.56\n",
      "I0130 11:13:57.792588 128561009378112 train.py:917] Step 00711: -126.0749 E_h, exp. variance=0.6316 E_h^2, pmove=0.59\n",
      "I0130 11:13:58.095137 128561009378112 train.py:917] Step 00712: -127.3963 E_h, exp. variance=0.6098 E_h^2, pmove=0.58\n",
      "I0130 11:13:58.396006 128561009378112 train.py:917] Step 00713: -126.2034 E_h, exp. variance=0.5794 E_h^2, pmove=0.60\n",
      "I0130 11:13:58.698805 128561009378112 train.py:917] Step 00714: -125.5835 E_h, exp. variance=0.6394 E_h^2, pmove=0.60\n",
      "I0130 11:13:59.000540 128561009378112 train.py:917] Step 00715: -127.5316 E_h, exp. variance=0.6513 E_h^2, pmove=0.59\n",
      "I0130 11:13:59.304295 128561009378112 train.py:917] Step 00716: -126.8088 E_h, exp. variance=0.5871 E_h^2, pmove=0.58\n",
      "I0130 11:13:59.600510 128561009378112 train.py:917] Step 00717: -125.6899 E_h, exp. variance=0.6232 E_h^2, pmove=0.58\n",
      "I0130 11:13:59.901090 128561009378112 train.py:917] Step 00718: -125.2713 E_h, exp. variance=0.7230 E_h^2, pmove=0.57\n",
      "I0130 11:14:00.202131 128561009378112 train.py:917] Step 00719: -125.7534 E_h, exp. variance=0.6981 E_h^2, pmove=0.57\n",
      "I0130 11:14:00.504019 128561009378112 train.py:917] Step 00720: -127.6691 E_h, exp. variance=0.7717 E_h^2, pmove=0.58\n",
      "I0130 11:14:00.817047 128561009378112 train.py:917] Step 00721: -128.6456 E_h, exp. variance=1.0963 E_h^2, pmove=0.58\n",
      "I0130 11:14:01.110393 128561009378112 train.py:917] Step 00722: -126.3324 E_h, exp. variance=1.0019 E_h^2, pmove=0.57\n",
      "I0130 11:14:01.400719 128561009378112 train.py:917] Step 00723: -125.3968 E_h, exp. variance=1.0553 E_h^2, pmove=0.56\n",
      "I0130 11:14:01.698290 128561009378112 train.py:917] Step 00724: -126.4716 E_h, exp. variance=0.9507 E_h^2, pmove=0.57\n",
      "I0130 11:14:02.004133 128561009378112 train.py:917] Step 00725: -126.0991 E_h, exp. variance=0.8749 E_h^2, pmove=0.56\n",
      "I0130 11:14:02.300707 128561009378112 train.py:917] Step 00726: -126.8444 E_h, exp. variance=0.7971 E_h^2, pmove=0.56\n",
      "I0130 11:14:02.616498 128561009378112 train.py:917] Step 00727: -126.3198 E_h, exp. variance=0.7221 E_h^2, pmove=0.58\n",
      "I0130 11:14:02.930170 128561009378112 train.py:917] Step 00728: -125.0704 E_h, exp. variance=0.8406 E_h^2, pmove=0.57\n",
      "I0130 11:14:03.240900 128561009378112 train.py:917] Step 00729: -125.4228 E_h, exp. variance=0.8390 E_h^2, pmove=0.58\n",
      "I0130 11:14:03.550173 128561009378112 train.py:917] Step 00730: -125.4779 E_h, exp. variance=0.8137 E_h^2, pmove=0.58\n",
      "I0130 11:14:03.865595 128561009378112 train.py:917] Step 00731: -125.8139 E_h, exp. variance=0.7460 E_h^2, pmove=0.56\n",
      "I0130 11:14:04.179203 128561009378112 train.py:917] Step 00732: -126.4164 E_h, exp. variance=0.6771 E_h^2, pmove=0.61\n",
      "I0130 11:14:04.496639 128561009378112 train.py:917] Step 00733: -127.9002 E_h, exp. variance=0.8726 E_h^2, pmove=0.55\n",
      "I0130 11:14:04.799319 128561009378112 train.py:917] Step 00734: -129.6693 E_h, exp. variance=1.7704 E_h^2, pmove=0.59\n",
      "I0130 11:14:05.103923 128561009378112 train.py:917] Step 00735: -128.2148 E_h, exp. variance=1.8021 E_h^2, pmove=0.57\n",
      "I0130 11:14:05.415165 128561009378112 train.py:917] Step 00736: -128.8561 E_h, exp. variance=1.9862 E_h^2, pmove=0.57\n",
      "I0130 11:14:05.714207 128561009378112 train.py:917] Step 00737: -128.9953 E_h, exp. variance=2.1298 E_h^2, pmove=0.55\n",
      "I0130 11:14:06.026444 128561009378112 train.py:917] Step 00738: -127.9874 E_h, exp. variance=1.9670 E_h^2, pmove=0.58\n",
      "I0130 11:14:06.316032 128561009378112 train.py:917] Step 00739: -128.8766 E_h, exp. variance=1.9897 E_h^2, pmove=0.57\n",
      "I0130 11:14:06.604480 128561009378112 train.py:917] Step 00740: -127.4467 E_h, exp. variance=1.7908 E_h^2, pmove=0.58\n",
      "I0130 11:14:06.901255 128561009378112 train.py:917] Step 00741: -126.2136 E_h, exp. variance=1.7535 E_h^2, pmove=0.56\n",
      "I0130 11:14:07.191588 128561009378112 train.py:917] Step 00742: -127.2884 E_h, exp. variance=1.5784 E_h^2, pmove=0.59\n",
      "I0130 11:14:07.480972 128561009378112 train.py:917] Step 00743: -126.6618 E_h, exp. variance=1.4617 E_h^2, pmove=0.58\n",
      "I0130 11:14:07.775274 128561009378112 train.py:917] Step 00744: -126.8619 E_h, exp. variance=1.3306 E_h^2, pmove=0.57\n",
      "I0130 11:14:08.068005 128561009378112 train.py:917] Step 00745: -126.6358 E_h, exp. variance=1.2292 E_h^2, pmove=0.58\n",
      "I0130 11:14:08.357795 128561009378112 train.py:917] Step 00746: -127.7112 E_h, exp. variance=1.1326 E_h^2, pmove=0.58\n",
      "I0130 11:14:08.647634 128561009378112 train.py:917] Step 00747: -126.3425 E_h, exp. variance=1.0893 E_h^2, pmove=0.58\n",
      "I0130 11:14:08.944404 128561009378112 train.py:917] Step 00748: -126.9027 E_h, exp. variance=0.9853 E_h^2, pmove=0.57\n",
      "I0130 11:14:09.233027 128561009378112 train.py:917] Step 00749: -126.0507 E_h, exp. variance=0.9883 E_h^2, pmove=0.56\n",
      "I0130 11:14:09.523038 128561009378112 train.py:917] Step 00750: -126.9516 E_h, exp. variance=0.8897 E_h^2, pmove=0.58\n",
      "I0130 11:14:09.815243 128561009378112 train.py:917] Step 00751: -126.8135 E_h, exp. variance=0.8039 E_h^2, pmove=0.58\n",
      "I0130 11:14:10.109116 128561009378112 train.py:917] Step 00752: -126.1968 E_h, exp. variance=0.7790 E_h^2, pmove=0.58\n",
      "I0130 11:14:10.398925 128561009378112 train.py:917] Step 00753: -127.6450 E_h, exp. variance=0.7506 E_h^2, pmove=0.57\n",
      "I0130 11:14:10.689625 128561009378112 train.py:917] Step 00754: -128.3257 E_h, exp. variance=0.8390 E_h^2, pmove=0.60\n",
      "I0130 11:14:10.985566 128561009378112 train.py:917] Step 00755: -126.6746 E_h, exp. variance=0.7724 E_h^2, pmove=0.59\n",
      "I0130 11:14:11.277719 128561009378112 train.py:917] Step 00756: -128.1817 E_h, exp. variance=0.8066 E_h^2, pmove=0.59\n",
      "I0130 11:14:11.566215 128561009378112 train.py:917] Step 00757: -127.6062 E_h, exp. variance=0.7423 E_h^2, pmove=0.57\n",
      "I0130 11:14:11.861373 128561009378112 train.py:917] Step 00758: -128.5023 E_h, exp. variance=0.8154 E_h^2, pmove=0.58\n",
      "I0130 11:14:12.151256 128561009378112 train.py:917] Step 00759: -127.3775 E_h, exp. variance=0.7339 E_h^2, pmove=0.58\n",
      "I0130 11:14:12.441334 128561009378112 train.py:917] Step 00760: -127.4859 E_h, exp. variance=0.6621 E_h^2, pmove=0.60\n",
      "I0130 11:14:12.731935 128561009378112 train.py:917] Step 00761: -128.4189 E_h, exp. variance=0.6956 E_h^2, pmove=0.56\n",
      "I0130 11:14:13.031631 128561009378112 train.py:917] Step 00762: -126.5160 E_h, exp. variance=0.7082 E_h^2, pmove=0.58\n",
      "I0130 11:14:13.345942 128561009378112 train.py:917] Step 00763: -126.7068 E_h, exp. variance=0.6778 E_h^2, pmove=0.58\n",
      "I0130 11:14:13.655355 128561009378112 train.py:917] Step 00764: -127.8195 E_h, exp. variance=0.6334 E_h^2, pmove=0.58\n",
      "I0130 11:14:13.956313 128561009378112 train.py:917] Step 00765: -126.8788 E_h, exp. variance=0.5909 E_h^2, pmove=0.58\n",
      "I0130 11:14:14.267394 128561009378112 train.py:917] Step 00766: -127.9298 E_h, exp. variance=0.5662 E_h^2, pmove=0.58\n",
      "I0130 11:14:14.579779 128561009378112 train.py:917] Step 00767: -127.8203 E_h, exp. variance=0.5275 E_h^2, pmove=0.57\n",
      "I0130 11:14:14.885994 128561009378112 train.py:917] Step 00768: -128.0526 E_h, exp. variance=0.5109 E_h^2, pmove=0.58\n",
      "I0130 11:14:15.182740 128561009378112 train.py:917] Step 00769: -126.4047 E_h, exp. variance=0.5643 E_h^2, pmove=0.56\n",
      "I0130 11:14:15.482523 128561009378112 train.py:917] Step 00770: -126.6967 E_h, exp. variance=0.5492 E_h^2, pmove=0.56\n",
      "I0130 11:14:15.800563 128561009378112 train.py:917] Step 00771: -126.7622 E_h, exp. variance=0.5209 E_h^2, pmove=0.56\n",
      "I0130 11:14:16.117228 128561009378112 train.py:917] Step 00772: -126.2256 E_h, exp. variance=0.5636 E_h^2, pmove=0.58\n",
      "I0130 11:14:16.428373 128561009378112 train.py:917] Step 00773: -127.2106 E_h, exp. variance=0.5076 E_h^2, pmove=0.56\n",
      "I0130 11:14:16.732432 128561009378112 train.py:917] Step 00774: -125.5379 E_h, exp. variance=0.6924 E_h^2, pmove=0.58\n",
      "I0130 11:14:17.029145 128561009378112 train.py:917] Step 00775: -126.7122 E_h, exp. variance=0.6303 E_h^2, pmove=0.59\n",
      "I0130 11:14:17.325589 128561009378112 train.py:917] Step 00776: -125.7165 E_h, exp. variance=0.7076 E_h^2, pmove=0.57\n",
      "I0130 11:14:17.615201 128561009378112 train.py:917] Step 00777: -126.6740 E_h, exp. variance=0.6394 E_h^2, pmove=0.57\n",
      "I0130 11:14:17.908950 128561009378112 train.py:917] Step 00778: -126.0489 E_h, exp. variance=0.6295 E_h^2, pmove=0.58\n",
      "I0130 11:14:18.200996 128561009378112 train.py:917] Step 00779: -128.1582 E_h, exp. variance=0.7459 E_h^2, pmove=0.57\n",
      "I0130 11:14:18.499125 128561009378112 train.py:917] Step 00780: -126.9621 E_h, exp. variance=0.6718 E_h^2, pmove=0.56\n",
      "I0130 11:14:18.802048 128561009378112 train.py:917] Step 00781: -126.8158 E_h, exp. variance=0.6052 E_h^2, pmove=0.58\n",
      "I0130 11:14:19.120500 128561009378112 train.py:917] Step 00782: -125.0652 E_h, exp. variance=0.8435 E_h^2, pmove=0.58\n",
      "I0130 11:14:19.424796 128561009378112 train.py:917] Step 00783: -127.0380 E_h, exp. variance=0.7691 E_h^2, pmove=0.57\n",
      "I0130 11:14:19.717398 128561009378112 train.py:917] Step 00784: -126.1840 E_h, exp. variance=0.7198 E_h^2, pmove=0.57\n",
      "I0130 11:14:20.028414 128561009378112 train.py:917] Step 00785: -127.6686 E_h, exp. variance=0.7353 E_h^2, pmove=0.57\n",
      "I0130 11:14:20.319619 128561009378112 train.py:917] Step 00786: -126.7911 E_h, exp. variance=0.6618 E_h^2, pmove=0.57\n",
      "I0130 11:14:20.621191 128561009378112 train.py:917] Step 00787: -127.5004 E_h, exp. variance=0.6420 E_h^2, pmove=0.57\n",
      "I0130 11:14:20.933393 128561009378112 train.py:917] Step 00788: -128.0515 E_h, exp. variance=0.7068 E_h^2, pmove=0.56\n",
      "I0130 11:14:21.243735 128561009378112 train.py:917] Step 00789: -128.0589 E_h, exp. variance=0.7421 E_h^2, pmove=0.56\n",
      "I0130 11:14:21.532813 128561009378112 train.py:917] Step 00790: -128.4019 E_h, exp. variance=0.8246 E_h^2, pmove=0.58\n",
      "I0130 11:14:21.824023 128561009378112 train.py:917] Step 00791: -126.9098 E_h, exp. variance=0.7504 E_h^2, pmove=0.57\n",
      "I0130 11:14:22.130988 128561009378112 train.py:917] Step 00792: -128.2820 E_h, exp. variance=0.7839 E_h^2, pmove=0.56\n",
      "I0130 11:14:22.435529 128561009378112 train.py:917] Step 00793: -125.8068 E_h, exp. variance=0.9045 E_h^2, pmove=0.58\n",
      "I0130 11:14:22.746670 128561009378112 train.py:917] Step 00794: -128.9247 E_h, exp. variance=1.0991 E_h^2, pmove=0.57\n",
      "I0130 11:14:23.059891 128561009378112 train.py:917] Step 00795: -126.4013 E_h, exp. variance=1.0657 E_h^2, pmove=0.56\n",
      "I0130 11:14:23.353873 128561009378112 train.py:917] Step 00796: -127.7348 E_h, exp. variance=0.9820 E_h^2, pmove=0.56\n",
      "I0130 11:14:23.644157 128561009378112 train.py:917] Step 00797: -128.2403 E_h, exp. variance=0.9666 E_h^2, pmove=0.56\n",
      "I0130 11:14:23.934795 128561009378112 train.py:917] Step 00798: -129.7361 E_h, exp. variance=1.3707 E_h^2, pmove=0.56\n",
      "I0130 11:14:24.230473 128561009378112 train.py:917] Step 00799: -128.1447 E_h, exp. variance=1.2591 E_h^2, pmove=0.57\n",
      "I0130 11:14:24.520239 128561009378112 train.py:917] Step 00800: -126.4505 E_h, exp. variance=1.2662 E_h^2, pmove=0.58\n",
      "I0130 11:14:24.813225 128561009378112 train.py:917] Step 00801: -126.8117 E_h, exp. variance=1.1879 E_h^2, pmove=0.51\n",
      "I0130 11:14:25.105861 128561009378112 train.py:917] Step 00802: -128.5744 E_h, exp. variance=1.1786 E_h^2, pmove=0.54\n",
      "I0130 11:14:25.398328 128561009378112 train.py:917] Step 00803: -127.5655 E_h, exp. variance=1.0608 E_h^2, pmove=0.54\n",
      "I0130 11:14:25.689780 128561009378112 train.py:917] Step 00804: -127.2239 E_h, exp. variance=0.9661 E_h^2, pmove=0.53\n",
      "I0130 11:14:25.984149 128561009378112 train.py:917] Step 00805: -126.8324 E_h, exp. variance=0.9151 E_h^2, pmove=0.51\n",
      "I0130 11:14:26.276696 128561009378112 train.py:917] Step 00806: -126.0454 E_h, exp. variance=1.0071 E_h^2, pmove=0.53\n",
      "I0130 11:14:26.565209 128561009378112 train.py:917] Step 00807: -126.1061 E_h, exp. variance=1.0413 E_h^2, pmove=0.54\n",
      "I0130 11:14:26.856190 128561009378112 train.py:917] Step 00808: -125.0124 E_h, exp. variance=1.3710 E_h^2, pmove=0.55\n",
      "I0130 11:14:27.151552 128561009378112 train.py:917] Step 00809: -125.6693 E_h, exp. variance=1.3906 E_h^2, pmove=0.53\n",
      "I0130 11:14:27.442274 128561009378112 train.py:917] Step 00810: -126.4874 E_h, exp. variance=1.2638 E_h^2, pmove=0.53\n",
      "I0130 11:14:27.733204 128561009378112 train.py:917] Step 00811: -124.7895 E_h, exp. variance=1.5083 E_h^2, pmove=0.54\n",
      "I0130 11:14:28.030152 128561009378112 train.py:917] Step 00812: -126.4609 E_h, exp. variance=1.3597 E_h^2, pmove=0.53\n",
      "I0130 11:14:28.342347 128561009378112 train.py:917] Step 00813: -127.7198 E_h, exp. variance=1.3363 E_h^2, pmove=0.55\n",
      "I0130 11:14:28.643743 128561009378112 train.py:917] Step 00814: -127.7294 E_h, exp. variance=1.2957 E_h^2, pmove=0.54\n",
      "I0130 11:14:28.943031 128561009378112 train.py:917] Step 00815: -127.0495 E_h, exp. variance=1.1711 E_h^2, pmove=0.52\n",
      "I0130 11:14:29.241000 128561009378112 train.py:917] Step 00816: -127.2439 E_h, exp. variance=1.0688 E_h^2, pmove=0.54\n",
      "I0130 11:14:29.537339 128561009378112 train.py:917] Step 00817: -127.4851 E_h, exp. variance=0.9950 E_h^2, pmove=0.51\n",
      "I0130 11:14:29.834100 128561009378112 train.py:917] Step 00818: -125.9266 E_h, exp. variance=0.9878 E_h^2, pmove=0.54\n",
      "I0130 11:14:30.130500 128561009378112 train.py:917] Step 00819: -127.1552 E_h, exp. variance=0.8981 E_h^2, pmove=0.54\n",
      "I0130 11:14:30.427690 128561009378112 train.py:917] Step 00820: -127.0409 E_h, exp. variance=0.8109 E_h^2, pmove=0.55\n",
      "I0130 11:14:30.723141 128561009378112 train.py:917] Step 00821: -128.9569 E_h, exp. variance=1.1155 E_h^2, pmove=0.53\n",
      "I0130 11:14:31.020430 128561009378112 train.py:917] Step 00822: -126.6844 E_h, exp. variance=1.0190 E_h^2, pmove=0.55\n",
      "I0130 11:14:31.318515 128561009378112 train.py:917] Step 00823: -126.6637 E_h, exp. variance=0.9308 E_h^2, pmove=0.55\n",
      "I0130 11:14:31.614369 128561009378112 train.py:917] Step 00824: -126.3145 E_h, exp. variance=0.8817 E_h^2, pmove=0.54\n",
      "I0130 11:14:31.911009 128561009378112 train.py:917] Step 00825: -129.5126 E_h, exp. variance=1.3873 E_h^2, pmove=0.52\n",
      "I0130 11:14:32.207518 128561009378112 train.py:917] Step 00826: -124.9175 E_h, exp. variance=1.7178 E_h^2, pmove=0.53\n",
      "I0130 11:14:32.503760 128561009378112 train.py:917] Step 00827: -127.7355 E_h, exp. variance=1.5984 E_h^2, pmove=0.54\n",
      "I0130 11:14:32.800697 128561009378112 train.py:917] Step 00828: -127.6715 E_h, exp. variance=1.4735 E_h^2, pmove=0.54\n",
      "I0130 11:14:33.097130 128561009378112 train.py:917] Step 00829: -126.9739 E_h, exp. variance=1.3278 E_h^2, pmove=0.53\n",
      "I0130 11:14:33.393911 128561009378112 train.py:917] Step 00830: -126.5229 E_h, exp. variance=1.2247 E_h^2, pmove=0.56\n",
      "I0130 11:14:33.689209 128561009378112 train.py:917] Step 00831: -126.0520 E_h, exp. variance=1.1901 E_h^2, pmove=0.54\n",
      "I0130 11:14:33.983562 128561009378112 train.py:917] Step 00832: -126.1126 E_h, exp. variance=1.1329 E_h^2, pmove=0.55\n",
      "I0130 11:14:34.278878 128561009378112 train.py:917] Step 00833: -126.5318 E_h, exp. variance=1.0292 E_h^2, pmove=0.54\n",
      "I0130 11:14:34.570473 128561009378112 train.py:917] Step 00834: -126.3612 E_h, exp. variance=0.9457 E_h^2, pmove=0.55\n",
      "I0130 11:14:34.881955 128561009378112 train.py:917] Step 00835: -126.4889 E_h, exp. variance=0.8587 E_h^2, pmove=0.54\n",
      "I0130 11:14:35.191995 128561009378112 train.py:917] Step 00836: -126.4102 E_h, exp. variance=0.7832 E_h^2, pmove=0.56\n",
      "I0130 11:14:35.486682 128561009378112 train.py:917] Step 00837: -126.9978 E_h, exp. variance=0.7121 E_h^2, pmove=0.54\n",
      "I0130 11:14:35.783404 128561009378112 train.py:917] Step 00838: -127.6685 E_h, exp. variance=0.7177 E_h^2, pmove=0.53\n",
      "I0130 11:14:36.078437 128561009378112 train.py:917] Step 00839: -126.7287 E_h, exp. variance=0.6470 E_h^2, pmove=0.54\n",
      "I0130 11:14:36.370675 128561009378112 train.py:917] Step 00840: -128.1961 E_h, exp. variance=0.7513 E_h^2, pmove=0.53\n",
      "I0130 11:14:36.664112 128561009378112 train.py:917] Step 00841: -127.5794 E_h, exp. variance=0.7103 E_h^2, pmove=0.52\n",
      "I0130 11:14:36.970134 128561009378112 train.py:917] Step 00842: -127.3468 E_h, exp. variance=0.6486 E_h^2, pmove=0.54\n",
      "I0130 11:14:37.274641 128561009378112 train.py:917] Step 00843: -126.7574 E_h, exp. variance=0.5918 E_h^2, pmove=0.53\n",
      "I0130 11:14:37.587107 128561009378112 train.py:917] Step 00844: -127.3897 E_h, exp. variance=0.5445 E_h^2, pmove=0.54\n",
      "I0130 11:14:37.898307 128561009378112 train.py:917] Step 00845: -127.0227 E_h, exp. variance=0.4902 E_h^2, pmove=0.54\n",
      "I0130 11:14:38.217165 128561009378112 train.py:917] Step 00846: -126.5451 E_h, exp. variance=0.4650 E_h^2, pmove=0.53\n",
      "I0130 11:14:38.535784 128561009378112 train.py:917] Step 00847: -127.3285 E_h, exp. variance=0.4277 E_h^2, pmove=0.52\n",
      "I0130 11:14:38.837324 128561009378112 train.py:917] Step 00848: -127.5876 E_h, exp. variance=0.4120 E_h^2, pmove=0.53\n",
      "I0130 11:14:39.131936 128561009378112 train.py:917] Step 00849: -126.5180 E_h, exp. variance=0.4007 E_h^2, pmove=0.53\n",
      "I0130 11:14:39.428704 128561009378112 train.py:917] Step 00850: -127.8199 E_h, exp. variance=0.4158 E_h^2, pmove=0.54\n",
      "I0130 11:14:39.728443 128561009378112 train.py:917] Step 00851: -127.6530 E_h, exp. variance=0.4002 E_h^2, pmove=0.54\n",
      "I0130 11:14:40.047205 128561009378112 train.py:917] Step 00852: -127.6943 E_h, exp. variance=0.3851 E_h^2, pmove=0.53\n",
      "I0130 11:14:40.356887 128561009378112 train.py:917] Step 00853: -127.3280 E_h, exp. variance=0.3476 E_h^2, pmove=0.54\n",
      "I0130 11:14:40.651977 128561009378112 train.py:917] Step 00854: -127.3643 E_h, exp. variance=0.3144 E_h^2, pmove=0.55\n",
      "I0130 11:14:40.959503 128561009378112 train.py:917] Step 00855: -126.3159 E_h, exp. variance=0.3607 E_h^2, pmove=0.54\n",
      "I0130 11:14:41.275016 128561009378112 train.py:917] Step 00856: -128.0248 E_h, exp. variance=0.3931 E_h^2, pmove=0.52\n",
      "I0130 11:14:41.584172 128561009378112 train.py:917] Step 00857: -127.9804 E_h, exp. variance=0.4032 E_h^2, pmove=0.53\n",
      "I0130 11:14:41.882941 128561009378112 train.py:917] Step 00858: -127.3742 E_h, exp. variance=0.3632 E_h^2, pmove=0.53\n",
      "I0130 11:14:42.182811 128561009378112 train.py:917] Step 00859: -127.8015 E_h, exp. variance=0.3478 E_h^2, pmove=0.54\n",
      "I0130 11:14:42.483760 128561009378112 train.py:917] Step 00860: -125.9179 E_h, exp. variance=0.5022 E_h^2, pmove=0.53\n",
      "I0130 11:14:42.788113 128561009378112 train.py:917] Step 00861: -126.5826 E_h, exp. variance=0.4889 E_h^2, pmove=0.54\n",
      "I0130 11:14:43.093971 128561009378112 train.py:917] Step 00862: -126.7061 E_h, exp. variance=0.4585 E_h^2, pmove=0.52\n",
      "I0130 11:14:43.397665 128561009378112 train.py:917] Step 00863: -126.2526 E_h, exp. variance=0.4794 E_h^2, pmove=0.52\n",
      "I0130 11:14:43.703638 128561009378112 train.py:917] Step 00864: -127.4866 E_h, exp. variance=0.4504 E_h^2, pmove=0.53\n",
      "I0130 11:14:44.002717 128561009378112 train.py:917] Step 00865: -126.4050 E_h, exp. variance=0.4456 E_h^2, pmove=0.54\n",
      "I0130 11:14:44.303416 128561009378112 train.py:917] Step 00866: -127.2766 E_h, exp. variance=0.4076 E_h^2, pmove=0.54\n",
      "I0130 11:14:44.616788 128561009378112 train.py:917] Step 00867: -126.5827 E_h, exp. variance=0.3851 E_h^2, pmove=0.56\n",
      "I0130 11:14:44.922940 128561009378112 train.py:917] Step 00868: -126.2188 E_h, exp. variance=0.3999 E_h^2, pmove=0.54\n",
      "I0130 11:14:45.223190 128561009378112 train.py:917] Step 00869: -129.5909 E_h, exp. variance=1.0060 E_h^2, pmove=0.55\n",
      "I0130 11:14:45.525495 128561009378112 train.py:917] Step 00870: -126.3034 E_h, exp. variance=0.9745 E_h^2, pmove=0.56\n",
      "I0130 11:14:45.831676 128561009378112 train.py:917] Step 00871: -126.3825 E_h, exp. variance=0.9223 E_h^2, pmove=0.56\n",
      "I0130 11:14:46.128770 128561009378112 train.py:917] Step 00872: -124.0113 E_h, exp. variance=1.6453 E_h^2, pmove=0.53\n",
      "I0130 11:14:46.432812 128561009378112 train.py:917] Step 00873: -126.9204 E_h, exp. variance=1.4844 E_h^2, pmove=0.55\n",
      "I0130 11:14:46.736971 128561009378112 train.py:917] Step 00874: -127.6059 E_h, exp. variance=1.4034 E_h^2, pmove=0.53\n",
      "I0130 11:14:47.041520 128561009378112 train.py:917] Step 00875: -128.5459 E_h, exp. variance=1.5291 E_h^2, pmove=0.54\n",
      "I0130 11:14:47.341275 128561009378112 train.py:917] Step 00876: -126.5836 E_h, exp. variance=1.3917 E_h^2, pmove=0.52\n",
      "I0130 11:14:47.641432 128561009378112 train.py:917] Step 00877: -127.0775 E_h, exp. variance=1.2539 E_h^2, pmove=0.52\n",
      "I0130 11:14:47.943941 128561009378112 train.py:917] Step 00878: -126.4846 E_h, exp. variance=1.1496 E_h^2, pmove=0.55\n",
      "I0130 11:14:48.240643 128561009378112 train.py:917] Step 00879: -126.6864 E_h, exp. variance=1.0396 E_h^2, pmove=0.54\n",
      "I0130 11:14:48.546653 128561009378112 train.py:917] Step 00880: -128.5995 E_h, exp. variance=1.1964 E_h^2, pmove=0.54\n",
      "I0130 11:14:48.858104 128561009378112 train.py:917] Step 00881: -127.9628 E_h, exp. variance=1.1489 E_h^2, pmove=0.52\n",
      "I0130 11:14:49.156471 128561009378112 train.py:917] Step 00882: -127.5903 E_h, exp. variance=1.0509 E_h^2, pmove=0.54\n",
      "I0130 11:14:49.455813 128561009378112 train.py:917] Step 00883: -126.9390 E_h, exp. variance=0.9520 E_h^2, pmove=0.53\n",
      "I0130 11:14:49.758549 128561009378112 train.py:917] Step 00884: -128.0669 E_h, exp. variance=0.9285 E_h^2, pmove=0.54\n",
      "I0130 11:14:50.059484 128561009378112 train.py:917] Step 00885: -126.9526 E_h, exp. variance=0.8444 E_h^2, pmove=0.54\n",
      "I0130 11:14:50.357474 128561009378112 train.py:917] Step 00886: -126.6777 E_h, exp. variance=0.7876 E_h^2, pmove=0.54\n",
      "I0130 11:14:50.656591 128561009378112 train.py:917] Step 00887: -128.0219 E_h, exp. variance=0.7731 E_h^2, pmove=0.55\n",
      "I0130 11:14:50.957751 128561009378112 train.py:917] Step 00888: -126.2878 E_h, exp. variance=0.7811 E_h^2, pmove=0.53\n",
      "I0130 11:14:51.266745 128561009378112 train.py:917] Step 00889: -127.2076 E_h, exp. variance=0.7032 E_h^2, pmove=0.54\n",
      "I0130 11:14:51.565773 128561009378112 train.py:917] Step 00890: -128.0514 E_h, exp. variance=0.7030 E_h^2, pmove=0.53\n",
      "I0130 11:14:51.878940 128561009378112 train.py:917] Step 00891: -130.3666 E_h, exp. variance=1.5031 E_h^2, pmove=0.53\n",
      "I0130 11:14:52.194802 128561009378112 train.py:917] Step 00892: -128.0130 E_h, exp. variance=1.3707 E_h^2, pmove=0.54\n",
      "I0130 11:14:52.514910 128561009378112 train.py:917] Step 00893: -126.7929 E_h, exp. variance=1.2940 E_h^2, pmove=0.54\n",
      "I0130 11:14:52.842519 128561009378112 train.py:917] Step 00894: -126.7464 E_h, exp. variance=1.2199 E_h^2, pmove=0.53\n",
      "I0130 11:14:53.145284 128561009378112 train.py:917] Step 00895: -127.7580 E_h, exp. variance=1.1064 E_h^2, pmove=0.53\n",
      "I0130 11:14:53.456666 128561009378112 train.py:917] Step 00896: -127.3564 E_h, exp. variance=0.9972 E_h^2, pmove=0.54\n",
      "I0130 11:14:53.763733 128561009378112 train.py:917] Step 00897: -123.9248 E_h, exp. variance=2.0285 E_h^2, pmove=0.53\n",
      "I0130 11:14:54.076791 128561009378112 train.py:917] Step 00898: -126.9827 E_h, exp. variance=1.8272 E_h^2, pmove=0.54\n",
      "I0130 11:14:54.390414 128561009378112 train.py:917] Step 00899: -128.4743 E_h, exp. variance=1.8140 E_h^2, pmove=0.52\n",
      "I0130 11:14:54.693250 128561009378112 train.py:917] Step 00900: -128.4683 E_h, exp. variance=1.7685 E_h^2, pmove=0.53\n",
      "I0130 11:14:54.999516 128561009378112 train.py:917] Step 00901: -126.1263 E_h, exp. variance=1.7291 E_h^2, pmove=0.54\n",
      "I0130 11:14:55.308604 128561009378112 train.py:917] Step 00902: -128.1020 E_h, exp. variance=1.6233 E_h^2, pmove=0.54\n",
      "I0130 11:14:55.615102 128561009378112 train.py:917] Step 00903: -125.6157 E_h, exp. variance=1.7239 E_h^2, pmove=0.55\n",
      "I0130 11:14:55.916676 128561009378112 train.py:917] Step 00904: -126.5888 E_h, exp. variance=1.5803 E_h^2, pmove=0.53\n",
      "I0130 11:14:56.215839 128561009378112 train.py:917] Step 00905: -125.6843 E_h, exp. variance=1.6020 E_h^2, pmove=0.55\n",
      "I0130 11:14:56.517194 128561009378112 train.py:917] Step 00906: -126.1850 E_h, exp. variance=1.4953 E_h^2, pmove=0.55\n",
      "I0130 11:14:56.823100 128561009378112 train.py:917] Step 00907: -125.5597 E_h, exp. variance=1.5025 E_h^2, pmove=0.54\n",
      "I0130 11:14:57.123582 128561009378112 train.py:917] Step 00908: -126.2411 E_h, exp. variance=1.3753 E_h^2, pmove=0.54\n",
      "I0130 11:14:57.425591 128561009378112 train.py:917] Step 00909: -126.4856 E_h, exp. variance=1.2418 E_h^2, pmove=0.55\n",
      "I0130 11:14:57.727100 128561009378112 train.py:917] Step 00910: -126.2042 E_h, exp. variance=1.1376 E_h^2, pmove=0.52\n",
      "I0130 11:14:58.032943 128561009378112 train.py:917] Step 00911: -126.6817 E_h, exp. variance=1.0241 E_h^2, pmove=0.54\n",
      "I0130 11:14:58.340838 128561009378112 train.py:917] Step 00912: -127.7483 E_h, exp. variance=1.0335 E_h^2, pmove=0.52\n",
      "I0130 11:14:58.653641 128561009378112 train.py:917] Step 00913: -127.4515 E_h, exp. variance=0.9750 E_h^2, pmove=0.52\n",
      "I0130 11:14:58.967458 128561009378112 train.py:917] Step 00914: -126.0697 E_h, exp. variance=0.9276 E_h^2, pmove=0.52\n",
      "I0130 11:14:59.284773 128561009378112 train.py:917] Step 00915: -126.9573 E_h, exp. variance=0.8391 E_h^2, pmove=0.52\n",
      "I0130 11:14:59.599535 128561009378112 train.py:917] Step 00916: -127.4079 E_h, exp. variance=0.7926 E_h^2, pmove=0.51\n",
      "I0130 11:14:59.916932 128561009378112 train.py:917] Step 00917: -126.7619 E_h, exp. variance=0.7137 E_h^2, pmove=0.54\n",
      "I0130 11:15:00.232501 128561009378112 train.py:917] Step 00918: -127.1439 E_h, exp. variance=0.6518 E_h^2, pmove=0.53\n",
      "I0130 11:15:00.550050 128561009378112 train.py:917] Step 00919: -127.3851 E_h, exp. variance=0.6121 E_h^2, pmove=0.53\n",
      "I0130 11:15:00.868306 128561009378112 train.py:917] Step 00920: -126.1674 E_h, exp. variance=0.6000 E_h^2, pmove=0.54\n",
      "I0130 11:15:01.183350 128561009378112 train.py:917] Step 00921: -128.4416 E_h, exp. variance=0.7731 E_h^2, pmove=0.52\n",
      "I0130 11:15:01.483010 128561009378112 train.py:917] Step 00922: -127.3281 E_h, exp. variance=0.7058 E_h^2, pmove=0.55\n",
      "I0130 11:15:01.792382 128561009378112 train.py:917] Step 00923: -126.8700 E_h, exp. variance=0.6375 E_h^2, pmove=0.53\n",
      "I0130 11:15:02.103445 128561009378112 train.py:917] Step 00924: -127.7937 E_h, exp. variance=0.6288 E_h^2, pmove=0.55\n",
      "I0130 11:15:02.408575 128561009378112 train.py:917] Step 00925: -127.8260 E_h, exp. variance=0.6148 E_h^2, pmove=0.55\n",
      "I0130 11:15:02.716758 128561009378112 train.py:917] Step 00926: -127.8251 E_h, exp. variance=0.5928 E_h^2, pmove=0.55\n",
      "I0130 11:15:03.020426 128561009378112 train.py:917] Step 00927: -127.3145 E_h, exp. variance=0.5341 E_h^2, pmove=0.54\n",
      "I0130 11:15:03.328778 128561009378112 train.py:917] Step 00928: -127.7822 E_h, exp. variance=0.5074 E_h^2, pmove=0.54\n",
      "I0130 11:15:03.632627 128561009378112 train.py:917] Step 00929: -127.0049 E_h, exp. variance=0.4641 E_h^2, pmove=0.53\n",
      "I0130 11:15:03.949827 128561009378112 train.py:917] Step 00930: -126.7281 E_h, exp. variance=0.4435 E_h^2, pmove=0.55\n",
      "I0130 11:15:04.265853 128561009378112 train.py:917] Step 00931: -127.7853 E_h, exp. variance=0.4289 E_h^2, pmove=0.53\n",
      "I0130 11:15:04.583653 128561009378112 train.py:917] Step 00932: -126.5623 E_h, exp. variance=0.4308 E_h^2, pmove=0.54\n",
      "I0130 11:15:04.890910 128561009378112 train.py:917] Step 00933: -128.3511 E_h, exp. variance=0.5076 E_h^2, pmove=0.53\n",
      "I0130 11:15:05.190764 128561009378112 train.py:917] Step 00934: -126.9626 E_h, exp. variance=0.4678 E_h^2, pmove=0.54\n",
      "I0130 11:15:05.492810 128561009378112 train.py:917] Step 00935: -125.9373 E_h, exp. variance=0.5827 E_h^2, pmove=0.52\n",
      "I0130 11:15:05.802232 128561009378112 train.py:917] Step 00936: -128.0558 E_h, exp. variance=0.5994 E_h^2, pmove=0.53\n",
      "I0130 11:15:06.109950 128561009378112 train.py:917] Step 00937: -128.9407 E_h, exp. variance=0.8014 E_h^2, pmove=0.54\n",
      "I0130 11:15:06.418670 128561009378112 train.py:917] Step 00938: -127.4107 E_h, exp. variance=0.7212 E_h^2, pmove=0.57\n",
      "I0130 11:15:06.735602 128561009378112 train.py:917] Step 00939: -125.9565 E_h, exp. variance=0.8382 E_h^2, pmove=0.55\n",
      "I0130 11:15:07.050639 128561009378112 train.py:917] Step 00940: -127.7511 E_h, exp. variance=0.7760 E_h^2, pmove=0.55\n",
      "I0130 11:15:07.364534 128561009378112 train.py:917] Step 00941: -128.0939 E_h, exp. variance=0.7537 E_h^2, pmove=0.54\n",
      "I0130 11:15:07.678743 128561009378112 train.py:917] Step 00942: -127.7304 E_h, exp. variance=0.6889 E_h^2, pmove=0.52\n",
      "I0130 11:15:07.995947 128561009378112 train.py:917] Step 00943: -127.2237 E_h, exp. variance=0.6235 E_h^2, pmove=0.54\n",
      "I0130 11:15:08.307123 128561009378112 train.py:917] Step 00944: -127.3031 E_h, exp. variance=0.5621 E_h^2, pmove=0.54\n",
      "I0130 11:15:08.623697 128561009378112 train.py:917] Step 00945: -128.1019 E_h, exp. variance=0.5511 E_h^2, pmove=0.54\n",
      "I0130 11:15:08.935882 128561009378112 train.py:917] Step 00946: -127.2078 E_h, exp. variance=0.5019 E_h^2, pmove=0.55\n",
      "I0130 11:15:09.247376 128561009378112 train.py:917] Step 00947: -127.6698 E_h, exp. variance=0.4565 E_h^2, pmove=0.53\n",
      "I0130 11:15:09.567504 128561009378112 train.py:917] Step 00948: -127.0538 E_h, exp. variance=0.4258 E_h^2, pmove=0.54\n",
      "I0130 11:15:09.885018 128561009378112 train.py:917] Step 00949: -126.9329 E_h, exp. variance=0.4046 E_h^2, pmove=0.54\n",
      "I0130 11:15:10.205689 128561009378112 train.py:917] Step 00950: -128.7455 E_h, exp. variance=0.5340 E_h^2, pmove=0.53\n",
      "I0130 11:15:10.516500 128561009378112 train.py:917] Step 00951: -126.5199 E_h, exp. variance=0.5687 E_h^2, pmove=0.54\n",
      "I0130 11:15:10.822929 128561009378112 train.py:917] Step 00952: -128.0301 E_h, exp. variance=0.5464 E_h^2, pmove=0.53\n",
      "I0130 11:15:11.142677 128561009378112 train.py:917] Step 00953: -127.4837 E_h, exp. variance=0.4918 E_h^2, pmove=0.54\n",
      "I0130 11:15:11.458951 128561009378112 train.py:917] Step 00954: -127.0745 E_h, exp. variance=0.4569 E_h^2, pmove=0.54\n",
      "I0130 11:15:11.777036 128561009378112 train.py:917] Step 00955: -126.3361 E_h, exp. variance=0.5196 E_h^2, pmove=0.55\n",
      "I0130 11:15:12.097479 128561009378112 train.py:917] Step 00956: -128.4878 E_h, exp. variance=0.5896 E_h^2, pmove=0.54\n",
      "I0130 11:15:12.402231 128561009378112 train.py:917] Step 00957: -126.4553 E_h, exp. variance=0.6179 E_h^2, pmove=0.53\n",
      "I0130 11:15:12.723203 128561009378112 train.py:917] Step 00958: -126.5322 E_h, exp. variance=0.6151 E_h^2, pmove=0.52\n",
      "I0130 11:15:13.047785 128561009378112 train.py:917] Step 00959: -127.7204 E_h, exp. variance=0.5726 E_h^2, pmove=0.54\n",
      "I0130 11:15:13.368156 128561009378112 train.py:917] Step 00960: -127.4046 E_h, exp. variance=0.5162 E_h^2, pmove=0.54\n",
      "I0130 11:15:13.682258 128561009378112 train.py:917] Step 00961: -126.3552 E_h, exp. variance=0.5478 E_h^2, pmove=0.54\n",
      "I0130 11:15:13.996432 128561009378112 train.py:917] Step 00962: -127.1933 E_h, exp. variance=0.4931 E_h^2, pmove=0.54\n",
      "I0130 11:15:14.311042 128561009378112 train.py:917] Step 00963: -126.4742 E_h, exp. variance=0.4935 E_h^2, pmove=0.53\n",
      "I0130 11:15:14.615759 128561009378112 train.py:917] Step 00964: -125.7463 E_h, exp. variance=0.6198 E_h^2, pmove=0.53\n",
      "I0130 11:15:14.915468 128561009378112 train.py:917] Step 00965: -125.8390 E_h, exp. variance=0.6798 E_h^2, pmove=0.54\n",
      "I0130 11:15:15.220105 128561009378112 train.py:917] Step 00966: -128.1675 E_h, exp. variance=0.7594 E_h^2, pmove=0.53\n",
      "I0130 11:15:15.522803 128561009378112 train.py:917] Step 00967: -128.3276 E_h, exp. variance=0.8385 E_h^2, pmove=0.53\n",
      "I0130 11:15:15.832201 128561009378112 train.py:917] Step 00968: -128.4937 E_h, exp. variance=0.9180 E_h^2, pmove=0.54\n",
      "I0130 11:15:16.141795 128561009378112 train.py:917] Step 00969: -126.2490 E_h, exp. variance=0.9221 E_h^2, pmove=0.53\n",
      "I0130 11:15:16.446283 128561009378112 train.py:917] Step 00970: -127.8326 E_h, exp. variance=0.8685 E_h^2, pmove=0.53\n",
      "I0130 11:15:16.750686 128561009378112 train.py:917] Step 00971: -127.3770 E_h, exp. variance=0.7832 E_h^2, pmove=0.55\n",
      "I0130 11:15:17.060291 128561009378112 train.py:917] Step 00972: -126.3401 E_h, exp. variance=0.7805 E_h^2, pmove=0.56\n",
      "I0130 11:15:17.369922 128561009378112 train.py:917] Step 00973: -127.3304 E_h, exp. variance=0.7049 E_h^2, pmove=0.54\n",
      "I0130 11:15:17.684192 128561009378112 train.py:917] Step 00974: -127.3914 E_h, exp. variance=0.6384 E_h^2, pmove=0.56\n",
      "I0130 11:15:17.990724 128561009378112 train.py:917] Step 00975: -126.4613 E_h, exp. variance=0.6240 E_h^2, pmove=0.55\n",
      "I0130 11:15:18.299861 128561009378112 train.py:917] Step 00976: -127.1735 E_h, exp. variance=0.5618 E_h^2, pmove=0.55\n",
      "I0130 11:15:18.606259 128561009378112 train.py:917] Step 00977: -127.0505 E_h, exp. variance=0.5062 E_h^2, pmove=0.55\n",
      "I0130 11:15:18.913173 128561009378112 train.py:917] Step 00978: -126.9931 E_h, exp. variance=0.4572 E_h^2, pmove=0.53\n",
      "I0130 11:15:19.220565 128561009378112 train.py:917] Step 00979: -126.7624 E_h, exp. variance=0.4224 E_h^2, pmove=0.53\n",
      "I0130 11:15:19.516491 128561009378112 train.py:917] Step 00980: -127.1573 E_h, exp. variance=0.3808 E_h^2, pmove=0.56\n",
      "I0130 11:15:19.817629 128561009378112 train.py:917] Step 00981: -129.0213 E_h, exp. variance=0.6802 E_h^2, pmove=0.53\n",
      "I0130 11:15:20.114253 128561009378112 train.py:917] Step 00982: -126.7558 E_h, exp. variance=0.6368 E_h^2, pmove=0.52\n",
      "I0130 11:15:20.414214 128561009378112 train.py:917] Step 00983: -126.7151 E_h, exp. variance=0.5966 E_h^2, pmove=0.54\n",
      "I0130 11:15:20.712677 128561009378112 train.py:917] Step 00984: -126.5022 E_h, exp. variance=0.5777 E_h^2, pmove=0.55\n",
      "I0130 11:15:21.020460 128561009378112 train.py:917] Step 00985: -126.3407 E_h, exp. variance=0.5729 E_h^2, pmove=0.54\n",
      "I0130 11:15:21.328103 128561009378112 train.py:917] Step 00986: -126.5313 E_h, exp. variance=0.5380 E_h^2, pmove=0.55\n",
      "I0130 11:15:21.630510 128561009378112 train.py:917] Step 00987: -126.1757 E_h, exp. variance=0.5426 E_h^2, pmove=0.54\n",
      "I0130 11:15:21.934550 128561009378112 train.py:917] Step 00988: -127.9941 E_h, exp. variance=0.5960 E_h^2, pmove=0.53\n",
      "I0130 11:15:22.236537 128561009378112 train.py:917] Step 00989: -127.3331 E_h, exp. variance=0.5458 E_h^2, pmove=0.54\n",
      "I0130 11:15:22.535848 128561009378112 train.py:917] Step 00990: -126.9126 E_h, exp. variance=0.4927 E_h^2, pmove=0.55\n",
      "I0130 11:15:22.835093 128561009378112 train.py:917] Step 00991: -127.5447 E_h, exp. variance=0.4674 E_h^2, pmove=0.54\n",
      "I0130 11:15:23.144124 128561009378112 train.py:917] Step 00992: -127.6023 E_h, exp. variance=0.4451 E_h^2, pmove=0.56\n",
      "I0130 11:15:23.451464 128561009378112 train.py:917] Step 00993: -126.2467 E_h, exp. variance=0.4713 E_h^2, pmove=0.54\n",
      "I0130 11:15:23.764072 128561009378112 train.py:917] Step 00994: -126.7264 E_h, exp. variance=0.4333 E_h^2, pmove=0.54\n",
      "I0130 11:15:24.064399 128561009378112 train.py:917] Step 00995: -126.6028 E_h, exp. variance=0.4050 E_h^2, pmove=0.54\n",
      "I0130 11:15:24.372462 128561009378112 train.py:917] Step 00996: -127.0255 E_h, exp. variance=0.3648 E_h^2, pmove=0.55\n",
      "I0130 11:15:24.673141 128561009378112 train.py:917] Step 00997: -127.8653 E_h, exp. variance=0.3994 E_h^2, pmove=0.54\n",
      "I0130 11:15:24.973519 128561009378112 train.py:917] Step 00998: -126.7229 E_h, exp. variance=0.3700 E_h^2, pmove=0.52\n",
      "I0130 11:15:25.275587 128561009378112 train.py:917] Step 00999: -126.7033 E_h, exp. variance=0.3427 E_h^2, pmove=0.54\n"
     ]
    }
   ],
   "source": [
    "!ferminet --config ferminet/configs/ne.py --config.optim.iterations 1000 --config.log.save_path runs/ne_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>energy</th>\n",
       "      <th>ewmean</th>\n",
       "      <th>ewvar</th>\n",
       "      <th>pmove</th>\n",
       "      <th>ewstddev</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>step</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-59.430088</td>\n",
       "      <td>-59.430088</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.923828</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-60.200474</td>\n",
       "      <td>-59.507126</td>\n",
       "      <td>0.053414</td>\n",
       "      <td>0.913281</td>\n",
       "      <td>0.231116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-60.759552</td>\n",
       "      <td>-59.632370</td>\n",
       "      <td>0.189244</td>\n",
       "      <td>0.907813</td>\n",
       "      <td>0.435022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-60.814760</td>\n",
       "      <td>-59.750610</td>\n",
       "      <td>0.296144</td>\n",
       "      <td>0.918750</td>\n",
       "      <td>0.544191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-60.647830</td>\n",
       "      <td>-59.840332</td>\n",
       "      <td>0.338980</td>\n",
       "      <td>0.902344</td>\n",
       "      <td>0.582220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         energy     ewmean     ewvar     pmove  ewstddev\n",
       "step                                                    \n",
       "0    -59.430088 -59.430088 -0.000000  0.923828 -0.000000\n",
       "1    -60.200474 -59.507126  0.053414  0.913281  0.231116\n",
       "2    -60.759552 -59.632370  0.189244  0.907813  0.435022\n",
       "3    -60.814760 -59.750610  0.296144  0.918750  0.544191\n",
       "4    -60.647830 -59.840332  0.338980  0.902344  0.582220"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd, matplotlib.pyplot as plt, numpy as np\n",
    "ne_results = pd.read_csv(\"runs/ne_1000/train_stats.csv\", index_col=\"step\")\n",
    "ne_results[\"ewstddev\"] = np.sqrt(ne_results[\"ewvar\"]) # compute std dev from variance\n",
    "ne_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (lax, rax) = plt.subplots(figsize=(8,4), ncols=2, sharey=True)\n",
    "fig.suptitle(\"Ground state energy for Neon\")\n",
    "# plot smoothed values on the left\n",
    "ne_results[\"ewmean\"].plot.line(label=\"exponentially weighted energy\", ax=lax)\n",
    "mu, sig = ne_results[\"ewmean\"], ne_results[\"ewstddev\"]\n",
    "lax.fill_between(ne_results.index, mu - sig, mu + sig, alpha=0.5, label=r\"$\\pm1\\sigma$-interval\")\n",
    "lax.axhline(-128.9366, label=\"baseline from FermiNet paper\", c=\"k\", ls=\"--\")\n",
    "lax.set_ylabel(r\"energy in $E_h$\")\n",
    "lax.grid(True)\n",
    "lax.legend()\n",
    "# plot batch values on the right\n",
    "ne_results[\"energy\"].plot.line(label=\"local energy of batch\", c=\"gray\", ax=rax)\n",
    "rax.axhline(-128.9366, label=\"baseline from FermiNet paper\", c=\"k\", ls=\"--\")\n",
    "rax.grid(True)\n",
    "rax.legend()\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory 2: Using prebuilt models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "from functools import partial\n",
    "from ferminet import networks, train, envelopes, loss, hamiltonian\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['walkers.npy', 'params.npz', 'geometry.npz']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"models/neon/\"\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'params.npz', 'rb') as f:\n",
    "  params = dict(np.load(f, allow_pickle=True))\n",
    "  params = params['arr_0'].tolist()\n",
    "\n",
    "with open(path + 'walkers.npy', 'rb') as f:\n",
    "  data = np.load(f)\n",
    "\n",
    "with open(path + 'geometry.npz', 'rb') as f:\n",
    "  geometry = dict(np.load(f, allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': (4096, 30),\n",
       " 'geometry': {'atoms': (1, 3), 'charges': (1,), 'spins': (2,)},\n",
       " 'params': {'double': [{'b': (32,), 'w': (4, 32)},\n",
       "   {'b': (32,), 'w': (32, 32)},\n",
       "   {'b': (32,), 'w': (32, 32)}],\n",
       "  'envelope': [{'pi': (1, 80), 'sigma': (1, 80)},\n",
       "   {'pi': (1, 80), 'sigma': (1, 80)}],\n",
       "  'orbital': [{'w': (256, 80)}, {'w': (256, 80)}],\n",
       "  'single': [{'b': (256,), 'w': (20, 256)},\n",
       "   {'b': (256,), 'w': (832, 256)},\n",
       "   {'b': (256,), 'w': (832, 256)},\n",
       "   {'b': (256,), 'w': (832, 256)}]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print parameter shapes\n",
    "jax.tree_util.tree_map(\n",
    "  lambda p: p.shape,\n",
    "  dict(params=params, data=data, geometry=geometry),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geometry[\"atoms\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: verify\n",
    "nspins = tuple(geometry[\"spins\"])\n",
    "nspins # alpha and beta electrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'double': [{'b': (32,), 'w': (4, 32)},\n",
       "  {'b': (32,), 'w': (32, 32)},\n",
       "  {'b': (32,), 'w': (32, 32)}],\n",
       " 'envelope': [{'pi': (1, 80), 'sigma': (1, 80)},\n",
       "  {'pi': (1, 80), 'sigma': (1, 80)}],\n",
       " 'orbital': [{'w': (256, 80)}, {'w': (256, 80)}],\n",
       " 'single': [{'b': (256,), 'w': (20, 256)},\n",
       "  {'b': (256,), 'w': (832, 256)},\n",
       "  {'b': (256,), 'w': (832, 256)},\n",
       "  {'b': (256,), 'w': (832, 256)}]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.tree_util.tree_map(\n",
    "  lambda p: p.shape,\n",
    "  #dict(params=params, data=data, geometry=geometry),\n",
    "  params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'envelope': [{'pi': (1, 80), 'sigma': (1, 80)},\n",
       "  {'pi': (1, 80), 'sigma': (1, 80)}],\n",
       " 'layers': {'input': {},\n",
       "  'streams': [{'double': {'b': (32,), 'w': (4, 32)},\n",
       "    'single': {'b': (256,), 'w': (20, 256)}},\n",
       "   {'double': {'b': (32,), 'w': (32, 32)},\n",
       "    'single': {'b': (256,), 'w': (832, 256)}},\n",
       "   {'single': {'b': (256,), 'w': (832, 256)}}]},\n",
       " 'orbital': [{'w': (256, 80)}, {'w': (256, 80)}]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signed_network = networks.make_fermi_net(\n",
    "  nspins,\n",
    "  geometry[\"charges\"], # charges (atomic number)\n",
    "  envelope=None, # by default, an isotropic envelope is constructed\n",
    "  full_det=False,\n",
    ")\n",
    "network = lambda *args: signed_network.apply(*args)[1]\n",
    "#batched_network = jax.vmap(network, (None, 0), 0)\n",
    "#signed_network.apply()\n",
    "random_net_params = signed_network.init(jax.random.PRNGKey(42))\n",
    "jax.tree_util.tree_map(\n",
    "  lambda p: p.shape,\n",
    "  #dict(params=params, data=data, geometry=geometry),\n",
    "  random_net_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signed_network.apply(\n",
    "  random_net_params,\n",
    "  \n",
    "  nspins,\n",
    "  geometry[\"atoms\"],\n",
    "  geometry[\"charges\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function ferminet.hamiltonian.local_energy.<locals>._e_l(params: Union[jax.Array, Iterable[ForwardRef('ParamTree')], MutableMapping[Any, ForwardRef('ParamTree')]], key: jax.Array, data: ferminet.networks.FermiNetData) -> Tuple[jax.Array, Optional[jax.Array]]>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_energy_fn = hamiltonian.local_energy(\n",
    "  network,\n",
    "  geometry[\"charges\"],\n",
    "  nspins,\n",
    ")\n",
    "local_energy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<jax._src.custom_derivatives.custom_jvp at 0x742d8a3f1150>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = loss.make_loss(\n",
    "  network,\n",
    "  local_energy_fn, \n",
    "  clip_local_energy=5., # from prebuilt models example\n",
    ")\n",
    "loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_data = networks.FermiNetData(\n",
    "  positions=data[0][None,...],\n",
    "  spins=geometry[\"spins\"][None,...],\n",
    "  atoms=geometry[\"atoms\"][None,...],\n",
    "  charges=geometry[\"charges\"][None,...],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 5 frame]\u001b[0m\n",
      "File \u001b[0;32m~/work/ferminet/ferminet/loss.py:205\u001b[0m, in \u001b[0;36mmake_loss.<locals>.total_energy\u001b[0;34m(params, key, data)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluates the total energy of the network for a batch of configurations.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mNote: the signature of this function is fixed to match that expected by\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m  over the batch and over all devices inside a pmap.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m keys \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key, num\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mpositions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 205\u001b[0m e_l, e_l_mat \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_local_energy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m loss \u001b[38;5;241m=\u001b[39m constants\u001b[38;5;241m.\u001b[39mpmean(jnp\u001b[38;5;241m.\u001b[39mmean(e_l))\n\u001b[1;32m    207\u001b[0m loss_diff \u001b[38;5;241m=\u001b[39m e_l \u001b[38;5;241m-\u001b[39m loss\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m~/work/ferminet/ferminet/hamiltonian.py:394\u001b[0m, in \u001b[0;36mlocal_energy.<locals>._e_l\u001b[0;34m(params, key, data)\u001b[0m\n\u001b[1;32m    388\u001b[0m ae, _, r_ae, r_ee \u001b[38;5;241m=\u001b[39m networks\u001b[38;5;241m.\u001b[39mconstruct_input_features(\n\u001b[1;32m    389\u001b[0m     data\u001b[38;5;241m.\u001b[39mpositions, data\u001b[38;5;241m.\u001b[39matoms\n\u001b[1;32m    390\u001b[0m )\n\u001b[1;32m    391\u001b[0m potential \u001b[38;5;241m=\u001b[39m (potential_energy(r_ae, r_ee, data\u001b[38;5;241m.\u001b[39matoms, effective_charges) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    392\u001b[0m              pp_local(r_ae) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    393\u001b[0m              pp_nonlocal(key, f, params, data, ae, r_ae))\n\u001b[0;32m--> 394\u001b[0m kinetic \u001b[38;5;241m=\u001b[39m \u001b[43mke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m total_energy \u001b[38;5;241m=\u001b[39m potential \u001b[38;5;241m+\u001b[39m kinetic\n\u001b[1;32m    396\u001b[0m energy_mat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Not necessary for ground state\u001b[39;00m\n",
      "File \u001b[0;32m~/work/ferminet/ferminet/hamiltonian.py:114\u001b[0m, in \u001b[0;36mlocal_kinetic_energy.<locals>._lapl_over_f\u001b[0;34m(params, data)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad_f_closure\u001b[39m(x):\n\u001b[1;32m    112\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_f(params, x, data\u001b[38;5;241m.\u001b[39mspins, data\u001b[38;5;241m.\u001b[39matoms, data\u001b[38;5;241m.\u001b[39mcharges)\n\u001b[0;32m--> 114\u001b[0m primal, dgrad_f \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinearize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_f_closure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m complex_output:\n\u001b[1;32m    117\u001b[0m   grad_phase \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mgrad(phase_f, argnums\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 5 frame]\u001b[0m\n",
      "File \u001b[0;32m~/work/ferminet/ferminet/hamiltonian.py:112\u001b[0m, in \u001b[0;36mlocal_kinetic_energy.<locals>._lapl_over_f.<locals>.grad_f_closure\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad_f_closure\u001b[39m(x):\n\u001b[0;32m--> 112\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matoms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharges\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m~/work/ferminet/ferminet/utils/utils.py:25\u001b[0m, in \u001b[0;36mselect_output.<locals>.f_selected\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf_selected\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 25\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[argnum]\n",
      "Cell \u001b[0;32mIn[59], line 7\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m      1\u001b[0m signed_network \u001b[38;5;241m=\u001b[39m networks\u001b[38;5;241m.\u001b[39mmake_fermi_net(\n\u001b[1;32m      2\u001b[0m   nspins,\n\u001b[1;32m      3\u001b[0m   geometry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharges\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;66;03m# charges\u001b[39;00m\n\u001b[1;32m      4\u001b[0m   envelope\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# by default, an isotropic envelope is constructed\u001b[39;00m\n\u001b[1;32m      5\u001b[0m   full_det\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m network \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs: \u001b[43msigned_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      8\u001b[0m batched_network \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(network, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/work/ferminet/ferminet/networks.py:1472\u001b[0m, in \u001b[0;36mmake_fermi_net.<locals>.apply\u001b[0;34m(params, pos, spins, atoms, charges)\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   1452\u001b[0m     params,\n\u001b[1;32m   1453\u001b[0m     pos: jnp\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1456\u001b[0m     charges: jnp\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m   1457\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[jnp\u001b[38;5;241m.\u001b[39mndarray, jnp\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Forward evaluation of the Fermionic Neural Network for a single datum.\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m \n\u001b[1;32m   1460\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;124;03m    of and log absolute of the network evaluated at x.\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1472\u001b[0m   orbitals \u001b[38;5;241m=\u001b[39m \u001b[43morbitals_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matoms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1473\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m options\u001b[38;5;241m.\u001b[39mstates:\n\u001b[1;32m   1474\u001b[0m     batch_logdet_matmul \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(network_blocks\u001b[38;5;241m.\u001b[39mlogdet_matmul, in_axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/work/ferminet/ferminet/networks.py:1175\u001b[0m, in \u001b[0;36mmake_orbitals.<locals>.apply\u001b[0;34m(params, pos, spins, atoms, charges)\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward evaluation of the Fermionic Neural Network up to the orbitals.\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m \n\u001b[1;32m   1161\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;124;03m  nalpha+nbeta) (or (ndet, nalpha, nalpha) and (ndet, nbeta, nbeta)).\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1173\u001b[0m ae, ee, r_ae, r_ee \u001b[38;5;241m=\u001b[39m construct_input_features(pos, atoms, ndim\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mndim)\n\u001b[1;32m   1174\u001b[0m h_to_orbitals \u001b[38;5;241m=\u001b[39m equivariant_layers_apply(\n\u001b[0;32m-> 1175\u001b[0m     \u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlayers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m   1176\u001b[0m     ae\u001b[38;5;241m=\u001b[39mae,\n\u001b[1;32m   1177\u001b[0m     r_ae\u001b[38;5;241m=\u001b[39mr_ae,\n\u001b[1;32m   1178\u001b[0m     ee\u001b[38;5;241m=\u001b[39mee,\n\u001b[1;32m   1179\u001b[0m     r_ee\u001b[38;5;241m=\u001b[39mr_ee,\n\u001b[1;32m   1180\u001b[0m     spins\u001b[38;5;241m=\u001b[39mspins,\n\u001b[1;32m   1181\u001b[0m     charges\u001b[38;5;241m=\u001b[39mcharges,\n\u001b[1;32m   1182\u001b[0m )\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m options\u001b[38;5;241m.\u001b[39menvelope\u001b[38;5;241m.\u001b[39mapply_type \u001b[38;5;241m==\u001b[39m envelopes\u001b[38;5;241m.\u001b[39mEnvelopeType\u001b[38;5;241m.\u001b[39mPRE_ORBITAL:\n\u001b[1;32m   1185\u001b[0m   envelope_factor \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39menvelope\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m   1186\u001b[0m       ae\u001b[38;5;241m=\u001b[39mae, r_ae\u001b[38;5;241m=\u001b[39mr_ae, r_ee\u001b[38;5;241m=\u001b[39mr_ee, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menvelope\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m   1187\u001b[0m   )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'layers'"
     ]
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(42)\n",
    "loss_fn(params, key, net_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the first argument must be callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m signed_network \u001b[38;5;241m=\u001b[39m \u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetworks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvelope_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43misotropic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_det\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeometry\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# networks.fermi_net gives the sign/log of the wavefunction. We only care about the latter.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m network \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m p, x: signed_network(p, x)[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: the first argument must be callable"
     ]
    }
   ],
   "source": [
    "signed_network = partial(networks, envelope_type='isotropic', full_det=False, **geometry)\n",
    "# networks.fermi_net gives the sign/log of the wavefunction. We only care about the latter.\n",
    "network = lambda p, x: signed_network(p, x)[1]\n",
    "batch_network = jax.vmap(network, (None, 0), 0)\n",
    "loss = train.make_loss(network, batch_network, geometry['atoms'], geometry['charges'], clip_local_energy=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function ferminet.networks.make_fermi_net(nspins: Tuple[int, int], charges: jax.Array, *, ndim: int = 3, determinants: int = 16, states: int = 0, envelope: Optional[ferminet.envelopes.Envelope] = None, feature_layer: Optional[ferminet.networks.FeatureLayer] = None, jastrow: Union[str, ferminet.jastrows.JastrowType] = <JastrowType.NONE: 1>, complex_output: bool = False, bias_orbitals: bool = False, full_det: bool = True, rescale_inputs: bool = False, hidden_dims: Tuple[Tuple[int, int], ...] = ((256, 32), (256, 32), (256, 32)), use_last_layer: bool = False, separate_spin_channels: bool = False, schnet_electron_electron_convolutions: Tuple[int, ...] = (), electron_nuclear_aux_dims: Tuple[int, ...] = (), nuclear_embedding_dim: int = 0, schnet_electron_nuclear_convolutions: Tuple[int, ...] = ()) -> ferminet.networks.Network>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "networks.make_fermi_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any\n",
      "ApplyLayersFn\n",
      "BaseNetworkOptions\n",
      "FeatureApply\n",
      "FeatureInit\n",
      "FeatureLayer\n",
      "FeatureLayerType\n",
      "FermiLayers\n",
      "FermiNetData\n",
      "FermiNetLike\n",
      "FermiNetOptions\n",
      "InitFermiNet\n",
      "InitLayersFn\n",
      "Iterable\n",
      "LogFermiNetLike\n",
      "MakeFeatureLayer\n",
      "Mapping\n",
      "MutableMapping\n",
      "Network\n",
      "Optional\n",
      "OrbitalFnLike\n",
      "Param\n",
      "ParamTree\n",
      "Protocol\n",
      "Sequence\n",
      "Tuple\n",
      "Union\n",
      "__builtins__\n",
      "__cached__\n",
      "__doc__\n",
      "__file__\n",
      "__loader__\n",
      "__name__\n",
      "__package__\n",
      "__spec__\n",
      "_combine_spin_pairs\n",
      "_split_spin_pairs\n",
      "attr\n",
      "chex\n",
      "construct_input_features\n",
      "construct_symmetric_features\n",
      "enum\n",
      "envelopes\n",
      "functools\n",
      "jastrows\n",
      "jax\n",
      "jnp\n",
      "make_fermi_net\n",
      "make_fermi_net_layers\n",
      "make_ferminet_features\n",
      "make_orbitals\n",
      "make_schnet_convolution\n",
      "make_schnet_electron_nuclear_convolution\n",
      "make_state_matrix\n",
      "make_total_ansatz\n",
      "network_blocks\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(dir(networks)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ferminet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
